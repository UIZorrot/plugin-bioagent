{
  "structure": "- .env.example\n- .gitignore\n- README.md\n- analysis_report/\n  - report_20250425_114245.md\n- drizzle.config.ts\n- src/\n  - actions/\n    - dkgInsert.ts\n    - index.ts\n  - constants.ts\n  - db/\n    - index.ts\n    - migration.ts\n    - schemas/\n      - customTypes.ts\n      - driveSync.ts\n      - fileMetadata.ts\n      - gdriveChannels.ts\n      - hypotheses.ts\n      - hypothesesSummary.ts\n      - index.ts\n  - evaluators/\n    - evaluationPrompt.ts\n    - index.ts\n    - types.ts\n  - helper.ts\n  - index.ts\n  - routes/\n    - controller.ts\n    - gdrive/\n      - index.ts\n      - manualSync.ts\n      - webhook.ts\n    - health.ts\n    - index.ts\n  - services/\n    - anthropic/\n      - chooseTwoRelevant.ts\n      - client.ts\n      - discordSplitter.ts\n      - errors.ts\n      - evaluateHypothesis.ts\n      - generateHypothesis.ts\n      - hypGenEvalLoop.ts\n      - sparql/\n        - makeRequest.ts\n        - queries.ts\n      - types.ts\n    - gdrive/\n      - buildQuery.ts\n      - client.ts\n      - extract/\n        - README.md\n        - config.ts\n        - index.ts\n        - prompts.ts\n        - types.ts\n        - z.ts\n      - index.ts\n      - storeJsonLdToKg.ts\n      - watchFiles.ts\n    - index.ts\n    - kaService/\n      - anthropicClient.ts\n      - biologyApi.ts\n      - downloadPaper.ts\n      - exampleForPrompts.ts\n      - kaService.ts\n      - llmPrompt.ts\n      - processPaper.ts\n      - regex.ts\n      - sparqlQueries.ts\n      - unstructuredPartitioning.ts\n      - vectorize.ts\n  - templates.ts\n  - types.ts\n- tsup.config.ts",
  "files": [
    {
      "path": ".env.example",
      "detailed_description": ".env.example 是一个示例环境变量配置文件，旨在为项目的开发和部署提供必要的环境设置。该文件包含多个配置项，涵盖了数据库连接信息、日志记录设置、客户端和模型提供者的配置等。具体而言，PROD_URL 和 DEV_URL 用于区分生产和开发环境的 URL；POSTGRES_URL 提供了 PostgreSQL 数据库的连接字符串；而 DEFAULT_LOG_LEVEL 则允许开发者设定日志记录的详细程度。此外，文件还包含 Discord、OpenAI、Anthropic 及其他服务的 API 密钥和配置项，确保在使用这些外部服务时能够正确初始化和连接。通过提供这样的示例，开发者可以根据自己的需求复制和修改文件内容，以适应本地开发或生产环境的要求。",
      "summary": "该文件提供了项目所需的环境变量的示例配置，以支持开发和部署过程。",
      "raw": "## Deployed Eliza URL\nPROD_URL=\nDEV_URL= # use ngrok to get a local url\n\n## DB\nPOSTGRES_URL=postgresql://user:password@localhost:5432/dbname\nPROD_POSTGRES_PASSWORD=your_production_password\n\n# Logging\nDEFAULT_LOG_LEVEL=debug\nLOG_JSON_FORMAT=false            # Print everything in logger as json; false by default\n\n###############################\n#### Client Configurations ####\n###############################\n\n# Discord Configuration\nDISCORD_APPLICATION_ID=your_discord_app_id\nDISCORD_API_TOKEN=your_discord_bot_token\nDISCORD_VOICE_CHANNEL_ID=your_voice_channel_id  # Optional\nDISCORD_CHANNEL_ID=your_text_channel_id\n\n#######################################\n#### Model Provider Configurations ####\n#######################################\n\n# OpenAI Configuration\nOPENAI_API_KEY=your_openai_api_key  # Starting with sk-\n\n# Anthropic Configuration\nANTHROPIC_API_KEY=your_anthropic_api_key  # For Claude\n\n# OriginTrail DKG\nDKG_ENVIRONMENT=\"testnet\"  # Values: \"development\", \"testnet\", \"mainnet\"\nDKG_HOSTNAME=\"your_dkg_hostname\"\nDKG_PORT=\"8900\"\nDKG_PUBLIC_KEY=\"your_dkg_public_key\"\nDKG_PRIVATE_KEY=\"your_dkg_private_key\"\nDKG_BLOCKCHAIN_NAME=\"base:84532\"  # Values: (mainnet) \"base:8453\", \"gnosis:100\", \"otp:2043\" (testnet) \"base:84532\", \"gnosis:10200\", \"otp:20430\"\n\n# Unstructured API Key\nUNSTRUCTURED_API_KEY=your_unstructured_api_key\n\n# Biontology API Key\nBIONTOLOGY_KEY=your_biontology_api_key\n\n# GCP\nGCP_JSON_CREDENTIALS={\"type\": \"service_account\", \"project_id\": \"your_project_id\", ...}  # Your full GCP service account JSON\nGOOGLE_DRIVE_FOLDER_ID= # the id of the folder where you want to dump the scientific\n\n# Railway\nRAILWAY_ENVIRONMENT_NAME=local\n\n# Oxigraph\nPROD_OXIGRAPH_HOST=http://oxigraph:7878 # railway\nLOCAL_OXIGRAPH_HOST=your_local_oxigraph_host "
    },
    {
      "path": ".gitignore",
      "detailed_description": ".gitignore 文件用于指定在 Git 版本控制中忽略的文件和目录。通过列出不需要跟踪的文件类型和路径，开发者可以确保这些文件不会被意外提交到版本库中，从而保持代码库的整洁和安全。常见的忽略项包括 node_modules 目录（用于存放项目依赖的第三方库），.env 文件（存储敏感环境变量），以及生成的文件如 dist 目录和各种锁文件（如 *.lock）。此外，.gitignore 还排除了项目生成的迁移文件、临时文件和特定格式的文件（如 JSON 和 YAML 文件），确保版本控制系统只跟踪真正需要的源代码和配置文件。这对于避免将不必要的或敏感的文件上传到版本库至关重要，尤其是在团队协作和开源项目中。通过有效地利用 .gitignore 文件，开发者可以提升项目的可维护性和安全性。",
      "summary": "该文件指定了在 Git 版本控制中应被忽略的文件和目录，以保持代码库的整洁。",
      "raw": "node_modules\n.env\n.migration-complete\ndrizzle\n*.lock\n*.json\n*.yaml\n.git\ndist\nimages"
    },
    {
      "path": "README.md",
      "detailed_description": "README.md 文件是项目的核心文档，详细介绍了 BioAgent Plugin 的功能、安装步骤和配置要求。该插件旨在帮助研究人员和生物科学家将科学论文转换为结构化的知识资产，进而集成到 RDF 三元组存储中，如 OriginTrail 的去中心化知识图或 Oxigraph。这一过程包括监控用户指定的 Google Drive 文件夹，通过检测新文档并自动转化为知识资产，最终将其整合到一个动态的知识图中。插件内置智能代理，可系统地生成新的生物假设，并通过 JudgeLLM 进行评估和打分，确保假设的准确性和相关性。README.md 还提供了系统依赖的安装、项目克隆、依赖安装、Docker 容器启动和开发服务器启动的详细指引。此外，文件中包括了关于如何配置环境变量、创建 Google Cloud 服务账户以及设置 Google Drive 文件夹访问权限的说明，以便用户能够顺利运行和使用该插件。整体而言，该文档为用户提供了必要的背景信息和实用的操作步骤，确保用户能够快速上手并有效利用该插件。",
      "summary": "README.md 文件介绍了 BioAgent Plugin 的功能、安装步骤和配置要求，旨在帮助研究人员将科学论文转化为结构化知识资产。",
      "raw": "# BioAgent Plugin 🤖🧬\n\nAn Eliza plugin tailored for researchers and bioscientists, converting scientific papers into structured Knowledge Assets (KAs). These Knowledge Assets integrate seamlessly into RDF triple stores such as [OriginTrail's DKG](https://origintrail.io/technology/decentralized-knowledge-graph) or [Oxigraph](https://github.com/oxigraph/oxigraph), enhancing data management and advancing biological research capabilities. 📄🔍🌐\n\n## 🚀 How It Works\n\nThe BioAgent Plugin continuously monitors a specified Google Drive folder for new scientific documents. Upon detection, it automatically transforms these papers into structured Knowledge Assets and incorporates them into a dynamic knowledge graph. An intelligent agent explores this graph to systematically generate novel biological hypotheses across multiple domains, including genetics, molecular biology, and biotechnology. These hypotheses are then rigorously evaluated and scored by the integrated JudgeLLM according to a precise rubric, ensuring their accuracy and relevance. By default, this cycle of hypothesis generation and evaluation occurs every 90 seconds, but the frequency can be customized. 🔄🤖✨\n\n> [!IMPORTANT]\n> This plugin is currently under development and requires additional refinement before production use. Note that the Google Drive webhook functionality only works with publicly accessible URLs. For local development, we're using ngrok to create a temporary public endpoint for the webhook. In future versions, we plan to implement a simpler approach that directly monitors changes to a local folder.\n\n> [!NOTE]\n> This plugin is already added to the BioAgents repo here: https://github.com/bio-xyz/bioagents\n\n## 🛠 Getting Started\n\nFollow these steps to install and launch the BioAgent Plugin:\n\n### 1. Install System Dependencies\n\nEnsure the following dependencies are installed on your system:\n\n```bash\n# For Ubuntu/Debian\nsudo apt-get install ghostscript graphicsmagick\n```\n\n### 2. Clone the Repository\n\n```bash\ngit clone https://github.com/bio-xyz/plugin-bioagent.git\ncd plugin-bioagent\n```\n\n### 3. Install Project Dependencies\n\n```bash\npnpm install\n```\n\n### 4. Start Required Docker Containers\n\nStart PostgreSQL with pgvector extension for vector storage:\n\n```bash\ndocker run --name plugin-bioagent-postgres -e POSTGRES_PASSWORD=123 -p 5432:5432 -d pgvector/pgvector:pg17\n```\n\nStart Oxigraph RDF triple store or use OriginTrail's DKG:\n\n```bash\ndocker run --rm -v $PWD/oxigraph:/data -p 7878:7878 ghcr.io/oxigraph/oxigraph serve --location /data --bind 0.0.0.0:7878\n```\n\n### 5. Launch the Development Server\n\n```bash\npnpm run dev\n```\n\nThis command starts the development server at `http://localhost:3000`. Allow around 90 seconds for initial setup, after which the BioAgent Plugin will begin generating and evaluating hypotheses.\n\n## 🔧 Configure Environment Variables\n\nCopy and rename the example environment file:\n\n```bash\nmv .env.example .env\n```\n\n### Essential Environment Variables\n\nUpdate your `.env` file with the following variables:\n\n```env\nPOSTGRES_URL=postgresql://user:password@localhost:5432/dbname\nOPENAI_API_KEY=your_openai_api_key\nANTHROPIC_API_KEY=your_anthropic_api_key\nGCP_JSON_CREDENTIALS={\"type\": \"service_account\", \"project_id\": \"your_project_id\", ...}  # Your full GCP service account JSON\nGOOGLE_DRIVE_FOLDER_ID=your_google_drive_folder_id  # Google Drive folder ID for scientific papers\nDISCORD_APPLICATION_ID=your_discord_app_id\nDISCORD_API_TOKEN=your_discord_bot_token\nDISCORD_VOICE_CHANNEL_ID=your_voice_channel_id  # Optional\nDISCORD_CHANNEL_ID=your_text_channel_id\nBIONTOLOGY_KEY=your_biontology_api_key  # Obtain at https://bioportal.bioontology.org/accounts/new\n```\n\n## 📋 Obtaining Google Cloud Service Account JSON & Setting Permissions\n\n### Creating a Service Account\n\n1. Visit the [Google Cloud Console](https://console.cloud.google.com/).\n2. Select or create your desired project.\n3. Navigate to **APIs & Services** > **Credentials**.\n4. Click **+ CREATE CREDENTIALS**, then select **Service Account**.\n5. Provide a descriptive name for the service account and click **Create**.\n6. Assign necessary roles (e.g., Editor) and click **Continue**.\n7. Open the newly created service account, go to the **Keys** tab.\n8. Click **Add Key** > **Create new key**, choose **JSON**, and click **Create**. The JSON file will automatically download.\n\n### Granting Access to Google Drive Folder\n\n1. Open your [Google Drive](https://drive.google.com/).\n2. Navigate to the folder associated with the `GOOGLE_DRIVE_FOLDER_ID`.\n3. Right-click the folder and select **Share**.\n4. Enter the service account email (available in your downloaded JSON file) into the sharing field.\n5. Set permissions (\"Editor\" or \"Viewer\") accordingly and click **Send**.\n\nYour Google Cloud service account now has access to the specified folder. 📁🔑✅\n"
    },
    {
      "path": "drizzle.config.ts",
      "detailed_description": "该文件 drizzle.config.ts 是一个用于配置 Drizzle ORM 的 TypeScript 配置文件，主要用于 PostgreSQL 数据库的连接和操作。它首先导入了 Drizzle Kit 的 Config 类型以及 dotenv 库，以便从环境变量中读取数据库连接信息。配置对象中定义了数据库架构的位置 schema，指定了输出目录 out，设置了数据库方言 dialect 为 'postgresql'，以及数据库凭证 dbCredentials，其中包含从环境变量 POSTGRES_URL 读取的数据库连接 URL。schemaFilter 用于过滤需要处理的架构，这里只包含 'biograph'，而 migrations 则指定了迁移文件将归属于 'biograph' 架构。通过这种方式，项目能够灵活地管理数据库结构和迁移，确保数据库的一致性和可维护性。",
      "summary": "该文件用于配置 Drizzle ORM 以连接和管理 PostgreSQL 数据库的架构和迁移。",
      "raw": "import type { Config } from \"drizzle-kit\";\nimport \"dotenv/config\";\n\nexport default {\n  schema: \"./src/db/schemas/*\",\n  out: \"./drizzle\",\n  dialect: \"postgresql\",\n  dbCredentials: {\n    url: process.env.POSTGRES_URL!,\n  },\n  schemaFilter: [\"biograph\"],\n  migrations: {\n    schema: \"biograph\",\n  },\n} satisfies Config;\n"
    },
    {
      "path": "tsup.config.ts",
      "detailed_description": "tsup.config.ts 是一个用于配置 tsup 打包工具的文件，主要负责设置 TypeScript 项目的构建过程。文件通过 import 语句引入 defineConfig 函数来定义构建配置。entry 指定了项目的入口文件为 src/index.ts，而 outDir 则设定了构建输出目录为 dist。tsconfig 属性引用了特定的 TypeScript 配置文件（tsconfig.build.json），以确保构建过程使用适当的编译选项。sourcemap 为 true，意味着生成源映射文件，有助于调试。clean 为 true，表示在构建前清理输出目录。format 被设置为 ['esm']，确保生成的代码适用于 ES 模块系统。同时，dts 被设定为 false，以避免生成类型声明文件，从而避免外部导入的问题。external 列表定义了哪些模块不应被打包，而是作为外部模块使用，确保在运行时使用 Node.js 内置模块（如 fs、path、http、https）和其他特定的外部库（如 dotenv、@elizaos/core、zod）。这一配置文件对于确保项目在构建时的灵活性和可维护性至关重要。",
      "summary": "tsup.config.ts 是该项目的 TypeScript 打包配置文件，用于定义构建过程的入口、输出目录、源映射和外部依赖等设置。",
      "raw": "import { defineConfig } from 'tsup';\n\nexport default defineConfig({\n  entry: ['src/index.ts'],\n  outDir: 'dist',\n  tsconfig: './tsconfig.build.json', // Use build-specific tsconfig\n  sourcemap: true,\n  clean: true,\n  format: ['esm'], // Ensure you're targeting CommonJS\n  dts: false, // Skip DTS generation to avoid external import issues // Ensure you're targeting CommonJS\n  external: [\n    'dotenv', // Externalize dotenv to prevent bundling\n    'fs', // Externalize fs to use Node.js built-in module\n    'path', // Externalize other built-ins if necessary\n    'https',\n    'http',\n    '@elizaos/core',\n    'zod',\n  ],\n});\n"
    },
    {
      "path": "analysis_report\\report_20250425_114245.md",
      "detailed_description": "文件 src/services/kaService/vectorize.ts 主要用于生成科学论文的摘要，通过与大型语言模型（LLM）进行交互以获取论文元数据的总结。文件定义了几个接口，包括 Graph、CitationEntry 和 SimilarCitationResult，分别用于描述图对象、单个引用条目和相似引用的结果结构。核心功能是 getSummary 函数，该函数接收一个 LLM 客户端和一个包含论文元数据的图对象作为参数。首先，它通过调用 get_prompt_vectorization_summary 函数创建适合 LLM 的提示，以便生成高质量的摘要。接着，使用 generateResponse 函数与 LLM 进行交互，获取生成的摘要内容。如果摘要生成成功，函数将记录摘要内容，并在过程中如发生异常，将捕获错误并记录相关信息，确保在运行时的可追踪性和可靠性。整体而言，该文件通过自动化生成论文摘要，增强了科学研究数据的可读性和可理解性，支持科研人员更高效地获取和利用文献信息。",
      "summary": "该文件用于生成科学论文的摘要，通过与大型语言模型进行交互以提取论文元数据的总结。",
      "raw": "# Project Analysis Report\n\n**Generated on**: 20250425_114245\n\n## Directory Structure\n```\n- .env.example\n- .gitignore\n- README.md\n- dist/\n  - index.js\n  - index.js.map\n- drizzle.config.ts\n- images/\n  - banner.jpg\n  - logo.jpg\n- src/\n  - actions/\n    - dkgInsert.ts\n    - index.ts\n  - constants.ts\n  - db/\n    - index.ts\n    - migration.ts\n    - schemas/\n      - customTypes.ts\n      - driveSync.ts\n      - fileMetadata.ts\n      - gdriveChannels.ts\n      - hypotheses.ts\n      - hypothesesSummary.ts\n      - index.ts\n  - evaluators/\n    - evaluationPrompt.ts\n    - index.ts\n    - types.ts\n  - helper.ts\n  - index.ts\n  - routes/\n    - controller.ts\n    - gdrive/\n      - index.ts\n      - manualSync.ts\n      - webhook.ts\n    - health.ts\n    - index.ts\n  - services/\n    - anthropic/\n      - chooseTwoRelevant.ts\n      - client.ts\n      - discordSplitter.ts\n      - errors.ts\n      - evaluateHypothesis.ts\n      - generateHypothesis.ts\n      - hypGenEvalLoop.ts\n      - sparql/\n        - makeRequest.ts\n        - queries.ts\n      - types.ts\n    - gdrive/\n      - buildQuery.ts\n      - client.ts\n      - extract/\n        - README.md\n        - config.ts\n        - index.ts\n        - prompts.ts\n        - types.ts\n        - z.ts\n      - index.ts\n      - storeJsonLdToKg.ts\n      - watchFiles.ts\n    - index.ts\n    - kaService/\n      - anthropicClient.ts\n      - biologyApi.ts\n      - downloadPaper.ts\n      - exampleForPrompts.ts\n      - kaService.ts\n      - llmPrompt.ts\n      - processPaper.ts\n      - regex.ts\n      - sparqlQueries.ts\n      - unstructuredPartitioning.ts\n      - vectorize.ts\n  - templates.ts\n  - types.ts\n- tsup.config.ts\n```\n\n## File Analysis\n\n### .env.example\n\n**Summary**: 该文件提供了应用程序在不同环境中所需的环境变量配置示例，包括数据库连接、API 密钥和其他服务的配置。\n\n**Detailed Description**:\n.env.example 文件是一个环境变量配置示例文件，主要用于存储应用程序在不同运行环境中所需的配置参数和密钥。该文件包含了多个部分，首先是针对生产和开发环境的 URL 配置，允许开发者在本地运行时通过 ngrok 获取一个临时的外部 URL。接着，文件定义了数据库连接字符串和相关的密码信息，确保应用程序能够安全地连接到 PostgreSQL 数据库。在日志记录方面，该文件提供了默认的日志级别和 JSON 格式的选项，帮助开发者管理和调试应用。文件还包括 Discord 相关的配置，允许与 Discord API 交互的必要凭证，如应用 ID 和 API 令牌。接下来是针对模型提供者的配置，包括 OpenAI 和 Anthropic 的 API 密钥，这些密钥使得应用能够访问各自的 AI 服务。最后，文件中还包含了与 DKG（去中心化知识图谱）相关的设置和其他 API 密钥的配置，如 Unstructured API 和 Biontology API 的密钥，确保应用可以与这些服务进行交互。整体而言，该文件为开发和部署提供了必要的配置模板，有助于确保应用在不同环境中正常运行。\n\n### .gitignore\n\n**Summary**: .gitignore 文件用于指定在 Git 版本控制中应忽略的文件和目录。\n\n**Detailed Description**:\n.gitignore 文件用于指定 Git 在版本控制中应忽略的文件和目录。通过使用此文件，开发者可以确保不必要的文件（如依赖项、临时文件、环境变量文件和特定于开发环境的配置）不会被提交到代码库中，从而保持代码库的整洁和安全。文件中列出的条目包括 'node_modules'，这是 Node.js 项目中安装的依赖项目录，通常不需要跟踪；'.env'，用于存储环境变量的文件，包含敏感信息；'.migration-complete'，可能是一个标识迁移是否完成的临时文件；'drizzle'，可能指与数据库或迁移相关的生成文件；'*.lock'，通常是包管理工具生成的锁定文件，用于确保依赖版本一致性；'*.json' 和 '*.yaml'，这些文件可能包含配置或数据，但在某些情况下可能不需要版本控制；以及 '.git' 目录本身，这是 Git 的内部目录。通过提供这种忽略机制，.gitignore 文件帮助开发者专注于版本控制的核心内容，避免泄露敏感信息，并减少无关文件的干扰。\n\n### README.md\n\n**Summary**: README.md 文件介绍了 BioAgent 插件的功能、使用方法及安装配置步骤，旨在支持科学论文的结构化知识资产生成和生物假设的自动化评估。\n\n**Detailed Description**:\nREADME.md 文件为 BioAgent 插件提供了全面的介绍和使用说明。该插件专为研究人员和生物科学家设计，旨在将科学论文转换为结构化的知识资产，并将其集成到 RDF 三元组存储中，如 OriginTrail 的去中心化知识图谱和 Oxigraph。这份文档详细描述了插件的工作原理，包括如何监控 Google Drive 文件夹中的新文件，并自动将这些文件转换为知识资产，进而通过智能代理探索知识图谱生成新的生物假设，并使用 JudgeLLM 对这些假设进行评估和打分。文中还说明了安装和配置的步骤，包括系统依赖项、克隆代码库、安装项目依赖关系、启动 Docker 容器以及配置环境变量的详细指导。此外，README.md 还提供了如何创建 Google Cloud 服务账户和授予其访问 Google Drive 文件夹的步骤，以确保插件能正常工作。该插件目前仍在开发中，用户需留意其功能的完善与改进。\n\n### drizzle.config.ts\n\n**Summary**: 该文件用于配置 Drizzle ORM 以连接和管理 PostgreSQL 数据库，包括模式、输出路径和凭据等设置。\n\n**Detailed Description**:\ndrizzle.config.ts 是一个配置文件，主要用于设置 Drizzle ORM（对象关系映射）框架的参数，以便于与 PostgreSQL 数据库进行交互。该文件导入了 Drizzle 的类型定义，并通过 dotenv 库加载环境变量，以确保应用程序能够安全地访问数据库凭据。具体来说，配置的 schema 属性指定了数据库模式文件的位置，out 属性指明了生成的类型定义文件的输出目录，dialect 属性则设置数据库的类型为 PostgreSQL。dbCredentials 里定义了数据库的连接 URL，利用 process.env.POSTGRES_URL 读取环境变量中的数据库连接信息，确保灵活性和安全性。schemaFilter 允许指定仅包含特定模式（在此示例中为 'biograph'）的数据库表，而 migrations 属性则指明了用于数据库迁移的模式。整体上，该文件的设计旨在简化与 PostgreSQL 数据库的交互，提供一种结构化的方式来管理数据库模式和迁移。\n\n### tsup.config.ts\n\n**Summary**: tsup.config.ts 文件用于配置 TypeScript 项目的构建过程，指定入口、输出格式和外部依赖。\n\n**Detailed Description**:\ntsup.config.ts 是一个配置文件，用于定义 TypeScript 项目的构建选项和输出行为。该文件使用 tsup 工具，它是一个快速的 TypeScript 打包工具，旨在简化构建过程。文件中指定了入口文件为 src/index.ts，构建输出目录为 dist，确保生成的 JavaScript 文件符合 ES 模块格式。通过 tsconfig 字段引用了特定的 TypeScript 配置（tsconfig.build.json），以适应构建需求。源映射（sourcemap）功能被启用，便于调试生成的代码。同时，clean 选项被设置为 true，以便在每次构建前清理输出目录，确保构建的干净性。此外，外部化了一些依赖模块（如 dotenv、fs、path、http、https 等），以避免将它们打包到最终构建中。这保证了在运行时可以使用 Node.js 的内置模块以及其他外部库，而不会导致重复打包的问题。通过这种配置，tsup 提供了高效、可定制的构建流程，适合用于现代 TypeScript 项目。\n\n### dist\\index.js\n\n**Summary**: 该文件实现了一个与OriginTrail去中心化知识图交互的插件，允许用户将科学研究成果存储为知识资产。\n\n**Detailed Description**:\n该文件是一个服务和插件的实现，旨在与OriginTrail去中心化知识图（DKG）进行交互。它通过定义一个名为'dkgInsert'的操作，允许用户将科学论文的知识资产存储到DKG中。文件逻辑首先验证所需的环境变量是否存在，然后初始化DKG客户端。接着，通过读取PDF文件并提取文档内容，生成知识资产，并在DKG中创建相应的条目。插入操作会记录每个消息，并在成功插入后生成一个包含新条目链接的响应。此外，还包括错误处理和日志记录，以确保在进行网络请求或数据库操作时能捕获异常和状态信息。这个文件的用途是使得用户能够将科学研究成果文档化并通过去中心化的方式进行存储和管理，从而促进科研数据的共享和传播。\n\n### dist\\index.js.map\n\n**Summary**: 该文件用于监控 Google Drive 文件夹的变化，下载新文件，并处理相关的文件元数据。\n\n**Detailed Description**:\n该文件实现了与 Google Drive 的交互，主要用于监控和处理 Google Drive 文件的变化。它通过 OAuth 进行身份验证，连接到 Google Drive API，以便提取文件信息并下载 PDF 文件。文件中定义了多个函数，包括初始化 Google Drive 客户端、下载文件、获取文件信息及监视文件夹变化的功能。具体而言，`watchFolderChanges` 函数持续检查指定的 Google Drive 文件夹，查找新文件或已修改的文件，并在发现变化时进行处理。新文件会被下载并进一步处理，例如提取相关信息或存储到数据库。此外，文件中还实现了一些用于处理文件元数据的方法，如通过 MD5 校验和确保文件的唯一性和完整性。整体逻辑通过异步操作确保实时响应，适用于需要实时同步文件和数据的应用场景。\n\n### images\\banner.jpg\n\n**Summary**: images/banner.jpg 是用于增强 BioAgent 插件视觉效果的图像文件。\n\n**Detailed Description**:\n文件 images/banner.jpg 是一个图像文件，通常用于增强用户界面或文档的视觉吸引力。在 BioAgent 插件中，该图像可能用作应用程序的横幅或品牌标识，显示在用户界面中以提升用户体验。图像文件的内容不能直接读取，因为它是二进制文件，而非文本文件。正确的用途是将其嵌入到网页或应用程序中，提供视觉信息或标识。由于该图像的读取错误，可能表明文件损坏或格式不兼容，因此在使用时需要确保文件的完整性和正确性。\n\n### images\\logo.jpg\n\n**Summary**: images/logo.jpg 是 BioAgent 插件的品牌标志图像，用于提升用户界面视觉效果。\n\n**Detailed Description**:\nimages/logo.jpg 是 BioAgent 插件的标志性图像文件，通常用于在用户界面中展示品牌形象。该文件可能被用于应用程序的主界面、文档或营销材料中，以增强用户体验和视觉识别。虽然当前读取该文件时出现编码错误，但通常情况下，logo.jpg 文件应包含一个 JPEG 格式的图像，能够被大多数图像查看工具和网页浏览器解析。在软件开发中，logo 文件不仅是视觉元素，还可以传达品牌价值和专业形象，对于最终用户的第一印象至关重要。因此，确保该文件的有效性和可访问性对 BioAgent 插件的整体表现和用户满意度都有重要影响。\n\n### src\\constants.ts\n\n**Summary**: 该文件定义了社交媒体发布的结构化数据模板和相关 SPARQL 查询，用于与去中心化知识图交互。\n\n**Detailed Description**:\n文件 src/constants.ts 定义了一组常量，用于生成和执行与社交媒体发布相关的结构化数据和 SPARQL 查询。首先，dkgMemoryTemplate 是一个 JSON-LD 格式的模板，表示社交媒体发布内容，并包含作者、创建日期、用户交互统计、关键词和相关主题等信息。该模板可以用于描述在去中心化知识图（DKG）中存储的社交媒体内容。接下来，combinedSparqlExample 和 sparqlExamples 是一系列 SPARQL 查询示例，旨在从知识图中检索社交媒体发布的标题和正文，支持通过关键词和主题过滤结果。这些查询利用了 Schema.org 定义的社交媒体发布类型，并通过 LCASE 函数和包含过滤器确保对关键字的灵活匹配。generalSparqlQuery 是一个基本的查询，用于检索所有社交媒体发布的标题和正文，限制结果为 10 条。最后，DKG_EXPLORER_LINKS 提供了去中心化知识图的测试网和主网探索链接，便于开发者和用户访问和浏览存储在知识图中的数据。整体上，该文件为项目中社交媒体数据的表示和查询提供了基础结构和示例，支持与 DKG 进行交互和数据检索。\n\n### src\\helper.ts\n\n**Summary**: 该文件用于初始化数据库迁移和谷歌驱动同步，确保程序启动时环境的正确配置。\n\n**Detailed Description**:\n文件 src/helper.ts 主要负责在程序初始化时执行数据库迁移和谷歌驱动同步的设置。首先，它导入了必要的模块和类型，包括日志记录和数据库操作函数。initWithMigrations 函数首先调用 migrateDb() 函数执行数据库迁移，以确保数据库结构是最新的。如果迁移成功，它将继续调用 initDriveSync 函数来初始化谷歌驱动同步。在 initDriveSync 函数中，首先从数据库中查询 driveSyncTable，检查是否已有同步条目。如果没有，它会创建一个新的驱动同步。为此，它初始化谷歌驱动客户端，使用 ListFilesQueryContext 构造一个查询上下文，通过获取起始页面令牌来跟踪谷歌驱动中的文件变化。最后，它将新的驱动同步信息插入数据库中，包括驱动 ID 和起始页面令牌。此文件的主要目的是确保在应用程序启动时，数据库和谷歌驱动的状态是同步的，避免重复初始化，从而实现文件管理的高效性。\n\n### src\\index.ts\n\n**Summary**: 该文件定义并初始化了与 OriginTrail 知识图交互的 dkg 插件，支持记忆存储和数据同步功能。\n\n**Detailed Description**:\n文件 src/index.ts 定义了一个名为 dkgPlugin 的插件，它用于与 OriginTrail 去中心化知识图交互，以实现对记忆的存储。此插件通过实现 Plugin 接口，包含初始化方法、描述信息、操作、服务和路由。初始化方法中，插件首先记录初始化信息并输出配置，然后延迟 20 秒后调用 initWithMigrations 函数以确保数据库迁移的正确执行，避免因为数据库属性尚不可用而导致的错误。此外，该插件还引入了 dkgInsert 动作，HypothesisService 服务，以及健康检查和 Google Drive 的 Webhook 和手动同步路由，使得插件具有丰富的功能和可扩展性。整体上，该文件为 BioAgent 插件的核心功能提供了结构和逻辑支持，是实现科学研究成果结构化存储的关键组成部分。\n\n### src\\templates.ts\n\n**Summary**: 该文件用于定义生成 JSON-LD 结构化内存对象和提取科学论文 URLs 的模板字符串。\n\n**Detailed Description**:\n文件 src/templates.ts 定义了两个主要的模板字符串，用于生成结构化的 JSON-LD 对象和提取科学论文 URLs。第一个模板 createDKGMemoryTemplate 旨在为 AI 代理创建一个结构化的内存 JSON-LD 对象，描述了如何从用户查询和上下文中提取相关信息来填充 JSON-LD 模板。该模板包括具体的步骤和字段定义，指导如何识别主要思想、存储原始帖子、获取作者信息、提取关键词以及确保所有字段符合 schema.org 本体。第二个模板 extractScientificPaperUrls 则专注于从用户输入中提取科学论文的 URLs，并按照给定的 Zod schema 结构化输出。这一过程包括验证 URL 的有效性，并将其组织成 JSON 格式。如果未能找到有效的 URL，将返回一个空数组。整体上，这个文件为应用中的信息提取和结构化过程提供了清晰的指引和标准化的输出格式，确保数据的一致性与可用性。\n\n### src\\types.ts\n\n**Summary**: src/types.ts 文件定义了与 DKG 交互的数据结构和类型验证逻辑，确保数据的一致性和安全性。\n\n**Detailed Description**:\nsrc/types.ts 文件主要定义了与 DKG（去中心化知识图）交互相关的 TypeScript 类型和模式验证逻辑。它使用 Zod 库来创建和验证复杂的数据结构，包括 DKGMemorySchema 和 DKGSelectQuerySchema。DKGMemorySchema 描述了一种社交媒体发布的结构化数据格式，包含多个属性，如 '@context'、'@type'、'headline'、'articleBody' 以及与之相关的关键词和主题。通过定义这些模式，代码确保传输的数据符合预期结构，从而提高了数据的可靠性和一致性。此外，文件还定义了 DKGSelectQuerySchema，用于验证以 'SELECT' 开头的查询字符串。为了便于使用，使用 z.infer 提取了这些模式的类型，形成 DKGMemoryContent 和 DKGSelectQuery 的 TypeScript 类型。该文件还提供了两个类型保护函数 isDKGMemoryContent 和 isDKGSelectQuery，以安全地检查某个对象是否符合定义的模式，增强了类型安全性和代码的健壮性。最后，文件还定义了 scientificPaperUrlsSchema，用于验证科学论文 URL 的数组，进一步扩展了数据验证的功能。整体上，该文件在项目中扮演着确保数据结构一致性和有效性的重要角色，为其他模块提供了必要的类型支持和验证逻辑。\n\n### src\\actions\\dkgInsert.ts\n\n**Summary**: 该文件实现了将用户消息创建为去中心化知识图记忆的功能。\n\n**Detailed Description**:\n文件 dkgInsert.ts 实现了一个名为 'INSERT_MEMORY_ACTION' 的操作，允许在 OriginTrail 去中心化知识图上创建一个新的记忆。该操作使用 dotenv 加载环境变量，确保所需的配置（例如 DKG 环境、主机名、端口和区块链凭证）在运行时可用。函数 validate 会检查这些环境变量是否存在，如果缺少任何变量，则返回错误并记录缺失信息。在 handler 函数中，首先初始化 DKG 客户端，并通过正则表达式提取当前帖子中的 Twitter 用户名和 ID。接着，调用 generateKaFromPdf 函数生成知识资产，并尝试创建一个新的资产，将其发布到 DKG 上。成功后，记录创建结果，并通过回调函数回复用户，提供一个指向新创建记忆的链接。该文件的主要作用是将用户的消息转换为知识资产存储到去中心化知识图中，并提供相应的反馈。\n\n### src\\actions\\index.ts\n\n**Summary**: 该文件聚合并导出 dkgInsert.ts 中的所有功能，以简化模块的使用和导入。\n\n**Detailed Description**:\n文件 src/actions/index.ts 的主要功能是作为一个聚合模块，将其他模块的导出集合在一起。在这个特定的文件中，它只导出了 dkgInsert.ts 中的所有内容。这种结构使得对外部使用者来说，访问与去中心化知识图 (DKG) 插件相关的操作变得更加简洁和方便。通过集中导出，可以减少导入路径的复杂性，提升代码的可读性和可维护性。当其他模块需要使用 dkgInsert.ts 中定义的功能时，只需引入这个 index.ts 文件即可，无需单独导入 dkgInsert.ts。这种设计模式在大型代码库中尤其有用，有助于组织代码并保持模块之间的清晰结构。\n\n### src\\db\\index.ts\n\n**Summary**: Error: Could not generate summary for C:\\Users\\uizor\\Desktop\\plugin-bioagent\\src\\db\\index.ts\n\n**Detailed Description**:\nError: Failed to analyze C:\\Users\\uizor\\Desktop\\plugin-bioagent\\src\\db\\index.ts: Expecting ',' delimiter: line 2 column 392 (char 393)\n\n### src\\db\\migration.ts\n\n**Summary**: 该文件负责管理数据库迁移，确保在应用启动时正确执行数据库结构的更新。\n\n**Detailed Description**:\n该文件包含了用于管理数据库迁移的逻辑，使用 Drizzle ORM 连接 PostgreSQL 数据库。首先，文件定义了两个辅助函数，getMigrationFlag 和 setMigrationFlag，分别用于检查和设置迁移标志文件，以指示迁移是否已经执行。核心功能是 migrateDb 函数，它会在启动时检查是否已经执行过迁移，若未执行且环境变量 POSTGRES_URL 设置正确，则会创建数据库架构并运行指定的迁移脚本。迁移完成后，函数会创建一个标志文件以避免重复执行迁移，并在成功或失败时记录相关日志信息。该文件确保数据库结构的正确性和版本管理，支持插件的稳定运行。\n\n### src\\db\\schemas\\customTypes.ts\n\n**Summary**: 该文件定义了与假设状态和 Google Drive 类型相关的 PostgreSQL 枚举类型，以增强数据库的类型安全性。\n\n**Detailed Description**:\n文件 src/db/schemas/customTypes.ts 定义了两个 PostgreSQL 枚举类型，分别为 hypothesisStatusEnum 和 driveTypeEnum。这些枚举类型是通过 drizzle-orm 库中的 pgEnum 函数创建的，旨在为数据库中的相关字段提供类型安全。hypothesisStatusEnum 枚举包括 'pending'、'approved' 和 'rejected' 三个值，用于表示假设的状态，确保在数据库操作中只能使用这些特定的状态值，从而维护数据的一致性。driveTypeEnum 枚举则包括 'shared_folder' 和 'shared_drive'，用于标识 Google Drive 中的文件类型。这些枚举的使用能够提高代码的可读性和可维护性，因为它们限制了特定字段的值范围，减少了潜在的错误和不一致性，并且在进行数据验证和查询时提供了更强的语义。\n\n### src\\db\\schemas\\driveSync.ts\n\n**Summary**: 该文件定义了用于存储 Google Drive 同步信息的数据库表结构。\n\n**Detailed Description**:\n文件 src/db/schemas/driveSync.ts 定义了用于 PostgreSQL 数据库的 'drive_sync' 表的模式。这是通过使用 Drizzle ORM 来实现的，该 ORM 提供了一种更简洁的方式来与数据库交互。此表包含四个主要字段：'id' 是主键，使用文本类型并且不能为空；'startPageToken' 也是文本类型，用于存储与 Google Drive 同步过程相关的起始页面令牌，确保在每次同步时能够正确跟踪文件的状态；'driveType' 字段使用定义的枚举类型 'driveTypeEnum'，指示与 Google Drive 相关的文件类型，确保数据的类型安全性；最后，'lastSyncAt' 字段是一个带时区的时间戳，记录最后一次同步的时间，默认值为当前时间。这些字段的设计保证了在进行 Google Drive 文件同步时，能够有效地存储和追踪同步状态及相关信息。\n\n### src\\db\\schemas\\fileMetadata.ts\n\n**Summary**: 该文件定义了一个用于存储文件元数据的 PostgreSQL 数据库表及其类型定义。\n\n**Detailed Description**:\n该文件定义了一个名为 `file_metadata` 的数据库表，该表用于存储与文件相关的元数据。通过使用 Drizzle ORM 的 `pg-core` 库，文件中创建了一个 PostgreSQL 模式 `biograph`，并在其中定义了多个字段，包括 `id`、`hash`、`fileName`、`fileSize`、`createdAt` 和 `modifiedAt`。其中，`id` 是一个文本类型的非空字段，`hash` 是主键，确保每个文件的唯一性。`fileName` 记录文件的名称，`fileSize` 存储文件的大小，使用 `bigint` 类型以支持较大的数值。`createdAt` 和 `modifiedAt` 字段用于跟踪文件的创建和最后修改时间，这两个字段都设置为非空，并默认值为当前时间（使用 `defaultNow()` 方法）。此外，`tags` 字段是一个文本数组，用于存储与文件相关的标签信息，这可以帮助在后续操作中对文件进行分类和检索。文件末尾还导出了两个 TypeScript 类型：`FileMetadata` 用于选择数据时的类型验证，`NewFileMetadata` 用于插入新记录时的类型验证。通过这种方式，该文件提供了数据库结构的类型安全性和一致性，确保在与数据库交互时，数据的格式和类型是正确的。\n\n### src\\db\\schemas\\gdriveChannels.ts\n\n**Summary**: 该文件定义了用于存储 Google Drive 频道信息的数据库表结构。\n\n**Detailed Description**:\n文件 src/db/schemas/gdriveChannels.ts 负责定义与 Google Drive 频道相关的数据库表结构。该文件使用 Drizzle ORM 的 pg-core 模块来创建一个 PostgreSQL 表，表名为 'gdrive_channels'，属于 'biograph' 模式。表中包含多个字段，包括 'kind'（文本类型），'id'（文本类型，非空且为主键），'resource_id'（文本类型，非空），'resource_uri'（文本类型），'expiration'（时间戳类型，非空且带有时区），以及 'createdAt'（时间戳类型，默认值为当前时间）。这些字段的设计旨在存储 Google Drive 资源的元数据，包括其类型、标识符、URI 和过期时间等信息。通过这种结构，应用程序能够有效地管理和查询与 Google Drive 频道相关的信息，从而支持更复杂的同步和数据交互逻辑。\n\n### src\\db\\schemas\\hypotheses.ts\n\n**Summary**: 该文件定义了 PostgreSQL 中用于存储假设信息的表结构及其相关字段和类型。\n\n**Detailed Description**:\n该文件定义了一个用于存储假设信息的 PostgreSQL 数据库表结构，属于 Drizzle ORM 的实现。它首先导入了必要的模块，包括 UUID、文本、时间戳和数字等类型，这些类型在定义表结构时被用作字段的数据类型。它创建了一个名为 'hypotheses' 的表，表中包含多个字段，包括 'id'（主键，UUID 类型），'hypothesis'（文本类型，必填），'filesUsed'（文本数组，用于存储与假设相关的文件），'status'（使用自定义枚举类型，默认值为 'pending'），'judgellmScore' 和 'humanScore'（数字类型，使用精度和小数位数限制），以及 'research'、'evaluation' 和 'citations'（文本类型，部分字段为数组）。此外，还定义了 'createdAt' 和 'updatedAt' 字段，用于跟踪记录的创建和更新时间，确保其具有时间戳功能。最后，文件导出了两个类型，'Hypothesis' 和 'NewHypothesis'，分别用于推断对该表的查询和插入操作的类型，以便 TypeScript 在进行数据库操作时提供类型安全和智能提示。\n\n### src\\db\\schemas\\hypothesesSummary.ts\n\n**Summary**: 该文件用于定义 PostgreSQL 数据库中 'hypotheses_summary' 表的结构，以存储假设的摘要信息及相关元数据。\n\n**Detailed Description**:\n该文件定义了一个名为 'hypotheses_summary' 的数据库表结构，用于存储与假设相关的摘要信息。通过使用 Drizzle ORM 的 pg-core 模块，文件首先导入必要的类型和函数，并定义了一个名为 'biograph' 的 PostgreSQL 模式。接着，'hypotheses_summary' 表包含多个字段，包括：唯一标识符 'id'（UUID 类型，主键，自动生成），'hypothesisId'（与 'hypotheses' 表的外键关联，表示相关假设的 ID），'summary'（文本类型，存储假设的摘要），'keywords'（文本数组，存储相关关键词），'scientificEntities'（文本数组，存储与假设相关的科学实体），'createdAt' 和 'updatedAt'（时间戳类型，记录创建和更新的时间，均带有时区信息，并设定默认值为当前时间）。此外，该文件还导出两个类型：'HypothesesSummary' 和 'NewHypothesesSummary'，分别用于推断查询和插入操作的数据结构。这种设计确保了数据的结构化存储和类型安全性，有助于后续对假设摘要进行管理和检索。\n\n### src\\db\\schemas\\index.ts\n\n**Summary**: 该文件集中导出多个与数据库模式相关的模块，以简化导入过程和提高代码的可维护性。\n\n**Detailed Description**:\n文件 src/db/schemas/index.ts 用于集中导出与数据库模式相关的多个模块，以简化其他文件的导入过程。通过使用 ES6 的模块导出语法，该文件将 fileMetadata.ts、hypotheses.ts、customTypes.ts 和 driveSync.ts 中定义的所有内容重新导出，使得其他模块可以通过单一入口轻松访问这些数据库表结构及其相关类型定义。这种结构化的方式提高了代码的可维护性和可读性，避免了重复导入的繁琐，同时也使得在未来添加或修改数据库模式时，开发者可以只需在此文件中进行处理，而不必追踪到每一个具体的模块。这种模块化的设计符合现代 JavaScript 和 TypeScript 的最佳实践，有助于提升代码的组织性和模块间的清晰度。\n\n### src\\evaluators\\evaluationPrompt.ts\n\n**Summary**: 该文件提供了一个系统化的框架，用于评估科学假设的质量，包含信息收集和评分的提示。\n\n**Detailed Description**:\n文件 src/evaluators/evaluationPrompt.ts 定义了一组用于评估科学假设的提示字符串，旨在指导模型在评估过程中使用特定的标准和逻辑。该文件包含三个主要的提示：evaluationPrompt、stepOnePrompt 和 stepTwoPrompt。evaluationPrompt 提供了一个过时的评估框架，要求评估者考虑假设的清晰性、证据一致性、逻辑一致性、可预测性、可证伪性以及新颖性等六个标准，并对假设进行评分。stepOnePrompt 则引导评估者在正式评估之前，进行相关信息的调查和总结，强调要识别假设中的未知因素、潜在的研究缺口及相关文献。stepTwoPrompt 则详细说明了评估过程，指示评估者如何基于步骤一的结果进行评分，确保评估的规范性和科学性。整个文件的目的是提供一种系统化的评估框架，以帮助用户在科学研究中准确地分析和判断假设的有效性和重要性。\n\n### src\\evaluators\\index.ts\n\n**Summary**: 该文件实现了一个评估器，用于评估去中心化知识图的质量，提供基础的验证和处理逻辑。\n\n**Detailed Description**:\n文件 src/evaluators/index.ts 定义了一个名为 dkgEvaluator 的评估器对象，该对象用于评估去中心化知识图（DKG）的质量。它实现了 Evaluator 接口，包含多个属性和方法。属性 name 和 description 提供了该评估器的名称和描述，similes 列出了与该评估器相关的同义词，方便在系统中进行识别。该评估器的 validate 方法是一个异步函数，接受 IAgentRuntime 和 Memory 类型的参数，返回一个 Promise，表示是否通过验证，但当前实现始终返回 true，表明所有消息都被视为有效。handler 方法也是一个异步函数，负责处理评估逻辑，它接收相同的参数并在日志中记录一条信息，表明评估器正在执行。该设计为未来的扩展提供了灵活性，允许在 validate 和 handler 方法中实现更复杂的评估逻辑和条件。整体上，dkgEvaluator 旨在为 DKG 的操作提供质量评估的基础架构，确保数据的可靠性和准确性。\n\n### src\\evaluators\\types.ts\n\n**Summary**: 该文件定义了一个用于存储科学假设评估结果的 TypeScript 类型 'EvaluationResult'。\n\n**Detailed Description**:\n文件 src/evaluators/types.ts 定义了一个 TypeScript 类型 'EvaluationResult'，用于结构化存储科学假设评估的结果。该类型包含三个主要部分：'stepOne'、'stepTwo' 和 'score'。'stepOne' 包括 'research' 字段，记录具体的研究内容，以及 'timestamp' 字段，表示评估开始的时间；'stepTwo' 则包含 'evaluation' 字段，记录对假设的评价结果，以及 'timestamp' 字段，用于标记评价完成的时间。最后，'score' 字段提供一个字符串，表示评估的分数或结果。这种结构化的定义使得评估结果的存储和传递更加清晰，方便在项目其他模块中使用，支持后续的分析和决策过程。\n\n### src\\routes\\controller.ts\n\n**Summary**: 该文件用于同步 Google Drive 文件夹的变更并更新相关文件元数据到数据库。\n\n**Detailed Description**:\n该文件实现了一个名为 syncGoogleDriveChanges 的异步函数，负责处理 Google Drive 文件夹的变更。首先，它从数据库中获取驱动同步数据，并确保已经初始化驱动同步记录。如果没有记录，函数将抛出错误。接着，函数初始化 Google Drive 客户端并配置 API 请求参数，以便调用 Google Drive 的 changes.list 方法获取自上次同步以来的文件变更。处理变更时，函数会根据文件的状态（如被删除、移动到垃圾箱或是新文件/修改过的文件）采取相应的措施。其中，对于新上传或修改的 PDF 文件，函数将其元数据（如文件名、大小和哈希值）存储到数据库中，并在文件被移动到垃圾箱时从数据库中删除其记录。最后，它更新数据库中的开始页面令牌，以便在下次同步时使用。该文件通过详细的日志记录过程中的关键操作，以便进行调试和监控。\n\n### src\\routes\\health.ts\n\n**Summary**: 该文件实现了一个健康检查路由，返回服务状态的简单确认信息。\n\n**Detailed Description**:\n文件 src/routes/health.ts 定义了一个健康检查路由，采用 GET 请求方法。当客户端发送请求到 '/health' 路径时，服务器将返回一个 JSON 响应，内容为 { message: 'OK' }。这个路由的主要目的是提供服务的健康状态检查，常用于监控和负载均衡。通过这个端点，系统管理员或监控工具可以快速确认服务是否正常运行。此功能在微服务架构中尤为重要，因为它确保各个服务之间的可用性和响应能力。代码逻辑简单明了，使用了异步处理以应对可能的扩展性需求，虽然在当前实现中并没有涉及到复杂的逻辑或错误处理。整体上，该文件为系统提供了一个基本的健康检查机制，确保开发和运维团队能够及时了解服务状态。\n\n### src\\routes\\index.ts\n\n**Summary**: 该文件聚合并导出与 Google Drive 和健康检查相关的路由功能，简化了路由管理。\n\n**Detailed Description**:\n文件 src/routes/index.ts 作为路由模块的聚合点，负责导出与 Google Drive 相关的路由和健康检查路由的功能。通过使用 ES6 的模块导出语法，文件简单有效地将其他路由文件中的所有导出内容整合到一起，便于在整个应用程序中进行统一的路由管理。这种结构化的设计提高了代码的可读性和可维护性，使得在需要扩展或修改路由时，可以轻松地在此文件中进行调整，而不必逐一修改每个单独的路由文件。同时，通过导入不同的功能模块，src/routes/index.ts 也确保了应用程序的逻辑分离，促进了模块化编程的最佳实践。整体而言，该文件在项目中起到了连接各个路由功能的桥梁作用，简化了路由的管理和使用。\n\n### src\\routes\\gdrive\\index.ts\n\n**Summary**: 该文件聚合并导出与 Google Drive 相关的路由功能模块，简化了导入过程。\n\n**Detailed Description**:\n文件 src/routes/gdrive/index.ts 的主要作用是聚合并导出来自同一目录下的两个模块，即 webhook.ts 和 manualSync.ts。这种结构化的导出方式使得在其他文件中引入 Google Drive 相关的路由功能变得更加简洁和有组织。在实际应用中，webhook 模块可能用于处理 Google Drive 的事件通知，而 manualSync 模块则可能用于手动触发 Google Drive 文件的同步操作。因此，该文件的设计不仅优化了代码的模块化程度，还提高了可维护性，使得开发者能够在不直接引用每个单独模块的情况下，轻松访问与 Google Drive 互动的所有相关功能。\n\n### src\\routes\\gdrive\\manualSync.ts\n\n**Summary**: 该文件实现了一个手动触发 Google Drive 同步的路由，允许用户通过 GET 请求进行数据更新。\n\n**Detailed Description**:\n文件 src/routes/gdrive/manualSync.ts 定义了一个用于手动触发 Google Drive 同步的路由，使用 GET 请求方法。该路由的路径为 '/gdrive/sync'，当收到请求时，处理程序会首先记录信息，表明手动同步已被触发。随后，它调用 syncGoogleDriveChanges 函数，该函数负责检测 Google Drive 上的变更并将其同步到应用程序数据库。处理程序会检查返回的变更数量，如果有变更存在，它会循环调用 syncGoogleDriveChanges 直至没有更多变更。每次调用后，都会记录当前变更的数量。最终，若同步成功，路由将返回一个 JSON 响应，包含成功消息和变更的结果。如果在同步过程中发生错误，则会捕获异常并记录错误信息，同时返回 500 状态码和错误消息。这种设计允许用户通过发起 HTTP 请求来手动触发数据同步，增强了应用的灵活性和用户控制能力。\n\n### src\\routes\\gdrive\\webhook.ts\n\n**Summary**: 该文件处理 Google Drive 的 webhook 事件，并同步相关的文件变更。\n\n**Detailed Description**:\n文件 src/routes/gdrive/webhook.ts 实现了一个用于处理 Google Drive Webhook 的路由。该路由的路径为 '/gdrive/webhook'，采用 POST 请求方式。当有 webhook 事件触发时，该路由会调用 syncGoogleDriveChanges 函数来同步 Google Drive 中的变更，并记录相关日志。在成功执行后，它会返回一个 JSON 响应，包含 'OK' 消息和同步结果。如果在处理过程中出现任何错误，代码会捕获该错误，记录详细信息并返回状态码 500，以及包含错误信息的 JSON 响应。这种设计允许外部系统通过 webhook 通知服务，以便及时更新相关数据，确保应用程序与 Google Drive 的状态保持同步。\n\n### src\\services\\index.ts\n\n**Summary**: 该文件实现了一个 HypothesisService 类，用于生成和评估科学假设并处理相关的周期性任务。\n\n**Detailed Description**:\n该文件定义了 HypothesisService 类，作为一个服务来生成和评估科学假设。该服务继承自 @elizaos/core 库中的 Service 类，并实现了启动和停止服务的静态方法。在服务启动时，它会注册一个名为 'HGE' 的任务工作者，该工作者的主要功能是生成和评估假设，并通过 Discord 流式传输结果。服务还会检查当前的任务并处理周期性任务，确保在符合更新间隔时执行任务逻辑。每当任务执行后，服务会更新任务的最后更新时间，以便下次检查时能准确判断是否需要再次执行。服务的停止方法确保在服务关闭时正确停止工作。此模块的设计使得生成和评估假设的过程可以在后台运行，并能与其他系统（如 Discord）进行交互。\n\n### src\\services\\anthropic\\chooseTwoRelevant.ts\n\n**Summary**: 该文件提供了功能以从关键词和研究发现中选择两个相关项，以支持科学假设的生成。\n\n**Detailed Description**:\n该文件定义了两个异步函数，`chooseTwoRelevantKeywords` 和 `chooseTwoRelevantFindings`，用于从给定的关键词和研究发现中选择两个与之相关的项，以生成新的假设。这两个函数通过使用 Anthropic 的 API（特别是 Claude 模型）与用户的输入进行交互，确保所选的关键词和发现未在之前的假设中使用。每个函数都构建一个特定格式的请求，包括用户的角色和内容，向 API 发送请求后，解析响应并返回所需的两个关键词或发现。函数确保返回的关键词和发现的大小写与输入保持一致，并通过设置 `thinking` 选项控制预算 token 的使用，以优化生成的结果。错误处理机制确保如果没有文本内容返回，则抛出异常，提示调用者处理问题。这种选择机制对科学研究中的假设生成具有实用价值，可以帮助研究者发现新的研究方向。\n\n### src\\services\\anthropic\\client.ts\n\n**Summary**: 该文件用于配置与 Anthropic 和 OpenAI 的客户端连接，加载必要的 API 密钥以支持后续的人工智能功能实现。\n\n**Detailed Description**:\n该文件用于初始化和配置与两个人工智能平台的客户端连接，分别是 Anthropic 和 OpenAI。通过导入 'dotenv/config'，文件确保可以安全地加载环境变量，特别是 API 密钥，这些密钥在应用程序与这两个服务进行交互时至关重要。文件中创建了两个实例，'anthropic' 和 'openai'，分别用于与 Anthropic AI 的 SDK 和 OpenAI 的 API 进行交互。通过提供各自的 API 密钥，这些实例可以用于发送请求、获取模型响应和处理相关的人工智能任务。因此，该文件是进行与这两个平台集成的基础，支持后续的功能实现，如生成文本、处理自然语言和进行智能评估。\n\n### src\\services\\anthropic\\discordSplitter.ts\n\n**Summary**: 该文件提供了将 Markdown 文档拆分并格式化为适合 Discord 消息的功能。\n\n**Detailed Description**:\n该文件包含用于处理 Markdown 文档的功能，特别是将其拆分为适合 Discord 消息格式的多个部分。由于 Discord 对单条消息的字符限制为 2000 字符，文件通过定义常量 MAX_MESSAGE_LENGTH（设为 1800）来确保每条消息的内容在安全范围内。文件中定义了 MessageChunk 接口，用于表示消息块的内容和编号。主要功能是 splitMarkdownForDiscord，按行解析输入的 Markdown 文档，智能地将其拆分为多个块，确保在重要头部（如一级和二级标题）处进行拆分。此外，代码还处理列表项的合并，以便在可能的情况下保持列表的完整性。最终生成的消息块会被格式化为特定的 Discord 输出格式。processMarkdownFile 函数是该模块的入口，用于读取指定的 Markdown 文件，调用拆分和格式化功能，并将结果输出到控制台或指定的文件中，增强了 Markdown 文档在 Discord 中的可读性和结构化展示能力。\n\n### src\\services\\anthropic\\errors.ts\n\n**Summary**: 该文件定义了用于 SPARQL 查询和文件操作的自定义错误类，以增强错误处理的清晰度和可维护性。\n\n**Detailed Description**:\n文件 src/services/anthropic/errors.ts 定义了两个自定义错误类：SparqlError 和 FileError。这些错误类通过继承 JavaScript 的内置 Error 类，提供了更具体的错误处理方式。SparqlError 类用于处理与 SPARQL 查询相关的错误，其构造函数接受一个消息字符串和一个可选的 cause 参数，用于描述错误的根本原因。FileError 类则专注于文件操作中的错误，同样包含消息和可选的 cause 参数。这种结构化的错误处理方式可以帮助开发人员在调试和维护代码时，更清晰地识别和处理不同类型的错误，从而提高代码的可读性和可维护性。\n\n### src\\services\\anthropic\\evaluateHypothesis.ts\n\n**Summary**: 该文件用于评估科学假设，并将评估结果发送到 Discord 渠道。\n\n**Detailed Description**:\n文件 src/services/anthropic/evaluateHypothesis.ts 的主要功能是评估科学假设，通过与 OpenAI 和 Anthropic API 的交互来实现。该文件定义了两个异步函数：evaluateHypothesis 和 sendEvaluationToDiscord。evaluateHypothesis 函数接受一个假设字符串作为输入，首先通过 OpenAI 的 GPT-4 模型进行互联网研究，生成与假设相关的研究内容。接着，它将该研究结果与假设一起传递给另一个 GPT 模型进行假设评估，并提取出评估结果中的分数。分数是由 Anthropic 的 Claude 模型提取的，该模型从评估结果文本中提取出一个介于 0 到 100 之间的整数分数。评估函数的返回值是一个包含研究内容、评估内容和分数的对象。第二个函数 sendEvaluationToDiscord 则负责将评估结果发送到 Discord 渠道，它利用 agentRuntime 获取 Discord 客户端，并将研究和评估的内容逐段发送到指定的频道。此文件的逻辑清晰，充分利用了多种 AI 模型的能力，实现了科学假设的自动化评估和结果共享。\n\n### src\\services\\anthropic\\generateHypothesis.ts\n\n**Summary**: 该文件用于生成基于图数据库中信息的科学假设，并将其发送到 Discord 频道。\n\n**Detailed Description**:\n该文件 `generateHypothesis.ts` 定义了一个用于生成科学假设的函数，依赖于从 SPARQL 图数据库中提取的关键词、发现和相关文献的摘要。文件首先加载必要的模块和类型定义，然后通过一系列异步函数从数据库获取关键词、之前的假设、文献摘要和研究发现。接着，它会随机选择两个相关的发现和两个相关的关键词，生成关于这两者的假设提示，通过调用 Anthropic 的 API 生成实际的假设文本。生成的假设包含背景、知识空白、中心假设、提出的机制、可检验的预测以及实验方法等部分，同时记录使用的文献和相关的关键词。此外，该文件还处理了文件读取和 SPARQL 查询的错误，确保系统在运行过程中的稳定性。最后，生成的假设会被发送到指定的 Discord 频道，便于共享和讨论。\n\n### src\\services\\anthropic\\hypGenEvalLoop.ts\n\n**Summary**: 该文件实现了一个周期性生成和评估科学假设的循环，并将结果发送到 Discord 渠道。\n\n**Detailed Description**:\n该文件定义了一个名为 'hypGenEvalLoop' 的异步函数，用于在指定的时间间隔内生成科学假设并将其评估结果发送到 Discord 渠道。函数首先记录开始生成假设的消息，然后通过 setInterval 方法每 150 秒执行一次生成假设的操作。在每次调用中，它会调用 'generateHypothesis' 函数来生成新的假设，并获取该假设的消息 ID。生成的假设会通过 'elizaLogger' 进行记录，并随后调用 'sendEvaluationToDiscord' 函数，将生成的假设及其消息 ID 发送到 Discord 渠道。该文件还定义了一个 'stopHypGenEvalLoop' 函数，用于停止定时器，记录停止消息并清除定时器。这种循环生成与评估假设的机制对于自动化科学研究的假设生成过程至关重要，能够支持科学家和研究人员实时获取和评估新的研究假设。\n\n### src\\services\\anthropic\\types.ts\n\n**Summary**: 该文件定义了多个 TypeScript 类型，用于结构化科学研究数据，确保数据的一致性和类型安全性。\n\n**Detailed Description**:\n文件 src/services/anthropic/types.ts 定义了一组 TypeScript 类型，用于结构化科学研究相关数据。这些类型包括 Binding、Abstract、Finding、FindingResult 和 Hypothesis，旨在确保数据的一致性和可读性。具体来说，Binding 类型用于描述一个对象的类型和值，Abstract 类型则用于表示论文的摘要及其子项的信息。Finding 类型包括对研究发现的描述及其相关论文的信息，而 FindingResult 则表示特定发现及所关联论文的结果。最后，Hypothesis 类型用于表示科学假设及其对应的实体，这为后续的假设生成和评估提供了必要的数据结构。这些类型在整个项目中有助于确保数据交互的类型安全性，提升代码的可维护性和可理解性。\n\n### src\\services\\anthropic\\sparql\\makeRequest.ts\n\n**Summary**: 该文件实现了一个用于发送 SPARQL 查询请求并处理响应的功能模块。\n\n**Detailed Description**:\n文件 src/services/anthropic/sparql/makeRequest.ts 负责处理 SPARQL 查询请求，主要通过 axios 库发送 HTTP POST 请求到指定的 SPARQL 端点。在函数 sparqlRequest 中，传入的字符串参数 query 被用作请求体，并在请求的头部中设置了 Content-Type 和 Accept 字段，以确保请求被正确处理。该函数使用 async/await 语法处理异步操作，并在请求成功时返回从服务器获取的数据。如果请求失败，捕获到的错误会通过 AxiosError 进行处理，抛出一个自定义的 SparqlError，以增强错误信息的可读性和可维护性。这种结构化的错误处理机制使得调用该函数的代码能够更好地理解和响应 SPARQL 查询失败的情况。整体而言，该文件的功能是提供一个简洁且强健的接口，以便在应用程序中进行 SPARQL 查询。\n\n### src\\services\\anthropic\\sparql\\queries.ts\n\n**Summary**: 该文件定义了多个 SPARQL 查询，用于从图数据库中提取与科学研究相关的关键词、摘要和假设信息。\n\n**Detailed Description**:\n文件 src/services/anthropic/sparql/queries.ts 定义了一系列 SPARQL 查询，用于从图数据库中提取与科学研究相关的信息。具体来说，该文件包括多个常量，每个常量代表一个特定的查询。第一个查询 getKeywordsQuery 用于获取与研究论文相关的关键词。第二个查询 getAbstractsQuery 针对特定关键词检索对应的论文摘要。第三个查询 getFindingsQuery 则从 GO、CHEBI 和 ATC 本体中检索与研究论文相关的发现，获取描述和论文的基本信息。接下来，getAbstractsForPapersQuery 是一个动态生成的查询函数，接收论文引用列表作为参数，返回这些论文的摘要信息，包括可选的部分信息。最后，getPreviousHypothesesForKeywordsQuery 查询与特定关键词相关的先前假设，帮助用户了解已有的研究工作。整体而言，这个文件极大地增强了系统在处理和分析科学文献时的灵活性，支持了假设生成和评估的过程。\n\n### src\\services\\gdrive\\buildQuery.ts\n\n**Summary**: 该文件实现了 Google Drive 文件查询的策略模式，动态构建针对主文件夹或共享驱动的查询。\n\n**Detailed Description**:\n文件 src/services/gdrive/buildQuery.ts 实现了 Google Drive 文件查询的策略模式，提供了灵活的查询构建功能。该文件定义了 ListFilesQueryStrategy 接口和两个具体的策略类：SharedDriveFolderStrategy 和 SharedDriveStrategy。SharedDriveFolderStrategy 负责构建针对共享文件夹的查询，而 SharedDriveStrategy 则用于构建针对共享驱动的查询。这两个策略类的主要方法包括 buildQuery()、getStartPageTokenParams()、getDriveType()、getDriveId() 和 getWatchFolderParams()，它们分别用于生成查询参数、获取起始页面标记参数、获取驱动类型、获取驱动 ID 以及获取监视文件夹的参数。ListFilesQueryContext 类用作上下文，负责决定使用哪种策略，并提供统一的接口给调用者。通过这种设计，代码能够根据输入的主文件夹 ID 或共享驱动 ID 动态选择合适的查询策略，从而增强了代码的可扩展性和可维护性。\n\n### src\\services\\gdrive\\client.ts\n\n**Summary**: 该文件用于初始化 Google Drive 客户端并提供文件查询相关的工具函数。\n\n**Detailed Description**:\n该文件负责初始化和配置与 Google Drive 的连接，提供客户端功能以便进行文件操作。首先，它导入了 Google APIs 库及相关的类型定义，并通过 'dotenv/config' 加载环境变量。'initDriveClient' 函数接受一个 OAuth 范围数组作为参数，默认范围为只读权限。该函数尝试从环境变量中解析 Google Cloud Platform 的 JSON 凭据，使用这些凭据创建一个 GoogleAuth 实例，并返回一个配置好的 Google Drive 客户端。出错时，它会记录错误信息并抛出异常。此外，文件还定义了一个常量对象 'FOLDERS'，用于存储共享驱动和共享驱动文件夹的 ID，从环境变量中提取这些值。'getListFilesQuery' 和 'getStartPageTokenParams' 函数利用 'ListFilesQueryContext' 类构建查询和获取起始页面令牌，便于后续文件列表的获取。这一文件的主要用途是确保与 Google Drive 的安全连接，并为文件操作提供必要的工具和上下文。\n\n### src\\services\\gdrive\\index.ts\n\n**Summary**: 该文件整合了与 Google Drive 相关的服务，导出客户端和文件监控功能，并提供 PDF 转换的配置选项。\n\n**Detailed Description**:\n文件 src/services/gdrive/index.ts 主要负责整合与 Google Drive 相关的服务功能。它通过导出其他模块的功能，使得这些功能能够在其他部分的代码中更容易地访问和使用。具体来说，该文件导出了 client.ts 和 watchFiles.ts 中定义的所有功能，分别用于初始化 Google Drive 客户端和监控文件变化。此外，文件还定义了一个名为 pdf2PicOptions 的常量，用于设置将 PDF 文件转换为 PNG 格式时的选项，包括解析度（density）、输出格式（format）以及输出图像的宽度和高度。这些配置为后续处理 PDF 文件提供了必要的参数，确保转换结果的质量和一致性。总体而言，该文件在 Google Drive 相关的服务模块中起到了聚合与配置的作用，简化了模块的导入和使用。\n\n### src\\services\\gdrive\\storeJsonLdToKg.ts\n\n**Summary**: 该文件用于解析 JSON-LD 对象并将生成的 RDF 数据存储到 Oxigraph 数据库中。\n\n**Detailed Description**:\n该文件 `storeJsonLdToKg.ts` 定义了一个异步函数 `storeJsonLd`，其主要功能是接受一个 JSON-LD 对象，解析该对象并将生成的 RDF 数据存储到 Oxigraph 数据库中。函数使用 `n3` 库中的 `Store` 类来存储解析出的数据三元组（quads），并利用 `jsonld-streaming-parser` 库进行 JSON-LD 的解析。在函数内部，首先创建一个存储实例和解析器实例，然后通过监听解析器的 'data' 事件，将每个解析出的三元组添加到存储中。如果在解析过程中出现错误，错误将被捕获并记录。解析完成后，函数将存储中的数据转换为 N-Triples 格式，并通过 Axios 发送 POST 请求，将数据发送到 Oxigraph 的存储端点。在成功存储的数据返回状态为 204 时，函数返回 `true`，表示操作成功；否则，返回错误信息。该功能被设计为一个模块化服务的一部分，以支持科学数据的存储和管理。\n\n### src\\services\\gdrive\\watchFiles.ts\n\n**Summary**: 该文件用于监控 Google Drive 文件夹的变化，下载新文件并将其转换为知识资产，存储为 JSON-LD 格式。\n\n**Detailed Description**:\n该文件实现了监控 Google Drive 文件夹变化的功能，通过定期检查文件的哈希值来识别新文件。一旦检测到新文件，程序会下载该文件并将其转换为图片格式，随后生成知识资产（Knowledge Asset）并将其存储为 JSON-LD 格式到 Oxigraph 数据库中。具体来说，文件包含了多个辅助函数，如 `downloadFile` 用于下载文件，`getFilesInfo` 用于获取文件信息。此外，`watchFolderChanges` 函数负责设置监控逻辑，包括初始化 DKG 客户端、获取已处理文件的哈希值、定期检查文件夹的变化等。该文件使用了 Google Drive API，利用异步编程处理文件下载和转换，确保及时响应文件变化，并在发生错误时记录日志以便于调试和维护。\n\n### src\\services\\gdrive\\extract\\README.md\n\n**Summary**: 该文件描述了提取科学论文数据的基本实现逻辑，并提出了处理关键词不一致性的问题及其解决方案。\n\n**Detailed Description**:\n该文件 README.md 提供了关于在 BioAgent 插件中提取科学论文数据的基本逻辑和实现细节。它描述了如何生成样本 JSON-LD 数据的过程，并指出该实现需要与文件 watchFiles.ts 进行集成。文件中提到，watchFiles.ts 中的某些代码已经过时，特别是 evaluateHypothesis 函数，但监控 Google Drive 变更的逻辑仍然适用。一个主要挑战是处理不同论文中的 'keywords' 字段，因为 LLM 可能会为不同的论文生成不一致的术语，如一个文件使用“Alzheimer's disease”，而另一个文件则使用“AD”。为了解决这一问题，需要进行后处理，将这些不同的术语映射到标准化的关键词。为此，文件提到正在探索使用 MeSH（医学主题词表）或 UMLS（统一医学语言系统）的方法，通过模糊匹配来处理这个问题。一旦找到合适的解决方案，就可以安全地将其集成到 BioAgent 插件中，以提高数据提取的准确性和一致性。\n\n### src\\services\\gdrive\\extract\\config.ts\n\n**Summary**: 该文件用于管理与人工智能模型的客户端配置和相关设置，采用单例模式确保配置的一致性和可维护性。\n\n**Detailed Description**:\n该文件定义了一个单例模式的 Config 类，主要用于配置和管理与人工智能模型（Anthropic 和 OpenAI）相关的客户端连接及其设置。首先，通过导入环境变量，文件获取所需的 API 密钥和模型名称，并将默认值设置为 'claude-3-7-sonnet-latest' 和 'gpt-4o'。在初始化过程中，Config 类确保只创建一个客户端实例，并检查用于存储论文的目录是否存在，如不存在则创建。此类还提供了一些静态方法，使得其他模块可以方便地访问和更新客户端、模型以及配置选项。此外，类通过静态属性封装了与文件操作和模型配置相关的逻辑，确保数据一致性和安全性。通过这种设计，Config 类不仅提升了代码的可维护性，还增强了应用的灵活性，方便未来可能的扩展和修改。\n\n### src\\services\\gdrive\\extract\\index.ts\n\n**Summary**: 该文件用于从图像中提取科学论文和本体信息，并将结果结构化返回。\n\n**Detailed Description**:\n该文件定义了与科学论文和本体提取相关的功能，主要包括三个异步函数：extractPaper、extractOntologies 和 generateKa。extractPaper 函数负责从图像数组中提取科学论文的信息，使用 OpenAI 的 GPT-4o 模型进行处理，并将提取到的数据格式化为 PaperSchema 结构。extractOntologies 函数与之类似，专注于提取与本体相关的信息。这两个函数各自执行时，都会记录开始和完成的日志信息，以便于调试和跟踪。generateKa 函数则将这两个提取过程结合起来，首先并行执行 extractPaper 和 extractOntologies，然后将结果合并，最终返回包含论文信息和本体信息的对象。该文件的用途在于为从图像中提取科学知识提供了一种结构化的方法，支持后续的知识存储和处理。\n\n### src\\services\\gdrive\\extract\\prompts.ts\n\n**Summary**: 该文件提供了生成符合 PaperSchema 的 JSON-LD 对象和相关本体术语的提示字符串，以支持科学论文数据的结构化提取。\n\n**Detailed Description**:\n文件 src/services/gdrive/extract/prompts.ts 定义了两个主要的提示字符串，用于从科学论文中生成结构化的 JSON-LD 对象。第一个提示 'extractionPrompt' 指导生成符合 PaperSchema 规范的 JSON-LD 对象，明确列出所需字段及格式，如 '@context', '@id', '@type', 'dcterms:title' 等，确保输出的 JSON 结构符合既定标准，同时要求提供真实的信息而非占位符。第二个提示 'ontologiesExtractionPrompt' 用于生成与科学论文相关的本体术语，要求提取来自多个生物医学和书目本体的真实术语，确保每个术语包含正确的前缀格式和官方标签。该文件的设计目的是通过系统化的提示，帮助实现科学论文数据的自动化提取和结构化，促进知识的整合与管理。\n\n### src\\services\\gdrive\\extract\\types.ts\n\n**Summary**: 该文件定义了与图像处理和 AI 客户端相关的 TypeScript 类型，以确保类型安全和代码可维护性。\n\n**Detailed Description**:\n文件 src/services/gdrive/extract/types.ts 定义了一些与图像处理和人工智能客户端相关的 TypeScript 类型。具体来说，文件中包含了两个主要类型：AnthropicImage 和 OpenAIImage。AnthropicImage 类型用于表示从 Anthropic AI SDK 生成的图像数据，包含了图像的媒体类型和以 base64 编码的图像数据。OpenAIImage 类型则用于表示 OpenAI 生成的图像，包含图像的 URL。最后，文件还定义了一个 InstructorClient 类型，它是一个泛型类型，可接受 OpenAI 或 Anthropic 作为参数，表示与这两个 AI 服务的客户端连接。这些类型的定义方便了在项目中使用这些图像和客户端对象，确保类型安全性和代码的可维护性。\n\n### src\\services\\gdrive\\extract\\z.ts\n\n**Summary**: 该文件为科学论文知识资产提供了标准化的 JSON-LD 数据结构和验证逻辑，确保数据的可互操作性和类型安全性。\n\n**Detailed Description**:\n该文件定义了一个用于科学论文知识资产的标准化 JSON-LD 模式，利用 Zod 库来验证和描述数据结构。文件中包含多个模式，分别用于描述论文的上下文、创作者、出版场所、章节、引用以及本体等信息。通过对这些模式的定义，文件确保了生成的 JSON-LD 数据符合特定的语义标准，从而提升了科学文献的可互操作性和结构化程度。具体而言，'ContextSchema' 提供了用于 JSON-LD 的前缀映射，'CreatorSchema' 定义了创作者的标识和类型，'PublicationVenueSchema' 描述了出版场所的相关信息，'SectionSchema' 和 'CitationSchema' 则分别结构化了论文的章节内容和引用文献。最后，'PaperSchema' 聚合了上述所有结构，构成一个完整的科学论文表示，支持创建更为丰富和可查询的知识资产。整体上，文件通过类型安全和结构化的方式，促进了与去中心化知识图的交互和数据的自动化处理。\n\n### src\\services\\kaService\\anthropicClient.ts\n\n**Summary**: 该文件用于与 Anthropic AI 接口交互，提供生成 AI 响应的功能。\n\n**Detailed Description**:\n该文件实现了与 Anthropic AI 的交互，主要提供了获取客户端和生成 AI 响应的功能。它首先从环境变量中加载 API 密钥，并使用此密钥实例化一个 Anthropic 客户端。用户可以通过调用 getClient 函数获取此客户端。generateResponse 函数接受客户端、提示字符串、模型名称和最大令牌数作为参数，发送请求到 Anthropic 的消息 API，并期望从其返回的响应中提取文本内容。如果返回的内容有效且包含文本，函数将返回该文本；否则，将抛出错误，指示未收到任何响应。这使得开发者能够方便地与 Anthropic 的 AI 模型进行交互，生成基于用户提示的自然语言响应，适用于需要自动化生成文本的应用场景。\n\n### src\\services\\kaService\\biologyApi.ts\n\n**Summary**: 该文件提供了与生物本体的 API 交互功能，支持术语搜索和更新，以增强生物医学数据的语义理解。\n\n**Detailed Description**:\n该文件实现了一系列用于与生物本体（如基因本体、疾病本体、化学物质本体和药物本体）进行交互的 API 调用功能。主要功能包括通过 QuickGO 和 EBI API 搜索基因本体（GO）、DOID（疾病本体）、ChEBI（化学物质本体）和 ATC（药物本体）术语。每个搜索函数（如 searchGo、searchDoid、searchChebi 和 searchAtc）都会发送 GET 请求到相应的 API，获取与输入术语相关的候选项，并通过调用 generateResponse 函数生成新的术语。文件还提供了 updateGoTerms、updateDoidTerms、updateChebiTerms 和 updateAtcTerms 函数，用于批量更新数据中的术语，确保每个条目都与最佳匹配的本体术语进行关联。错误处理和日志记录功能也被集成，以便在 API 调用失败时提供反馈。整体而言，该文件的目的是增强生物医学数据的语义理解和互操作性。\n\n### src\\services\\kaService\\downloadPaper.ts\n\n**Summary**: 该文件用于从 bioRxiv 和 arXiv 的论文 URL 下载 PDF 文件并提取 DOI。\n\n**Detailed Description**:\n该文件包含一个名为 downloadPaperAndExtractDOI 的异步函数，用于从给定的论文 URL 下载 PDF 文件并提取数字对象标识符 (DOI)。支持 bioRxiv 和 arXiv 的论文链接。代码首先检查 URL 的类型，根据不同的来源构造 PDF 下载链接。对于 bioRxiv，PDF 链接通过在原始 URL 后加上 '.full.pdf' 来生成；对于 arXiv 论文，则通过将 '/abs/' 替换为 '/pdf/' 并添加 '.pdf' 后缀来生成。接着，使用 axios 库请求论文的 HTML 内容，并通过 cheerio 库解析 HTML 以提取 DOI。提取 DOI 的过程中，首先尝试从 'citation_doi' 元标记获取，如果未找到，则作为后备方案尝试从包含 DOI 的链接中提取。成功下载 PDF 文件后，记录下载成功的信息，并返回 PDF 的缓冲区和提取到的 DOI。如果在下载或提取过程中发生错误，将记录错误信息并返回 null 值。整体逻辑确保了用户能够方便地获取科学论文的 PDF 版本及其相关的 DOI 信息。\n\n### src\\services\\kaService\\exampleForPrompts.ts\n\n**Summary**: 该文件提供了科学论文数据结构化提取的示例输入和输出格式，确保数据处理的一致性和灵活性。\n\n**Detailed Description**:\n文件 src/services/kaService/exampleForPrompts.ts 提供了一系列示例输入和输出格式，用于支持科学论文数据的结构化提取和处理。这些示例涵盖了基本信息、引用、子图、基因本体、疾病本体、化学物质本体等多个方面，展示了如何将原始文本数据转换为结构化的 JSON-LD 格式。其中，basic_info_example_input 和 basic_info_example_output 用于展示论文的基本信息提取，包括标题、作者、DOI 等字段；citations_example_input 和 citations_example_output 则展示了如何处理引用列表；而 subgraph_go_example_input 和 subgraph_go_example_output 则为生物学过程相关的信息提供了结构化的表达方式。该文件的主要目的是为其他代码模块提供标准化的输入输出示例，以便于在处理科学数据时确保一致性和可预测性，从而提高数据处理的灵活性和可靠性。\n\n### src\\services\\kaService\\kaService.ts\n\n**Summary**: 该文件实现了从科学论文生成和管理知识资产的功能，包括解析论文、提取元数据和生成知识图谱。\n\n**Detailed Description**:\n该文件 src/services/kaService/kaService.ts 实现了与科学论文相关的知识资产生成和处理功能。它导入必要的依赖项，包括与人工智能 API 的客户端、PDF 文件处理工具、数据库查询函数和日志工具等。文件中定义了一些类型和接口以增强代码的可读性和类型安全。在主要功能上，jsonArrToKa 函数接收一个表示论文文本的 JSON 数组并返回一个包含提取的元数据、引用信息和总结的知识图谱。generateKaFromUrls 函数则通过 URL 下载论文并提取 DOI，随后通过调用其他处理函数生成知识资产。generateKaFromPdf 函数处理 PDF 文件，将其转换为图像并提取 DOI，随后检查该论文是否已存在于去中心化知识图（DKG）中，若不存在，则生成相应的知识资产并关联相关 DAO 信息。removeColonsRecursively 函数递归地移除对象中的冒号，确保生成的知识资产符合特定的格式要求。整体来看，该文件为科学研究提供了一个结构化的知识生成和管理方案，支持从论文和图像中提取信息并构建关联的知识图谱。\n\n### src\\services\\kaService\\llmPrompt.ts\n\n**Summary**: 该文件用于生成与科学论文相关的提示，以便提取和分析生物学领域的关键信息和术语。\n\n**Detailed Description**:\n该文件定义了一系列用于生成科学论文相关提示的函数，主要涉及生物学领域的本体、术语和数据提取。每个函数都接受特定的输入（如论文的 JSON 数组），并根据生物医学领域的标准（如基因本体 GO、疾病本体 DOID、化学实体 ChEBI 和解剖治疗化学 ATC）生成相应的提示。这些提示旨在引导人工智能模型分析和提取科学论文中的关键信息，例如选择最合适的本体术语，提取基本信息、引用内容及论文中的关系等。该文件还包含了关于如何将提取的信息转换为 JSON-LD 格式的具体指导，确保生成的结果符合语义网络的要求。这使得该文件在科学研究和数据挖掘中具有重要作用，能够提高信息的可获取性和可用性。\n\n### src\\services\\kaService\\processPaper.ts\n\n**Summary**: 该文件用于处理科学论文文本，提取和生成结构化信息以构建知识图谱。\n\n**Detailed Description**:\n文件 src/services/kaService/processPaper.ts 负责处理科学论文的文本数据，提取并生成相关的结构化信息。它通过调用多个助手函数来实现这一功能，包括提取论文的各个部分（如摘要、引言、方法、结果和讨论），生成基本信息和引用，构建 GO、DOID、ChEBI 和 ATC 的子图结构，并将这些信息转换为 JSON-LD 格式。该文件定义了多个异步函数，利用 Anthropic 客户端与大型语言模型（LLM）交互，以获得更复杂的语义和结构化数据。主要逻辑包括提取页面范围、生成子图、处理 JSON 数据，以及创建最终的知识图谱。此外，文件中包含错误处理机制，以确保在生成过程中捕获和记录任何异常情况，从而提高了代码的健壮性和可维护性。\n\n### src\\services\\kaService\\regex.ts\n\n**Summary**: 该文件提供了处理字符串的实用函数，包括提取括号内容、判断空数组和格式化为有效JSON字符串。\n\n**Detailed Description**:\n该文件提供了一系列用于处理字符串的实用函数，主要聚焦于提取和格式化文本内容。首先，`extractBracketContent` 函数通过正则表达式匹配输入字符串中的第一个括号内容，返回包括括号的字符串，如果未找到则返回 null。此功能类似于 Python 的 re.search 方法，适合于从文本中提取特定格式的数据。其次，`isEmptyArray` 函数用于检查给定字符串在去除空格后的内容是否精确为 '[]'，提供了简易的空数组判断逻辑。最后，`convertToValidJsonString` 函数旨在将字符串中的某些单引号替换为双引号以符合 JSON 格式，利用了复杂的正则表达式来确保在特定上下文中（如在键值对中）进行替换。这些函数的实现依赖于 JavaScript 的正则表达式特性，特别是后顾断言，因此要求运行环境支持这些特性。整体上，该文件为处理文本数据提供了基础的工具，尤其在需要进行格式化和数据提取的场合具有重要作用。\n\n### src\\services\\kaService\\sparqlQueries.ts\n\n**Summary**: 该文件提供了通过 DOI 查询和验证科学文献信息的 SPARQL 查询函数。\n\n**Detailed Description**:\n该文件包含两个导出函数，旨在通过 DOI（数字对象标识符）查询科学文献的相关信息。第一个函数 getPaperByDoi(doi: string) 生成一个 SPARQL 查询，该查询用于从知识图谱中提取与特定 DOI 相关的研究论文的标题、摘要、作者、相关多组学、实验、队列信息及分析描述。该查询使用了多个前缀（如 fabio, dcterms, foaf, obi, schema）来定义数据的语义，并通过可选模式（OPTIONAL）来处理可能缺失的信息，最后通过 GROUP_CONCAT 汇总多个值。第二个函数 paperExists(doi: string) 生成一个简洁的 SPARQL ASK 查询，用于检查特定的 DOI 是否存在于知识图谱中。此函数同样会处理 DOI 的格式，以确保查询的正确性。整体而言，该文件提供了一种结构化的方式来访问和验证科研文献的信息，支持其他模块对文献的引用和存在性检查。\n\n### src\\services\\kaService\\unstructuredPartitioning.ts\n\n**Summary**: 该文件用于向 Unstructured API 发送 PDF 文件并提取结构化信息。\n\n**Detailed Description**:\n该文件实现了与 Unstructured API 交互的功能，主要用于处理 PDF 文件并提取结构化信息。首先，通过导入所需的库（如 axios 和 FormData）以及环境变量中的 API 密钥，确保可以安全地进行 API 调用。核心函数 makeUnstructuredApiRequest 接受文件内容和文件名作为参数，构建一个 multipart/form-data 请求，包含 PDF 文件及其他配置（如表格结构推断）。请求被发送到 Unstructured API 的指定端点，并在成功后返回解析后的数据。此外，文件中还包含了一个注释掉的异步函数 processPdfFiles，这个函数示例展示了如何读取本地 PDF 文件并将其发送给 Unstructured API，同时将响应保存为 JSON 格式。这使得该文件不仅可以用于单独处理文件，还可以作为处理批量 PDF 文件的基础。\n\n### src\\services\\kaService\\vectorize.ts\n\n**Summary**: 该文件用于生成科学论文的摘要，通过与大型语言模型进行交互以获取论文元数据的总结。\n\n**Detailed Description**:\n文件 vectorize.ts 主要用于生成科学论文的摘要，通过与大型语言模型（LLM）交互来实现。它定义了几个接口，包括 Graph、CitationEntry 和 SimilarCitationResult，分别用于描述图对象、单个引用条目和相似引用的结果结构。在核心功能上，getSummary 函数接受一个 LLM 客户端和一个包含论文元数据的图对象作为参数。该函数首先通过调用 get_prompt_vectorization_summary 函数创建适合 LLM 的提示，然后使用 generateResponse 函数与 LLM 进行交互，生成摘要。在成功生成摘要后，函数会记录摘要内容，并在发生异常时捕获错误并记录相应的错误信息，确保在运行时的可追踪性和可靠性。整体而言，该文件通过自动化生成论文摘要，增强了科学研究数据的可读性和可理解性。\n\n"
    },
    {
      "path": "src\\constants.ts",
      "detailed_description": "该文件定义了项目中使用的一些常量和示例查询，主要聚焦于社交媒体发布的结构化数据。首先，`dkgMemoryTemplate` 提供了一个标准化的社交媒体发布格式，包含了上下文、作者信息、创建日期、互动统计、提及的账户、关键词和相关主题等字段，用于生成社交媒体内容的元数据。接下来，文件中包含了一些 SPARQL 查询示例，这些查询用于从图数据库中提取社交媒体发布的标题和正文内容，支持关键词和主题的过滤。通过使用 `CONTAINS` 和 `FILTER` 函数，这些查询可以灵活地检索符合特定条件的社交媒体数据。此外，`DKG_EXPLORER_LINKS` 提供了指向 DKG（去中心化知识图谱）浏览器的链接，分别针对测试网和主网。这些常量和查询示例对于构建与社交媒体数据相关的功能和接口非常重要，确保了数据的一致性和可访问性。",
      "summary": "该文件定义了社交媒体发布的标准化结构和 SPARQL 查询示例，用于提取和操作社交媒体数据的元信息。",
      "raw": "import { z } from \"zod\";\n// TODO: add isConnectedTo field or similar which you will use to connect w other KAs\nexport const dkgMemoryTemplate = {\n    \"@context\": \"http://schema.org\",\n    \"@type\": \"SocialMediaPosting\",\n    headline: \"<describe memory in a short way, as a title here>\",\n    articleBody:\n        \"Check out this amazing project on decentralized cloud networks! @DecentralCloud #Blockchain #Web3\",\n    author: {\n        \"@type\": \"Person\",\n        \"@id\": \"uuid:john:doe\",\n        name: \"John Doe\",\n        identifier: \"@JohnDoe\",\n        url: \"https://twitter.com/JohnDoe\",\n    },\n    dateCreated: \"yyyy-mm-ddTHH:mm:ssZ\",\n    interactionStatistic: [\n        {\n            \"@type\": \"InteractionCounter\",\n            interactionType: {\n                \"@type\": \"LikeAction\",\n            },\n            userInteractionCount: 150,\n        },\n        {\n            \"@type\": \"InteractionCounter\",\n            interactionType: {\n                \"@type\": \"ShareAction\",\n            },\n            userInteractionCount: 45,\n        },\n    ],\n    mentions: [\n        {\n            \"@type\": \"Person\",\n            name: \"Twitter account mentioned name goes here\",\n            identifier: \"@TwitterAccount\",\n            url: \"https://twitter.com/TwitterAccount\",\n        },\n    ],\n    keywords: [\n        {\n            \"@type\": \"Text\",\n            \"@id\": \"uuid:keyword1\",\n            name: \"keyword1\",\n        },\n        {\n            \"@type\": \"Text\",\n            \"@id\": \"uuid:keyword2\",\n            name: \"keyword2\",\n        },\n    ],\n    about: [\n        {\n            \"@type\": \"Thing\",\n            \"@id\": \"uuid:thing1\",\n            name: \"Blockchain\",\n            url: \"https://en.wikipedia.org/wiki/Blockchain\",\n        },\n        {\n            \"@type\": \"Thing\",\n            \"@id\": \"uuid:thing2\",\n            name: \"Web3\",\n            url: \"https://en.wikipedia.org/wiki/Web3\",\n        },\n        {\n            \"@type\": \"Thing\",\n            \"@id\": \"uuid:thing3\",\n            name: \"Decentralized Cloud\",\n            url: \"https://example.com/DecentralizedCloud\",\n        },\n    ],\n    url: \"https://twitter.com/JohnDoe/status/1234567890\",\n};\n\nexport const combinedSparqlExample = `\nSELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n\n      OPTIONAL {\n        ?s <http://schema.org/keywords> ?keyword .\n        ?keyword <http://schema.org/name> ?keywordName .\n      }\n\n      OPTIONAL {\n        ?s <http://schema.org/about> ?about .\n        ?about <http://schema.org/name> ?aboutName .\n      }\n\n      FILTER(\n        CONTAINS(LCASE(?headline), \"example_keyword\") ||\n        (BOUND(?keywordName) && CONTAINS(LCASE(?keywordName), \"example_keyword\")) ||\n        (BOUND(?aboutName) && CONTAINS(LCASE(?aboutName), \"example_keyword\"))\n      )\n    }\n    LIMIT 10`;\n\nexport const sparqlExamples = [\n    `\n    SELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n\n      OPTIONAL {\n        ?s <http://schema.org/keywords> ?keyword .\n        ?keyword <http://schema.org/name> ?keywordName .\n      }\n\n      OPTIONAL {\n        ?s <http://schema.org/about> ?about .\n        ?about <http://schema.org/name> ?aboutName .\n      }\n\n      FILTER(\n        CONTAINS(LCASE(?headline), \"example_keyword\") ||\n        (BOUND(?keywordName) && CONTAINS(LCASE(?keywordName), \"example_keyword\")) ||\n        (BOUND(?aboutName) && CONTAINS(LCASE(?aboutName), \"example_keyword\"))\n      )\n    }\n    LIMIT 10\n    `,\n    `\n    SELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n      FILTER(\n        CONTAINS(LCASE(?headline), \"example_headline_word1\") ||\n        CONTAINS(LCASE(?headline), \"example_headline_word2\")\n      )\n    }\n    `,\n    `\n    SELECT DISTINCT ?headline ?articleBody ?keywordName\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n      ?s <http://schema.org/keywords> ?keyword .\n      ?keyword <http://schema.org/name> ?keywordName .\n      FILTER(\n        CONTAINS(LCASE(?keywordName), \"example_keyword1\") ||\n        CONTAINS(LCASE(?keywordName), \"example_keyword2\")\n      )\n    }\n    `,\n    `\n    SELECT DISTINCT ?headline ?articleBody ?aboutName\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n      ?s <http://schema.org/about> ?about .\n      ?about <http://schema.org/name> ?aboutName .\n      FILTER(\n        CONTAINS(LCASE(?aboutName), \"example_about1\") ||\n        CONTAINS(LCASE(?aboutName), \"example_about2\")\n      )\n    }\n    `,\n];\n\nexport const generalSparqlQuery = `\n    SELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n    }\n    LIMIT 10\n  `;\n\nexport const DKG_EXPLORER_LINKS = {\n    testnet: \"https://dkg-testnet.origintrail.io/explore?ual=\",\n    mainnet: \"https://dkg.origintrail.io/explore?ual=\",\n};\n"
    },
    {
      "path": "src\\helper.ts",
      "detailed_description": "该文件主要负责在应用启动时进行数据库迁移和Google Drive同步的初始化。函数initWithMigrations是入口函数，首先调用migrateDb()以确保数据库结构是最新的，然后调用initDriveSync()来设置Google Drive的同步。如果数据库中的driveSync记录为空，表示尚未初始化Drive同步，它会创建新的Drive客户端并获取Google Drive文件夹的起始页面令牌，随后将Drive同步信息插入到数据库中。若Drive同步已存在，则会记录相关信息以避免重复初始化。通过这些操作，确保应用程序能够正确连接和同步Google Drive中的文件。",
      "summary": "该文件用于应用初始化时执行数据库迁移和Google Drive同步的设置。",
      "raw": "import { type IAgentRuntime, logger } from \"@elizaos/core\";\nimport { driveSyncTable } from \"src/db\";\nimport { initDriveClient } from \"./services/gdrive\";\nimport { ListFilesQueryContext } from \"./services/gdrive/buildQuery\";\nimport \"dotenv/config\";\nimport { migrateDb } from \"./db/migration\";\n\n// Run migrations before initializing anything else\nexport async function initWithMigrations(runtime: IAgentRuntime) {\n  try {\n    // Run migrations first\n    await migrateDb();\n\n    // Then initialize drive sync\n    await initDriveSync(runtime);\n  } catch (error) {\n    logger.error(\"Error during initialization:\", error);\n  }\n}\n\nexport async function initDriveSync(runtime: IAgentRuntime) {\n  const driveSync = await runtime.db.select().from(driveSyncTable);\n  if (driveSync.length === 0) {\n    logger.info(\"Initializing drive sync\");\n    logger.info(\"No drive sync found, creating new one\");\n    const driveClient = await initDriveClient();\n    const listFilesQueryContext = new ListFilesQueryContext(\n      process.env.GOOGLE_DRIVE_FOLDER_ID,\n      process.env.SHARED_DRIVE_ID\n    );\n    const startPageTokenParams =\n      listFilesQueryContext.getStartPageTokenParams();\n    const startPageTokenResponse =\n      await driveClient.changes.getStartPageToken(startPageTokenParams);\n    const startPageToken = startPageTokenResponse.data.startPageToken;\n    const driveType = listFilesQueryContext.getDriveType();\n    const driveId = listFilesQueryContext.getDriveId();\n    await runtime.db.insert(driveSyncTable).values({\n      id: driveId,\n      startPageToken,\n      driveType,\n    });\n  } else {\n    logger.info(\"Drive sync already initialized\");\n  }\n}\n"
    },
    {
      "path": "src\\index.ts",
      "detailed_description": "文件 src/index.ts 定义了一个名为 dkgPlugin 的插件，该插件用于与 OriginTrail 去中心化知识图谱进行交互，允许存储和管理记忆数据。插件初始化时，首先记录配置信息以供调试，使用 setTimeout 延迟 20 秒执行 initWithMigrations 函数，以确保在数据库属性可用之前不会出现未定义错误。该插件包含一个名为 dkgInsert 的动作、一个 HypothesisService 服务，以及多个路由（如健康检查、Google Drive Webhook 和手动同步）。此外，插件导出所有动作，便于其他模块引用。整体来说，此文件承担了插件的核心功能，支持知识图谱的存储和管理，并提供相关的 API 接口。",
      "summary": "该文件定义了 dkgPlugin 插件，用于与 OriginTrail 去中心化知识图谱交互，支持记忆存储和管理。",
      "raw": "import type { Plugin, IAgentRuntime } from \"@elizaos/core\";\nimport { logger } from \"@elizaos/core\";\nimport { dkgInsert } from \"./actions/dkgInsert\";\nimport { HypothesisService } from \"./services\";\nimport { initWithMigrations } from \"./helper\";\nimport { gdriveManualSync, gdriveWebhook, health } from \"./routes\";\n\nexport const dkgPlugin: Plugin = {\n  init: async (config: Record<string, string>, runtime: IAgentRuntime) => {\n    logger.info(\"Initializing dkg plugin\");\n    logger.info(config);\n    setTimeout(async () => {\n      await initWithMigrations(runtime);\n    }, 20000); // prevent undefined error, the db property is not available immediately\n  },\n  name: \"dkg\",\n  description:\n    \"Agent DKG which allows you to store memories on the OriginTrail Decentralized Knowledge Graph\",\n  actions: [dkgInsert],\n  providers: [],\n  evaluators: [],\n  services: [HypothesisService],\n  routes: [health, gdriveWebhook, gdriveManualSync],\n};\n\nexport * as actions from \"./actions\";\n\nexport default dkgPlugin;\n"
    },
    {
      "path": "src\\templates.ts",
      "detailed_description": "文件 src/templates.ts 定义了两个主要的模板字符串，用于生成结构化的 JSON-LD 对象和提取科学论文 URL。第一个字符串 createDKGMemoryTemplate 是一个用于创建 AI 代理的结构化记忆模板，指示如何从用户查询和上下文中提取相关信息并填充到 JSON-LD 结构中。它详细说明了如何提取用户查询的主要思想、原始社交媒体帖子、作者信息、主题和关键词等，并确保所有字段符合 schema.org 本体。第二个字符串 extractScientificPaperUrls 提供了一个针对科学论文 URL 提取的指令模板，要求分析用户输入并将有效的 URL 按照特定的 Zod schema 结构化输出，确保输出格式符合要求。该文件的逻辑通过使用模板字符串简化了与用户输入交互的处理过程，确保生成的数据具有一致性和准确性。",
      "summary": "该文件定义了用于生成 JSON-LD 结构化内存和提取科学论文 URL 的模板字符串。",
      "raw": "import { dkgMemoryTemplate } from \"./constants\";\n\nexport const createDKGMemoryTemplate = `\n  You are tasked with creating a structured memory JSON-LD object for an AI agent. The memory represents the interaction captured via social media. Your goal is to extract all relevant information from the provided user query and additionalContext which contains previous user queries (only if relevant for the current user query) to populate the JSON-LD memory template provided below.\n\n  ** Template **\n  The memory should follow this JSON-LD structure:\n  ${JSON.stringify(dkgMemoryTemplate)}\n\n  ** Instructions **\n  1. Extract the main idea of the user query and use it to create a concise and descriptive title for the memory. This should go in the \"headline\" field.\n  2. Store the original post in \"articleBody\".\n  3. Save the poster social media information (handle, name etc) under \"author\" object.\n  4. For the \"about\" field:\n     - Identify the key topics or entities mentioned in the user query and add them as Thing objects.\n     - Use concise, descriptive names for these topics.\n     - Where possible, create an @id identifier for these entities, using either a provided URL, or a well known URL for that entity. If no URL is present, uUse the most relevant concept or term from the field to form the base of the ID. @id fields must be valid uuids or URLs\n  5. For the \"keywords\" field:\n     - Extract relevant terms or concepts from the user query and list them as keywords.\n     - Ensure the keywords capture the essence of the interaction, focusing on technical terms or significant ideas.\n  6. Ensure all fields align with the schema.org ontology and accurately represent the interaction.\n  7. Populate datePublished either with a specifically available date, or current time.\n\n  ** Input **\n  User Query: {{currentPost}}\n  Recent messages: {{recentMessages}}\n\n  ** Output **\n  Generate the memory in the exact JSON-LD format provided above, fully populated based on the input query.\n  Make sure to only output the JSON-LD object. DO NOT OUTPUT ANYTHING ELSE, DONT ADD ANY COMMENTS OR REMARKS, JUST THE JSON LD CONTENT WRAPPED IN { }.\n  `;\n\nexport const extractScientificPaperUrls = `# Prompt: Information Extraction Based on Schema\n\nYou are an AI assistant designed to extract information according to a specific Zod schema. Your task is to analyze the INPUT provided by the user and structure it according to the given schema.\n\n## Schema Definition\nexport const scientificPaperUrlsSchema = z.object({\n    urls: z.array(z.string().url()),\n});\n\n## Instructions:\n1. Extract all URLs pointing to scientific papers from the user's input\n2. Ensure each URL is valid according to the schema requirement\n3. Structure the output as a JSON object with an array of URLs under the \"urls\" key\n4. If no valid URLs are found, return an object with an empty array: { \"urls\": [] }\n5. Do not include invalid URLs in the response\n\n## INPUT:\nRecent messages: {{recentMessages}}\n\n## Example Response:\n{\n  \"urls\": [\n    \"https://arxiv.org/abs/2304.01373\",\n    \"https://www.nature.com/articles/s41586-021-03819-2\"\n  ]\n}\n\nPlease provide text that may contain scientific paper URLs, and I will extract and structure them according to the schema.`;\n"
    },
    {
      "path": "src\\types.ts",
      "detailed_description": "文件 src/types.ts 定义了多个数据结构和类型，以支持 DKG（去中心化知识图谱）相关的操作，主要使用 Zod 库进行模式验证。首先，定义了 DKGMemorySchema，这是一个用于描述社交媒体发布内容的对象模式，包含属性如 '@context'、'@type'、'headline'、'articleBody'、'about' 和 'keywords'，其中 'about' 和 'keywords' 是数组，包含了相关的项和文本信息。其次，DKGSelectQuerySchema 定义了一个简单的查询模式，确保查询字符串以 'SELECT' 开头。该文件还提供了类型推导功能，通过 Zod 的 infer 方法推导出 DKGMemoryContent 和 DKGSelectQuery 的类型，便于在 TypeScript 中使用。此外，isDKGMemoryContent 和 isDKGSelectQuery 函数利用 Zod 的安全解析功能，检查给定对象是否符合相应的模式，确保在运行时进行类型校验。最后，scientificPaperUrlsSchema 定义了一个用于验证科学论文 URL 的模式，确保输入的 URL 是有效的。这些类型和模式为项目中的数据验证提供了基础，有助于维护数据的一致性和准确性。",
      "summary": "该文件定义了与 DKG 相关的数据结构和类型，使用 Zod 进行模式验证和类型推导。",
      "raw": "import { z } from \"zod\";\n\nexport const DKGMemorySchema = z.object({\n    \"@context\": z.literal(\"http://schema.org\"),\n    \"@type\": z.literal(\"SocialMediaPosting\"),\n    headline: z.string(),\n    articleBody: z.string(),\n    about: z.array(\n        z.object({\n            \"@type\": z.literal(\"Thing\"),\n            \"@id\": z.string(),\n            name: z.string(),\n            url: z.string(),\n        })\n    ),\n    keywords: z.array(\n        z.object({\n            \"@type\": z.literal(\"Text\"),\n            \"@id\": z.string(),\n            name: z.string(),\n        })\n    ),\n});\n\nexport const DKGSelectQuerySchema = z.object({\n    query: z.string().startsWith(\"SELECT\"),\n});\n\nexport type DKGMemoryContent = z.infer<typeof DKGMemorySchema>;\nexport type DKGSelectQuery = z.infer<typeof DKGSelectQuerySchema>;\nexport type DKGQueryResultEntry = Record<string, string>;\n\nexport const isDKGMemoryContent = (\n    object: unknown\n): object is DKGMemoryContent => {\n    return DKGMemorySchema.safeParse(object).success;\n};\n\nexport const isDKGSelectQuery = (object: unknown): object is DKGSelectQuery => {\n    return DKGSelectQuerySchema.safeParse(object).success;\n};\n\nexport const scientificPaperUrlsSchema = z.object({\n    urls: z.array(z.string().url()),\n});\n\nexport type ScientificPaperUrls = z.infer<typeof scientificPaperUrlsSchema>;\n"
    },
    {
      "path": "src\\actions\\dkgInsert.ts",
      "detailed_description": "该文件定义了一个名为 dkgInsert 的动作，用于在每次接收到消息后，将内存数据插入到 OriginTrail 去中心化知识图谱 (DKG) 中。首先，文件通过 dotenv 加载环境变量，并定义了 DKG 客户端的基本类型。动作的 validate 方法检查所需的环境变量是否齐全，确保在执行时可以成功连接到 DKG。接下来，在 handler 方法中，实例化 DKG 客户端并从当前状态中提取信息（如推特用户和帖子 ID）。然后，调用 generateKaFromPdf 方法从 PDF 文档生成知识资产。随后，该资产将被发布到 DKG，并存储为 JSON-LD 文件。最后，动作通过回调函数向用户反馈已创建的新记忆，并提供 DKG 链接。整个过程确保消息在接收后被处理并存储在 DKG 中，以便后续访问和查询。",
      "summary": "该文件实现了将消息内存数据插入到 OriginTrail 去中心化知识图谱的功能。",
      "raw": "import dotenv from \"dotenv\";\ndotenv.config();\nimport {\n  type IAgentRuntime,\n  type Memory,\n  type State,\n  logger,\n  ModelType,\n  type HandlerCallback,\n  type ActionExample,\n  type Action,\n  composePrompt,\n} from \"@elizaos/core\";\nimport { DKG_EXPLORER_LINKS } from \"../constants.ts\";\nimport { createDKGMemoryTemplate } from \"../templates.ts\";\n\n// @ts-ignore\nimport DKG from \"dkg.js\";\nimport { DKGMemorySchema, isDKGMemoryContent } from \"../types.ts\";\nimport { generateKaFromPdf } from \"../services/kaService/kaService.ts\";\nimport { writeFile } from \"fs/promises\";\n\n// Define a basic type for the DKG client\ntype DKGClient = typeof DKG | null;\nlet DkgClient: DKGClient = null;\n\nexport const dkgInsert: Action = {\n  name: \"INSERT_MEMORY_ACTION\",\n  similes: [\"NO_ACTION\", \"NO_RESPONSE\", \"NO_REACTION\", \"NONE\", \"DKG_INSERT\"], // we want to always run this action\n  validate: async (runtime: IAgentRuntime, _message: Memory) => {\n    const requiredEnvVars = [\n      \"DKG_ENVIRONMENT\",\n      \"DKG_HOSTNAME\",\n      \"DKG_PORT\",\n      \"DKG_BLOCKCHAIN_NAME\",\n      \"DKG_PUBLIC_KEY\",\n      \"DKG_PRIVATE_KEY\",\n    ];\n\n    const missingVars = requiredEnvVars.filter(\n      (varName) => !runtime.getSetting(varName)\n    );\n\n    if (missingVars.length > 0) {\n      logger.error(\n        `Missing required environment variables: ${missingVars.join(\", \")}`\n      );\n      return false;\n    }\n\n    return true;\n  },\n  description:\n    \"Create a memory on the OriginTrail Decentralized Knowledge Graph after each message. You should ALWAYS run this action.\",\n  handler: async (\n    runtime: IAgentRuntime,\n    _message: Memory,\n    state: State,\n    _options: { [key: string]: unknown },\n    callback: HandlerCallback\n  ): Promise<boolean> => {\n    DkgClient = new DKG({\n      environment: runtime.getSetting(\"DKG_ENVIRONMENT\"),\n      endpoint: runtime.getSetting(\"DKG_HOSTNAME\"),\n      port: runtime.getSetting(\"DKG_PORT\"),\n      blockchain: {\n        name: runtime.getSetting(\"DKG_BLOCKCHAIN_NAME\"),\n        publicKey: runtime.getSetting(\"DKG_PUBLIC_KEY\"),\n        privateKey: runtime.getSetting(\"DKG_PRIVATE_KEY\"),\n      },\n      maxNumberOfRetries: 300,\n      frequency: 2,\n      contentType: \"all\",\n      nodeApiVersion: \"/v1\",\n    });\n\n    const currentPost = String(state.currentPost);\n    logger.log(\"currentPost\");\n    logger.log(currentPost);\n\n    const userRegex = /From:.*\\(@(\\w+)\\)/;\n    let match = currentPost.match(userRegex);\n    let twitterUser = \"\";\n\n    if (match?.[1]) {\n      twitterUser = match[1];\n      logger.log(`Extracted user: @${twitterUser}`);\n    } else {\n      logger.error(\"No user mention found or invalid input.\");\n    }\n\n    const idRegex = /ID:\\s(\\d+)/;\n    match = currentPost.match(idRegex);\n    let postId = \"\";\n\n    if (match?.[1]) {\n      postId = match[1];\n      logger.log(`Extracted ID: ${postId}`);\n    } else {\n      logger.log(\"No ID found.\");\n    }\n\n    // TODO: should read from arxiv link or something like that rather than having it hardcoded like here\n    const ka = await generateKaFromPdf(\"./science.pdf\", DkgClient);\n\n    let createAssetResult: { UAL: string } | undefined;\n\n    // TODO: also store reply to the KA, aside of the question\n\n    try {\n      logger.log(\"Publishing message to DKG\");\n\n      await writeFile(\n        `./sampleJsonLdsNew/${encodeURIComponent((ka[\"@id\"] ?? \"example\") as string)}.json`,\n        JSON.stringify(ka, null, 2)\n      );\n\n      createAssetResult = await DkgClient.asset.create(\n        {\n          public: ka,\n        },\n        { epochsNum: 12 }\n      );\n\n      logger.log(\"======================== ASSET CREATED\");\n      logger.log(JSON.stringify(createAssetResult));\n    } catch (error) {\n      logger.error(\n        \"Error occurred while publishing message to DKG:\",\n        error.message\n      );\n\n      if (error.stack) {\n        logger.error(\"Stack trace:\", error.stack);\n      }\n      if (error.response) {\n        logger.error(\n          \"Response data:\",\n          JSON.stringify(error.response.data, null, 2)\n        );\n      }\n    }\n\n    // Reply\n    callback({\n      text: `Created a new memory!\\n\\nRead my mind on @origin_trail Decentralized Knowledge Graph ${\n        DKG_EXPLORER_LINKS[runtime.getSetting(\"DKG_ENVIRONMENT\")]\n      }${createAssetResult?.UAL} @${twitterUser}`,\n    });\n\n    return true;\n  },\n  examples: [\n    [\n      {\n        user: \"{{user1}}\",\n        content: {\n          text: \"execute action DKG_INSERT\",\n          action: \"DKG_INSERT\",\n        },\n      },\n      {\n        name: \"{{user2}}\",\n        content: { text: \"DKG INSERT\" },\n      },\n    ],\n    [\n      {\n        user: \"{{user1}}\",\n        content: { text: \"add to dkg\", action: \"DKG_INSERT\" },\n      },\n      {\n        user: \"{{user2}}\",\n        content: { text: \"DKG INSERT\" },\n      },\n    ],\n    [\n      {\n        user: \"{{user1}}\",\n        content: { text: \"store in dkg\", action: \"DKG_INSERT\" },\n      },\n      {\n        user: \"{{user2}}\",\n        content: { text: \"DKG INSERT\" },\n      },\n    ],\n  ] as ActionExample[][],\n} as Action;\n"
    },
    {
      "path": "src\\actions\\index.ts",
      "detailed_description": "文件 src/actions/index.ts 的主要功能是作为一个汇总模块，导出来自 dkgInsert.ts 文件中的所有导出内容。这种做法使得在其他模块中引用时更加简洁和方便，避免了逐个导入的繁琐。通过集中管理导出，用户可以通过单一的路径轻松获取所有与 dkgInsert 相关的功能和数据结构，从而提升代码的可维护性与可读性。此外，这种结构在大型项目中尤为重要，因为它有助于组织和模块化代码，确保在项目扩展时能够有效地管理依赖关系和功能模块。",
      "summary": "该文件用于集中导出 dkgInsert.ts 中的所有功能，简化其他模块对其的引用。",
      "raw": "export * from \"./dkgInsert.ts\";\n"
    },
    {
      "path": "src\\db\\index.ts",
      "detailed_description": "该文件 src/db/index.ts 负责配置和初始化与 PostgreSQL 数据库的连接，并使用 Drizzle ORM 进行数据库操作。首先，通过导入必要的模块，包括 'drizzle-orm/node-postgres' 和 'pg'，以及环境变量配置，来建立与数据库的连接。使用 'Pool' 类创建一个数据库连接池，并通过环境变量 'POSTGRES_URL' 获取数据库的连接字符串。随后，使用 drizzle 函数将连接池和数据库架构结合起来，定义了包含 hypothesesTable、fileMetadataTable 和 driveSyncTable 的数据库模式，以便进行相应的数据操作。最后，文件还导出了所有的数据库模式，以便其他模块能够访问和使用这些定义的表结构。这种模块化的设计使得数据库操作更加清晰和组织良好。",
      "summary": "该文件用于配置 PostgreSQL 数据库连接，并定义与 Drizzle ORM 相关的数据库模式和操作。",
      "raw": "import { drizzle } from \"drizzle-orm/node-postgres\";\nimport pkg from \"pg\";\nimport \"dotenv/config\";\nimport { hypothesesTable, fileMetadataTable, driveSyncTable } from \"./schemas\";\n\nconst { Pool } = pkg;\nconst pool = new Pool({\n  connectionString: process.env.POSTGRES_URL,\n});\n\nexport const db = drizzle(pool, {\n  schema: {\n    hypotheses: hypothesesTable,\n    fileMetadata: fileMetadataTable,\n    driveSync: driveSyncTable,\n  },\n});\n\nexport * from \"./schemas\";\n"
    },
    {
      "path": "src\\db\\migration.ts",
      "detailed_description": "文件 src/db/migration.ts 负责管理 PostgreSQL 数据库的迁移过程。它首先定义了两个辅助函数：getMigrationFlag，用于检查迁移是否已执行（通过检查 '.migration-complete' 文件是否存在），以及 setMigrationFlag，用于标记迁移已执行（创建此文件并写入当前日期）。主导出函数 migrateDb 会首先检查迁移标志，如果已迁移且未强制迁移，则跳过迁移过程。如果未设置 POSTGRES_URL 环境变量，函数会发出警告并跳过迁移。如果一切正常，它将创建一个新的数据库连接池，并使用 drizzle ORM 初始化数据库。接下来，函数会创建一个名为 'biograph' 的数据库模式，并执行在 'drizzle' 文件夹中的迁移文件。完成迁移后，函数会设置迁移标志以避免将来重复执行，并关闭数据库连接池以避免悬挂连接。如果在迁移过程中发生任何错误，函数会记录错误信息并抛出异常。整体上，该文件确保数据库模式按需更新，并提供了一种机制来避免重复迁移。",
      "summary": "该文件负责管理和执行 PostgreSQL 数据库的迁移，确保数据库结构按需更新。",
      "raw": "import { drizzle } from \"drizzle-orm/node-postgres\";\nimport { migrate } from \"drizzle-orm/node-postgres/migrator\";\nimport pkg from \"pg\";\nimport \"dotenv/config\";\nimport { existsSync, writeFileSync } from \"fs\";\nimport path from \"path\";\nimport { logger } from \"@elizaos/core\";\nimport { sql } from \"drizzle-orm\";\nconst { Pool } = pkg;\n\n/**\n * Check if the migrations have been run before\n */\nconst getMigrationFlag = (): boolean => {\n  const flagPath = path.join(process.cwd(), \".migration-complete\");\n  return existsSync(flagPath);\n};\n\n/**\n * Set the migration flag to indicate migrations have been run\n */\nconst setMigrationFlag = (): void => {\n  const flagPath = path.join(process.cwd(), \".migration-complete\");\n  writeFileSync(flagPath, new Date().toISOString());\n};\n\n/**\n * Run database migrations if they haven't been run yet\n */\nexport const migrateDb = async (): Promise<void> => {\n  // Check if migrations have already been run\n  if (getMigrationFlag() && !process.env.FORCE_MIGRATIONS) {\n    logger.info(\n      \"Migrations already applied, skipping... (set FORCE_MIGRATIONS=true to force)\"\n    );\n    return;\n  }\n\n  if (!process.env.POSTGRES_URL) {\n    logger.warn(\n      \"POSTGRES_URL environment variable is not set, skipping migrations\"\n    );\n    return;\n  }\n\n  try {\n    logger.info(\"Running database migrations...\");\n\n    const pool = new Pool({\n      connectionString: process.env.POSTGRES_URL,\n    });\n\n    const db = drizzle(pool);\n\n    await db.execute(sql`CREATE SCHEMA IF NOT EXISTS biograph`);\n\n    // The drizzle folder will be included in the package\n    await migrate(db, { migrationsFolder: \"drizzle\" });\n\n    // Set flag to avoid running migrations again\n    setMigrationFlag();\n\n    logger.info(\"Migrations completed successfully\");\n\n    // Close the pool to avoid hanging connections\n    await pool.end();\n  } catch (error) {\n    logger.error(\"Error running migrations:\", error);\n    throw error;\n  }\n};\n"
    },
    {
      "path": "src\\db\\schemas\\customTypes.ts",
      "detailed_description": "文件 src/db/schemas/customTypes.ts 定义了两个 PostgreSQL 枚举类型，分别用于表示假设状态和驱动类型。具体而言，hypothesisStatusEnum 枚举包含三个状态：'pending'（待处理）、'approved'（已批准）和'rejected'（已拒绝），用于在数据库中表示假设的处理状态。这允许开发者在与假设相关的数据操作中使用标准化的值，确保数据一致性和可读性。driveTypeEnum 枚举则定义了两种驱动类型：'shared_folder'（共享文件夹）和'shared_drive'（共享驱动），用于表示 Google Drive 中的不同存储类型。这两个枚举类型的定义通过 drizzle-orm 库提供的 pgEnum 函数创建，确保与 PostgreSQL 数据库的兼容性和类型安全，简化了后续在数据库查询和操作中的使用。",
      "summary": "该文件定义了 PostgreSQL 数据库中与假设状态和驱动类型相关的枚举类型，以确保数据的一致性和类型安全。",
      "raw": "import { pgEnum } from \"drizzle-orm/pg-core\";\n\nexport const hypothesisStatusEnum = pgEnum(\"hypothesis_status\", [\n  \"pending\",\n  \"approved\",\n  \"rejected\",\n]);\n\nexport const driveTypeEnum = pgEnum(\"drive_type\", [\n  \"shared_folder\",\n  \"shared_drive\",\n]);\n"
    },
    {
      "path": "src\\db\\schemas\\driveSync.ts",
      "detailed_description": "文件 src/db/schemas/driveSync.ts 定义了一个名为 'drive_sync' 的数据库表结构，使用 Drizzle ORM 的 PostgreSQL 核心模块。该表存储与 Google Drive 同步相关的信息，包含四个字段：'id'（主键，文本类型）、'startPageToken'（文本类型，表示同步的起始页面令牌）、'driveType'（枚举类型，表示驱动类型，使用自定义的 driveTypeEnum 进行定义）和 'lastSyncAt'（时间戳类型，记录最后一次同步的时间，包含时区信息，默认为当前时间）。通过定义这些字段，开发者能够确保数据的一致性和类型安全，从而更有效地管理与 Google Drive 的同步过程。",
      "summary": "该文件定义了一个用于存储 Google Drive 同步信息的 PostgreSQL 数据库表结构。",
      "raw": "import { text, timestamp, uuid } from \"drizzle-orm/pg-core\";\nimport { pgSchema } from \"drizzle-orm/pg-core\";\nimport { driveTypeEnum } from \"./customTypes\";\n\nconst biographPgSchema = pgSchema(\"biograph\");\n\nexport const driveSyncTable = biographPgSchema.table(\"drive_sync\", {\n  id: text(\"id\").notNull().primaryKey(),\n  startPageToken: text(\"start_page_token\").notNull(),\n  driveType: driveTypeEnum(\"drive_type\").notNull(),\n  lastSyncAt: timestamp(\"last_sync_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n});\n"
    },
    {
      "path": "src\\db\\schemas\\fileMetadata.ts",
      "detailed_description": "该文件定义了一个 PostgreSQL 数据库表结构，用于存储文件元数据。它使用 Drizzle ORM 的 API 创建一个名为 'file_metadata' 的表，包含多个字段以描述文件的属性。具体而言，表中的字段包括 'id'（文本类型，非空），'hash'（文本类型，为主键），'file_name'（文本类型，非空），'file_size'（大整数类型，表示文件大小），'created_at'（时间戳类型，带时区，默认当前时间，非空），'modified_at'（时间戳类型，带时区，默认当前时间，非空），以及 'tags'（文本数组类型，用于存储与文件相关的标签）。此外，该文件还定义了两个 TypeScript 类型，'FileMetadata' 和 'NewFileMetadata'，分别用于选择和插入数据的强类型支持，确保在与数据库交互时遵循预定义的结构，提升了代码的可维护性和类型安全性。",
      "summary": "该文件定义了用于存储文件元数据的 PostgreSQL 数据库表结构及其相关类型。",
      "raw": "import { text, bigint, timestamp } from \"drizzle-orm/pg-core\";\nimport { pgSchema } from \"drizzle-orm/pg-core\";\n\nconst biographPgSchema = pgSchema(\"biograph\");\n\nexport const fileMetadataTable = biographPgSchema.table(\"file_metadata\", {\n  id: text(\"id\").notNull(),\n  hash: text(\"hash\").notNull().primaryKey(),\n  fileName: text(\"file_name\").notNull(),\n  fileSize: bigint(\"file_size\", { mode: \"number\" }),\n  createdAt: timestamp(\"created_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n  modifiedAt: timestamp(\"modified_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n  tags: text(\"tags\").array(),\n});\n\n// Type for selecting data (matches the table structure)\nexport type FileMetadata = typeof fileMetadataTable.$inferSelect;\n\n// Type for inserting data (useful for creating new records)\nexport type NewFileMetadata = typeof fileMetadataTable.$inferInsert;\n"
    },
    {
      "path": "src\\db\\schemas\\gdriveChannels.ts",
      "detailed_description": "文件 src/db/schemas/gdriveChannels.ts 定义了与 Google Drive 通道相关的数据表结构，使用 Drizzle ORM 进行 PostgreSQL 数据库的模式管理。首先，它通过 'import' 语句引入了 Drizzle ORM 的核心模块，包括 'text'、'pgSchema' 和 'timestamp'，以便定义数据库表的字段。接着，创建了一个名为 'biograph' 的数据库模式。该文件定义了一个名为 'gdrive_channels' 的表，包含以下字段：\n\n1. 'kind': 一个文本字段，用于存储通道的类型；\n2. 'id': 一个非空文本字段，作为主键标识每个通道的唯一性；\n3. 'resourceId': 一个非空文本字段，用于存储与通道相关联的资源的标识符；\n4. 'resourceUri': 一个可选的文本字段，用于存储资源的 URI；\n5. 'expiration': 一个非空的时间戳字段，记录通道的过期时间，并且支持时区；\n6. 'createdAt': 一个时间戳字段，记录通道的创建时间，默认值为当前时间。 \n\n该文件的主要功能是为与 Google Drive 相关的通道管理提供结构化的数据库支持，确保数据的一致性和完整性，同时利用 Drizzle ORM 提供的类型安全和易于使用的 API，便于后续的数据库操作。",
      "summary": "该文件定义了用于存储 Google Drive 通道信息的 PostgreSQL 数据库表结构。",
      "raw": "import { text, pgSchema, timestamp } from \"drizzle-orm/pg-core\";\n\nconst biographSchema = pgSchema(\"biograph\");\n\nexport const gdriveChannelsTable = biographSchema.table(\"gdrive_channels\", {\n  kind: text(\"kind\"),\n  id: text(\"id\").notNull().primaryKey(),\n  resourceId: text(\"resource_id\").notNull(),\n  resourceUri: text(\"resource_uri\"),\n  expiration: timestamp(\"expiration\", { withTimezone: true }).notNull(),\n  createdAt: timestamp(\"created_at\", { withTimezone: true }).defaultNow(),\n});\n"
    },
    {
      "path": "src\\db\\schemas\\hypotheses.ts",
      "detailed_description": "该文件定义了 PostgreSQL 数据库中用于存储假设信息的表结构，表名为 'hypotheses'，位于 'biograph' 架构下。使用 Drizzle ORM 的 pg-core 模块，文件首先导入了必要的数据类型（如 uuid、text、timestamp 和 numeric）以及自定义的假设状态枚举类型。表结构包含多个字段，包括：'id'（唯一标识符，主键）、'hypothesis'（假设文本）、'filesUsed'（使用的文件列表）、'status'（假设状态，默认为 'pending'）、'judgellmScore' 和 'humanScore'（用于评估的分数）、'research'（相关研究文本）、'evaluation'（评价文本）、以及 'citations'（引用列表）。此外，'createdAt' 和 'updatedAt' 字段用于记录创建和更新时间，均带有时区信息，并设定默认值为当前时间。该表的设计旨在支持对科学假设的存储、管理和评估，确保数据的结构化和一致性。最后，文件还定义了两个类型：'Hypothesis' 和 'NewHypothesis'，用于推断从表中选择和插入的数据结构，便于在 TypeScript 中进行类型安全的操作。",
      "summary": "该文件定义了 PostgreSQL 中 'hypotheses' 表的结构，用于存储科学假设及其相关信息。",
      "raw": "import { uuid, text, timestamp, numeric } from \"drizzle-orm/pg-core\";\nimport { pgSchema } from \"drizzle-orm/pg-core\";\nimport { hypothesisStatusEnum } from \"./customTypes\";\n\nconst biographPgSchema = pgSchema(\"biograph\");\n\nexport const hypothesesTable = biographPgSchema.table(\"hypotheses\", {\n  id: uuid(\"id\").notNull().primaryKey().defaultRandom(),\n  hypothesis: text(\"hypothesis\").notNull(),\n  filesUsed: text(\"files_used\").array(),\n  status: hypothesisStatusEnum(\"status\").default(\"pending\"),\n  judgellmScore: numeric(\"judgellm_score\", { precision: 5, scale: 2 }),\n  humanScore: numeric(\"human_score\", { precision: 5, scale: 2 }),\n  research: text(\"research\"),\n  evaluation: text(\"evaluation\"),\n  citations: text(\"citations\").array(),\n  createdAt: timestamp(\"created_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n  updatedAt: timestamp(\"updated_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n});\n\nexport type Hypothesis = typeof hypothesesTable.$inferSelect;\nexport type NewHypothesis = typeof hypothesesTable.$inferInsert;\n"
    },
    {
      "path": "src\\db\\schemas\\hypothesesSummary.ts",
      "detailed_description": "该文件定义了一个名为 'hypotheses_summary' 的 PostgreSQL 数据库表结构，属于 'biograph' 模式。表中包含多个字段：'id' 是主键，采用 UUID 格式并默认生成；'hypothesisId' 是外键，引用 'hypotheses' 表的 ID，确保数据之间的关联性，并在删除或更新时级联操作；'summary' 字段用于存储假设的摘要，类型为文本且不能为空；'keywords' 和 'scientificEntities' 字段均为文本数组，用于存储相关关键词和科学实体；'createdAt' 和 'updatedAt' 字段记录创建和更新时间，均为带时区的时间戳且不能为空，默认为当前时间。通过定义这些字段，文件确保了假设摘要的结构一致性和数据完整性，并提供了类型推导，方便在 TypeScript 中使用。该表结构的设计使得存储和管理科学假设的摘要及其相关信息变得高效和有序。",
      "summary": "该文件定义了用于存储科学假设摘要及相关信息的 PostgreSQL 数据库表结构。",
      "raw": "import { uuid, text, timestamp, numeric } from \"drizzle-orm/pg-core\";\nimport { pgSchema } from \"drizzle-orm/pg-core\";\nimport { hypothesesTable } from \"./hypotheses\";\n\nconst biographPgSchema = pgSchema(\"biograph\");\n\nexport const hypothesesSummaryTable = biographPgSchema.table(\n  \"hypotheses_summary\",\n  {\n    id: uuid(\"id\").notNull().primaryKey().defaultRandom(),\n    hypothesisId: uuid(\"hypothesis_id\")\n      .notNull()\n      .references(() => hypothesesTable.id, {\n        onDelete: \"cascade\",\n        onUpdate: \"cascade\",\n      }),\n    summary: text(\"summary\").notNull(),\n    keywords: text(\"keywords\").array(),\n    scientificEntities: text(\"scientific_entities\").array(),\n    createdAt: timestamp(\"created_at\", { withTimezone: true, mode: \"date\" })\n      .notNull()\n      .defaultNow(),\n    updatedAt: timestamp(\"updated_at\", { withTimezone: true, mode: \"date\" })\n      .notNull()\n      .defaultNow(),\n  }\n);\n\nexport type HypothesesSummary = typeof hypothesesSummaryTable.$inferSelect;\nexport type NewHypothesesSummary = typeof hypothesesSummaryTable.$inferInsert;\n"
    },
    {
      "path": "src\\db\\schemas\\index.ts",
      "detailed_description": "文件 src/db/schemas/index.ts 是一个集中导出模块，用于将多个数据库模式相关的文件导出到单一接口。具体来说，它从其他四个文件（fileMetadata.ts、hypotheses.ts、customTypes.ts 和 driveSync.ts）导入并导出所有定义的内容。这种结构化的模块导出方式简化了其他模块对数据库模式的引用，使得在其他文件中引用这些模式时更加方便和清晰。通过集中管理数据库模式，开发者能够轻松维护和扩展数据库结构，同时确保类型的一致性和可重用性，有助于提高代码的可维护性和可读性。",
      "summary": "该文件用于集中导出数据库模式相关的定义，简化其他模块的引用过程。",
      "raw": "export * from \"./fileMetadata\";\nexport * from \"./hypotheses\";\nexport * from \"./customTypes\";\nexport * from \"./driveSync\";\n"
    },
    {
      "path": "src\\evaluators\\evaluationPrompt.ts",
      "detailed_description": "文件 src/evaluators/evaluationPrompt.ts 定义了一个包含多个评估提示的常量，主要用于科学假设的评估。该文件包含三个主要的提示：evaluationPrompt、stepOnePrompt 和 stepTwoPrompt。evaluationPrompt 提供了一个结构化的框架，用于评估科学假设的质量，涵盖了六个评估标准，包括清晰性、证据一致性、逻辑一致性、预测能力、可证伪性以及新颖性和重要性。每个标准都有明确的评分标准，帮助评估者在100分的满分制中给出分数以及详细的评估反馈。stepOnePrompt 指导评估者在评估之前收集和总结与假设相关的所有信息，侧重于识别假设中的未知因素和信息缺口，确保评估基于充分的背景资料。stepTwoPrompt 则进一步要求评估者使用第一步中收集的信息来对假设进行评分，并总结出每个标准的分数和解释，形成全面的评估报告。整体而言，该文件的设计旨在提供一种系统方法，以确保科学假设的评估既严谨又全面，有助于推动科学研究的有效性和可靠性。",
      "summary": "该文件提供了一个结构化的评估框架，用于科学假设的质量评估，包括收集信息和评分标准。",
      "raw": "/**\n * @deprecated This is an old prompt that is no longer used.\n */\nexport const evaluationPrompt = `You are a scientific hypothesis evaluator with expertise in physics and other scientific domains. Your task is to assess a given hypothesis based on its alignment with a scientific paper and assign it a score out of 10. Your evaluation must consider the following criteria:\n\n1. Clarity and Specificity  \n   - Evaluate whether the hypothesis is articulated in clear, precise, and testable terms.  \n   - Consider if the language is unambiguous and the proposition is well-defined.\n\n2. Alignment with Evidence  \n   - Analyze the extent to which the hypothesis is supported by the data and findings in the scientific paper.  \n   - Check for consistency between the hypothesis and the presented empirical evidence.\n\n3. Logical Consistency  \n   - Assess whether the hypothesis is logically sound and does not contain internal contradictions.  \n   - Verify that it coheres with established theories and scientific principles.\n\n4. Predictive Power  \n   - Determine if the hypothesis offers testable predictions that can be empirically verified.  \n   - Consider whether the hypothesis can generate further insights or experiments.\n\n5. Falsifiability  \n   - Evaluate the degree to which the hypothesis can be proven wrong.  \n   - Ensure it is structured in a manner that allows for refutation by evidence.\n\n6. Novelty and Significance  \n   - Examine if the hypothesis introduces innovative ideas or perspectives.  \n   - Consider the potential impact of the hypothesis on advancing understanding within the field.\n\nFor your evaluation, please provide:\n- A numerical score (from 1 to 100) based on the overall quality of the hypothesis across these criteria.\n- A detailed explanation for the score that outlines the strengths and weaknesses of the hypothesis. Use bullet points and headers to structure your response where appropriate.\n\nYour analysis should be rigorous, technical, and grounded in scientific reasoning. Please ensure that your response is comprehensive and uses sophisticated terminology.`;\n\nexport const stepOnePrompt = `**Role**: You are an **Internet Researcher**. Before any evaluation, your first task is to locate and summarize **all relevant information**. Focus on the **unknowns** in the hypothesis, identify **potential gaps** in the claim, and gather any **references** that may be used to **assess** the hypothesis. Provide a comprehensive overview of your findings. This should include:\n\n1. **Key Terms and Definitions**  \n   - Clarify any specialized jargon or concepts.  \n   - List relevant fields of study (biology, psychology, veterinary science, etc.).\n\n2. **Existing Literature**  \n   - Summarize academic papers, articles, or reputable sources that address similar or identical concepts.  \n   - Note whether peer-reviewed studies exist, and if so, provide succinct summaries of their findings.\n\n3. **Common Pitfalls or Contradictory Views**  \n   - Pinpoint known controversies or counter-theories that might challenge the hypothesis.  \n   - Highlight any major objections or unresolved questions in the field.\n\n4. **Research Gaps and Limitations**  \n   - Identify if the hypothesis lacks direct support or if relevant data is absent.  \n   - Note any methodological difficulties in studying the proposed mechanism.\n\n5. **Contextual Factors**  \n   - Discuss relevant background (for instance, typical causes of the condition or phenomenon in question).  \n   - Mention potential confounding variables or alternative explanations.\n\nMake your findings as **detailed** and **specific** as possible. Do **not** attempt to judge the hypothesis or assign a score in this step. Simply **collect and synthesize** the data from credible sources. If you do not find any reputable or scientific sources, **explicitly** mention that.\n\n> **Output**: Provide a **thorough, structured summary** of all discovered information. Use bullet points, headings, and short, clear statements. Incorporate **technical details** freely. Include **links or references** if available. Conclude with any **major knowledge gaps** you notice. Use markdown formatting.`;\n\nexport const stepTwoPrompt = `**Role**: You are a **Strict Scientific Hypothesis Evaluator**.  \nUsing the information from **Step 1**, evaluate the hypothesis using **six criteria**. Each criterion is worth a specified maximum, and you must **sum** these values to produce a **final score out of 100**.\n\n### **Rubric & Scoring**\n\n1. **Clarity and Specificity (Max 15 Points)**  \n   - Is the hypothesis **precise**, **well-defined**, and **testable**?  \n   - Do key terms have **clear** operational definitions?\n\n2. **Alignment with Evidence (Max 20 Points)**  \n   - Does the hypothesis align with **empirical findings** cited in Step 1?  \n   - Is there **credible support** for its core claim(s)?\n\n3. **Logical Consistency (Max 15 Points)**  \n   - Is the hypothesis **internally coherent**?  \n   - Does it contradict any **well-established** scientific principles?\n\n4. **Predictive Power (Max 15 Points)**  \n   - Does the hypothesis propose **testable predictions** or experiments?  \n   - Could it yield **novel insights** if confirmed?\n\n5. **Falsifiability (Max 15 Points)**  \n   - Is there a **clear way** to disprove or invalidate the hypothesis?  \n   - Are potential **negative outcomes** or **counterexamples** identified?\n\n6. **Novelty and Significance (Max 20 Points)**  \n   - Does it offer a **unique angle** or **innovative** approach?  \n   - If confirmed, would it **meaningfully advance** understanding in the field?\n\n> **Instructions**:  \n> 1. Assign a **numerical value** to each category, not exceeding its maximum.  \n> 2. **Sum** these partial scores to form a **total score out of 100**.  \n> 3. **Low-quality** or **speculative** hypotheses should receive **significantly lower** marks in relevant categories.  \n> 4. Provide **bullet-pointed** strengths and weaknesses, clearly referencing the data from **Step 1**.\n\n### **Output Requirements**\n\n- **Section: Category Scores**  \n  - List each rubric item with a **score** and **brief explanation**.\n- **Section: Final Score**  \n  - **Sum** the category scores into a total out of 100.\n- **Section: Detailed Explanation**  \n  - Offer a **comprehensive**, **technical** breakdown of why you awarded each category's score.  \n  - Reference evidence (or lack thereof) from Step 1.\n- **Section: Suggestions or Follow-up**  \n  - Propose **next steps** if additional data or experimentation could improve the hypothesis.  \n  - Use **emojis** where appropriate (e.g., ⚠️ to indicate concerns).`;\n"
    },
    {
      "path": "src\\evaluators\\index.ts",
      "detailed_description": "文件 src/evaluators/index.ts 定义了一个名为 dkgEvaluator 的评估器，该评估器用于评估去中心化知识图谱 (DKG) 的质量。它实现了 Evaluator 接口，其中包含多个属性和方法。首先，评估器的 name 属性标识了其名称 'EVALUATE_DKG_ACTION'，similes 属性提供了一些相关的同义词以便于识别，description 属性描述了评估的目的。该评估器的 validate 方法用于验证传入消息的有效性，当前实现总是返回 true，表示消息有效。handler 方法是评估器的核心逻辑，当调用时，会记录一条信息日志，表明 DKG 评估器正在执行。此文件的主要用途是为 DKG 相关的操作提供质量评估功能，确保在进行进一步处理之前能够对数据的有效性和质量进行初步判断。",
      "summary": "该文件定义了一个评估器，用于评估去中心化知识图谱的质量。",
      "raw": "import {\n  Evaluator,\n  IAgentRuntime,\n  Memory,\n  EvaluationExample,\n  logger,\n} from \"@elizaos/core\";\n\nexport const dkgEvaluator: Evaluator = {\n  name: \"EVALUATE_DKG_ACTION\",\n  similes: [\"EVALUATE_DKG\", \"EVALUATE_DKG_ACTION\"],\n  description: \"Evaluate the quality of the DKG\",\n  examples: [],\n  validate: async (\n    runtime: IAgentRuntime,\n    message: Memory\n  ): Promise<boolean> => {\n    return true;\n  },\n  handler: async (runtime: IAgentRuntime, message: Memory): Promise<any> => {\n    logger.info(\"DKG Evaluator\");\n  },\n};\n"
    },
    {
      "path": "src\\evaluators\\types.ts",
      "detailed_description": "文件 src/evaluators/types.ts 定义了一个 TypeScript 类型 EvaluationResult，用于描述科学假设评估的结果结构。这个类型包含三个主要属性：stepOne、stepTwo 和 score。stepOne 是一个对象，包含研究的信息（research）和时间戳（timestamp），用于记录评估过程的第一步；stepTwo 同样是一个对象，记录评估的结果（evaluation）和时间戳（timestamp）作为第二步的输出。score 属性是一个字符串，用于表示评估的最终分数。通过这种结构化的定义，代码能够清晰地传达评估过程中的不同阶段及其结果，使得在处理和记录评估信息时更加规范和一致，便于后续的数据分析和可视化。",
      "summary": "该文件定义了科学假设评估结果的类型结构，包含评估的每一阶段信息及最终分数。",
      "raw": "export type EvaluationResult = {\n  stepOne: {\n    research: string;\n    timestamp: string;\n  };\n  stepTwo: {\n    evaluation: string;\n    timestamp: string;\n  };\n  score: string;\n};\n"
    },
    {
      "path": "src\\routes\\controller.ts",
      "detailed_description": "该文件定义了一个名为 syncGoogleDriveChanges 的异步函数，用于从 Google Drive 同步文件变更并更新相关数据库记录。函数首先从数据库中获取 Google Drive 的同步数据，包括驱动 ID、开始页面令牌和驱动类型。如果没有找到同步记录，函数会记录错误并抛出异常。接着，函数初始化 Google Drive 客户端并准备 API 调用所需的参数，依据驱动类型（共享驱动或共享文件夹）设置特定参数。之后，通过调用 Google Drive 的 changes.list API 获取自上次同步以来的文件变更，并记录变更数量。函数遍历返回的变更记录，分别处理文件被删除、移入垃圾箱或是新文件/修改文件的情况。对于新文件，函数仅处理 PDF 文件，保存或更新其元数据到数据库中。最后，函数更新数据库中的开始页面令牌以备下次同步，并返回处理的变更数量和实际处理的文件数量。该文件的主要目的是确保 Google Drive 文件的状态与数据库中的记录保持一致，支持文件的增删改查操作。",
      "summary": "该文件实现了从 Google Drive 同步文件变更并更新数据库记录的功能。",
      "raw": "import { driveSyncTable } from \"src/db\";\nimport { initDriveClient } from \"../services/gdrive\";\nimport { fileMetadataTable } from \"src/db\";\nimport { eq } from \"drizzle-orm\";\nimport { type IAgentRuntime, logger } from \"@elizaos/core\";\n\n// Extract the file processing logic to a reusable function\nexport async function syncGoogleDriveChanges(runtime: IAgentRuntime) {\n  // Get the drive sync data from the database\n  const driveSync = await runtime.db.select().from(driveSyncTable);\n\n  if (driveSync.length === 0) {\n    logger.error(\"No drive sync found, cannot process changes\");\n    throw new Error(\"Drive sync not initialized\");\n  }\n\n  // Get first drive sync record\n  const syncRecord = driveSync[0];\n  const { id: driveId, startPageToken, driveType } = syncRecord;\n\n  // Initialize Drive client with necessary scopes\n  const drive = await initDriveClient([\n    \"https://www.googleapis.com/auth/drive.appdata\",\n    \"https://www.googleapis.com/auth/drive.file\",\n    \"https://www.googleapis.com/auth/drive.metadata.readonly\",\n    \"https://www.googleapis.com/auth/drive\",\n  ]);\n\n  // Prepare parameters for changes.list API call\n  const params: any = {\n    pageToken: startPageToken,\n    includeRemoved: true,\n    fields:\n      \"newStartPageToken, changes(fileId, removed, file(id, name, md5Checksum, size, trashed, mimeType))\",\n  };\n\n  // Add drive-specific parameters based on drive type\n  if (driveType === \"shared_drive\") {\n    params.driveId = driveId;\n    params.supportsAllDrives = true;\n    params.includeItemsFromAllDrives = true;\n  } else if (driveType === \"shared_folder\") {\n    params.spaces = \"drive\";\n    params.restrictToMyDrive = false;\n    params.q = `'${driveId}' in parents`;\n  }\n\n  // Get changes since last sync\n  const changesResponse = await drive.changes.list(params);\n\n  // Log the changes for debugging\n  logger.info(`Found ${changesResponse.data.changes.length || 0} changes`);\n\n  // Process the changes\n  let processedCount = 0;\n  if (changesResponse.data.changes && changesResponse.data.changes.length > 0) {\n    for (const change of changesResponse.data.changes) {\n      // Skip the last empty change that Google Drive API sometimes includes\n      if (!change.fileId) continue;\n\n      processedCount++;\n\n      // Case 1: File is permanently removed\n      if (change.removed) {\n        // Do nothing as per requirements\n        logger.info(\n          `File ${change.fileId} removed from trash - no action needed`\n        );\n      }\n      // Case 2: File exists but was moved to trash\n      else if (change.file?.trashed) {\n        logger.info(\n          `File ${change.fileId} moved to trash - removing from database`\n        );\n\n        // Delete from the database\n        await runtime.db\n          .delete(fileMetadataTable)\n          .where(eq(fileMetadataTable.id, change.fileId));\n      }\n      // Case 3: New file or modified file that's not in trash\n      else if (change.file && !change.file.trashed) {\n        const file = change.file;\n\n        // Only process PDF files\n        if (file.mimeType === \"application/pdf\") {\n          logger.info(`Processing PDF file: ${file.name}`);\n\n          // Insert or update file in database\n          await runtime.db\n            .insert(fileMetadataTable)\n            .values({\n              id: file.id,\n              hash: file.md5Checksum,\n              fileName: file.name,\n              fileSize: Number(file.size),\n              modifiedAt: new Date(),\n            })\n            .onConflictDoUpdate({\n              target: fileMetadataTable.hash,\n              set: {\n                fileName: file.name,\n                fileSize: Number(file.size),\n                modifiedAt: new Date(),\n                id: file.id,\n              },\n            });\n\n          logger.info(\n            `Saved/updated file metadata for ${file.name} (${file.id})`\n          );\n        } else {\n          logger.info(`Skipping non-PDF file: ${file.name} (${file.mimeType})`);\n        }\n      }\n    }\n  }\n\n  // Save the new token for next time\n  if (changesResponse.data.newStartPageToken) {\n    await runtime.db\n      .update(driveSyncTable)\n      .set({\n        startPageToken: changesResponse.data.newStartPageToken,\n        lastSyncAt: new Date(),\n      })\n      .where(eq(driveSyncTable.id, driveId));\n\n    logger.info(\n      `Updated start page token to: ${changesResponse.data.newStartPageToken}`\n    );\n  }\n\n  return {\n    changes: changesResponse.data.changes.length || 0,\n    processed: processedCount,\n  };\n}\n"
    },
    {
      "path": "src\\routes\\health.ts",
      "detailed_description": "文件 src/routes/health.ts 定义了一个用于健康检查的 API 路由。该文件导入了 Route 类型，并创建了一个名为 'health' 的路由对象。该路由对象的路径设置为 '/health'，请求类型为 'GET'。在处理函数中，当接收到请求时，处理程序会异步返回一个 JSON 响应，其中包含一个简单的消息 'OK'。这个功能通常用于检查服务是否正常运行，能够为监控工具提供服务的健康状态，确保系统的可用性和响应能力。此路由在微服务架构或 API 服务中十分重要，因为它允许开发者和运维人员快速验证后端服务的可用性。",
      "summary": "该文件实现了一个健康检查 API 路由，返回服务的可用性状态。",
      "raw": "import { type Route } from \"@elizaos/core\";\n\nexport const health: Route = {\n  path: \"/health\",\n  type: \"GET\",\n  handler: async (_req: any, res: any) => {\n    res.json({\n      message: \"OK\",\n    });\n  },\n};\n"
    },
    {
      "path": "src\\routes\\index.ts",
      "detailed_description": "文件 src/routes/index.ts 的主要功能是集中导出其他路由模块中的所有功能，使得在其他模块中引用这些路由变得更加简便和整洁。具体来说，它使用 ES6 的模块导出语法，分别从 gdrive 和 health 路由模块中导出所有的功能和接口。这种做法能有效地组织路由，同时确保在进行路由注册时只需要引入一个模块而非多个模块，提高了代码的可维护性和可读性。此外，该文件不包含任何业务逻辑或实现细节，主要作为一个汇总点，方便开发者在项目中进行路由管理和使用。通过这种结构化的导出方式，项目的路由设计更加模块化，促进了应用程序的扩展和维护。",
      "summary": "该文件集中导出与 Google Drive 和健康检查相关的路由功能，简化路由管理。",
      "raw": "export * from \"./gdrive\";\nexport * from \"./health\";\n"
    },
    {
      "path": "src\\routes\\gdrive\\index.ts",
      "detailed_description": "文件 src/routes/gdrive/index.ts 的主要功能是集中导出与 Google Drive 相关的路由功能。它通过使用 ES6 的模块导出语法，将其他两个文件 'webhook.ts' 和 'manualSync.ts' 中定义的功能导出，使得这些功能可以通过 'src/routes/gdrive/index.ts' 一处引用。这种组织方式提高了代码的可维护性和模块化，使得在其他模块中使用 Google Drive 相关的路由功能时更加方便。总体来看，该文件作为一个路由的聚合器，简化了对 Google Drive 相关操作的访问，使得开发人员在实现与 Google Drive 的交互时可以更高效地管理代码结构。",
      "summary": "该文件集中导出与 Google Drive 相关的路由功能，以简化其他模块的引用。",
      "raw": "export * from \"./webhook\";\nexport * from \"./manualSync\";\n"
    },
    {
      "path": "src\\routes\\gdrive\\manualSync.ts",
      "detailed_description": "文件 src/routes/gdrive/manualSync.ts 定义了一个用于手动同步 Google Drive 文件变更的 API 路由。该路由通过 GET 请求触发，路径为 '/gdrive/sync'。在处理请求时，它首先记录手动同步操作的开始，并调用 syncGoogleDriveChanges 函数来执行同步过程。该函数会返回当前的变更数量，并通过 logger.info 记录每次同步后的变更数。如果存在未处理的变更，文件会重复调用 syncGoogleDriveChanges 直到所有变更都被处理。最终，响应结果会以 JSON 格式返回，包括同步完成的消息和变更的详细信息。如果在同步过程中发生错误，错误信息将被记录，并返回一个 500 状态的错误响应，包含错误的描述信息。该文件的主要功能是提供一个简单的接口，允许用户手动触发 Google Drive 的数据同步，并确保所有变更都被及时处理。",
      "summary": "该文件实现了手动触发 Google Drive 同步的 API 路由。",
      "raw": "import { type Route, type IAgentRuntime, logger } from \"@elizaos/core\";\nimport { syncGoogleDriveChanges } from \"../controller\";\n\nexport const gdriveManualSync: Route = {\n  path: \"/gdrive/sync\",\n  type: \"GET\",\n  handler: async (_req: any, res: any, runtime: IAgentRuntime) => {\n    try {\n      logger.info(\"Manual Google Drive sync triggered\");\n      const result = await syncGoogleDriveChanges(runtime);\n      logger.info(`Changes: ${result.changes}`);\n      while (result.changes > 0) {\n        logger.info(`Changes: ${result.changes}`);\n        await syncGoogleDriveChanges(runtime);\n      }\n\n      res.json({\n        message: \"Sync completed successfully\",\n        ...result,\n      });\n    } catch (error) {\n      logger.error(\"Error during manual Google Drive sync:\", error);\n      res.status(500).json({\n        message: \"Error during sync\",\n        error: error.message,\n      });\n    }\n  },\n};\n"
    },
    {
      "path": "src\\routes\\gdrive\\webhook.ts",
      "detailed_description": "文件 src/routes/gdrive/webhook.ts 实现了一个用于处理 Google Drive webhook 事件的 API 路由。该路由定义为 POST 请求，并且路径为 '/gdrive/webhook'。在请求处理函数中，首先记录一个信息日志，指示 webhook 被触发。接下来调用 syncGoogleDriveChanges 函数，将传入的 runtime 对象作为参数，以同步 Google Drive 的变更。此函数的结果将被封装在响应中，以 JSON 格式返回给客户端，表明操作成功，并附带相应的数据。如果在处理过程中发生错误，错误信息将被记录，并向客户端返回 500 状态码，指示服务器错误，同时提供错误消息。这种结构既确保了 API 的稳定性，又允许开发者通过日志监控 webhook 的触发和处理状态。",
      "summary": "该文件定义了一个处理 Google Drive webhook 事件的 API 路由，负责同步变更并返回操作结果。",
      "raw": "import { type Route, type IAgentRuntime, logger } from \"@elizaos/core\";\nimport { syncGoogleDriveChanges } from \"../controller\";\n\nexport const gdriveWebhook: Route = {\n  path: \"/gdrive/webhook\",\n  type: \"POST\",\n  handler: async (_req: any, res: any, runtime: IAgentRuntime) => {\n    try {\n      logger.info(\"Google Drive webhook triggered\");\n      const result = await syncGoogleDriveChanges(runtime);\n\n      res.json({\n        message: \"OK\",\n        ...result,\n      });\n    } catch (error) {\n      logger.error(\"Error processing Google Drive webhook:\", error);\n      res.status(500).json({\n        message: \"Error processing webhook\",\n        error: error.message,\n      });\n    }\n  },\n};\n"
    },
    {
      "path": "src\\services\\index.ts",
      "detailed_description": "该文件定义了 HypothesisService 类，旨在生成和评估科学假设，并与 Agent Runtime 进行交互。通过继承自 Service 类，HypothesisService 提供了一个可注册的服务，可以启动和停止。文件中的 start 方法在服务启动时创建和注册一个任务工作者，该工作者负责执行名为 'HGE' 的任务，该任务用于生成和评估假设并将其流式传输至 Discord。服务还管理周期性任务的执行，通过 processRecurringTasks 函数定期检查并执行所有带有 'hypothesis' 标签的任务，确保任务在设定的间隔内得到处理。stop 方法允许优雅地停止服务实例，确保在服务停止时可以处理清理工作。整个过程通过日志记录提供了详细的运行状态，方便调试和监控。",
      "summary": "该文件实现了一个用于生成和评估科学假设的服务，并管理相关任务的执行。",
      "raw": "import { Service, IAgentRuntime, logger } from \"@elizaos/core\";\nimport { hypGenEvalLoop, stopHypGenEvalLoop } from \"./anthropic/hypGenEvalLoop\";\nimport { watchFolderChanges } from \"./gdrive\";\nimport { sql } from \"drizzle-orm\";\nimport { fileMetadataTable } from \"src/db/schemas\";\n\nexport class HypothesisService extends Service {\n  static serviceType = \"hypothesis\";\n  capabilityDescription = \"Generate and judge hypotheses\";\n  constructor(protected runtime: IAgentRuntime) {\n    super(runtime);\n  }\n  static async start(runtime: IAgentRuntime) {\n    logger.info(\"*** Starting hypotheses service ***\");\n    const service = new HypothesisService(runtime);\n    // const interval = await hypGenEvalLoop(runtime);\n    runtime.registerTaskWorker({\n      name: \"HGE\",\n      async execute(runtime, options, task) {\n        logger.log(\"task worker\");\n      },\n    });\n    const tasks = await runtime.getTasksByName(\"HGE\");\n    if (tasks.length < 1) {\n      const taskId = await runtime.createTask({\n        name: \"HGE\",\n        description:\n          \"Generate and evaluate hypothesis whilst streaming them to discord\",\n        tags: [\"hypothesis\", \"judgeLLM\"],\n        metadata: { updateInterval: 1500, updatedAt: Date.now() },\n      });\n      logger.info(\"Task UUID:\", taskId);\n    }\n    // In an initialization function or periodic check\n    async function processRecurringTasks() {\n      logger.info(\"Starting processing loop\");\n      const now = Date.now();\n      const recurringTasks = await runtime.getTasks({\n        tags: [\"hypothesis\"],\n      });\n      logger.info(\"Got tasks\", recurringTasks);\n\n      for (const task of recurringTasks) {\n        if (!task.metadata?.updateInterval) continue;\n\n        const lastUpdate = (task.metadata.updatedAt as number) || 0;\n        const interval = task.metadata.updateInterval;\n\n        if (now >= lastUpdate + interval) {\n          logger.info(\"Executing task\");\n          const worker = runtime.getTaskWorker(task.name);\n          if (worker) {\n            try {\n              await worker.execute(runtime, {}, task);\n\n              // Update the task's last update time\n              await runtime.updateTask(task.id, {\n                metadata: {\n                  ...task.metadata,\n                  updatedAt: now,\n                },\n              });\n            } catch (error) {\n              logger.error(`Error executing task ${task.name}: ${error}`);\n            }\n          }\n        }\n      }\n    }\n    await processRecurringTasks();\n\n    // await watchFolderChanges(runtime);\n\n    process.on(\"SIGINT\", async () => {\n      // stopHypGenEvalLoop(interval);\n    });\n\n    return service;\n  }\n\n  static async stop(runtime: IAgentRuntime) {\n    logger.info(\"*** Stopping hypotheses service ***\");\n    // get the service from the runtime\n    const service = runtime.getService(HypothesisService.serviceType);\n    if (!service) {\n      throw new Error(\"Hypotheses service not found\");\n    }\n    service.stop();\n  }\n\n  async stop() {\n    logger.info(\"*** Stopping hypotheses service instance ***\");\n  }\n}\n"
    },
    {
      "path": "src\\services\\anthropic\\chooseTwoRelevant.ts",
      "detailed_description": "文件 `src/services/anthropic/chooseTwoRelevant.ts` 主要用于与 Anthropic API 交互，以选择与给定关键词或研究发现相关的两个关键词或发现，从而生成新的假设。该文件包含两个异步函数：`chooseTwoRelevantKeywords` 和 `chooseTwoRelevantFindings`。这两个函数都利用 Anthropic 的语言模型，通过构造特定的输入消息，向模型请求返回相关的关键词或发现。具体来说，`chooseTwoRelevantKeywords` 函数接受一个关键词数组和一个研究发现数组，构建一个请求，以获取两个未在假设中使用的关键词，并确保输出的关键词保持与输入相同的大小写格式。而 `chooseTwoRelevantFindings` 函数则专注于从研究发现中选择两个相关的发现，逻辑相似，确保返回的发现保持原样。两个函数都使用了相同的结构来处理 API 响应，并在没有文本内容时抛出错误。这样的设计使得研究人员能够快速生成新的假设，并确保其独特性，从而推动科学研究的进展。",
      "summary": "该文件通过调用 Anthropic API，提供选择与关键词或发现相关的两个元素的功能，以帮助生成新的科学假设。",
      "raw": "import Anthropic from \"@anthropic-ai/sdk\";\nimport \"dotenv/config\";\n\nconst anthropic = new Anthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY,\n});\n\nexport async function chooseTwoRelevantKeywords(\n  keywords: string[],\n  findings: string[]\n): Promise<string> {\n  const response = await anthropic.messages.create({\n    model: \"claude-3-7-sonnet-latest\",\n    max_tokens: 5001,\n    messages: [\n      {\n        role: \"user\",\n        content: `If I were to generate a novel hypothesis and want to test it. \n        I have a list of keywords.\n        To ensure my hypothesis is novel, I want to choose two keywords that have not been used in a hypothesis before.\n        Which two keywords would you suggest I test first?\n        You should choose keywords that are relevant to the findings.\n        Make sure you follow the exact same case as the keywords.\n        Eg. if a keyword is \"Alzheimer disease\", you should return \"Alzheimer disease\" not \"alzheimer disease\".\n        Or if a keyword is \"neurodegenerative diseases\", you should return \"neurodegenerative diseases\" not \"Neurodegenerative diseases\".\n        Follow the output format exactly.\n        DO NOT include any other text in your response.\n        INPUT:\n        Here are the keywords: ${keywords.join(\", \")}\n        Here are the findings: ${findings.join(\", \")}\n        OUTPUT:\n        [KEYWORD1, KEYWORD2]`,\n      },\n    ],\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 5000,\n    },\n  });\n\n  if (response.content[1].type === \"text\") {\n    return response.content[1].text;\n  }\n  throw new Error(\"No text content found\");\n}\nexport async function chooseTwoRelevantFindings(\n  findings: string[]\n): Promise<string> {\n  const response = await anthropic.messages.create({\n    model: \"claude-3-7-sonnet-latest\",\n    max_tokens: 5001,\n    messages: [\n      {\n        role: \"user\",\n        content: `If I were to generate a novel hypothesis and want to test it. \n        I have a list of findings.\n        To ensure my hypothesis is novel, I want to choose two findings that have not been used in a hypothesis before.\n        Which two findings would you suggest I test first?\n        Make sure you follow the exact same case as the findings.\n        Eg. if a finding is \"Alzheimer disease\", you should return \"Alzheimer disease\" not \"alzheimer disease\".\n        Or if a finding is \"neurodegenerative diseases\", you should return \"neurodegenerative diseases\" not \"Neurodegenerative diseases\".\n        Follow the output format exactly.\n        DO NOT include any other text in your response.\n        INPUT:\n        Here are the findings: ${findings.join(\", \")}\n        OUTPUT:\n        [FINDING1;;;FINDING2]`,\n      },\n    ],\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 5000,\n    },\n  });\n\n  if (response.content[1].type === \"text\") {\n    return response.content[1].text;\n  }\n  throw new Error(\"No text content found\");\n}\n"
    },
    {
      "path": "src\\services\\anthropic\\client.ts",
      "detailed_description": "该文件 src/services/anthropic/client.ts 负责初始化与 Anthropic 和 OpenAI 的 API 客户端连接。首先，该文件导入 dotenv/config 模块以支持环境变量的加载，从而使得在运行时能够安全地获取 API 密钥。接着，使用 @anthropic-ai/sdk 库创建一个 Anthropic 客户端实例，通过将环境变量中存储的 ANTHROPIC_API_KEY 传递给构造函数来进行身份验证。类似地，使用 openai 库创建一个 OpenAI 客户端实例，并同样通过环境变量获取 OPENAI_API_KEY。这样，其他模块可以通过引入此文件来使用这两个 API 的功能，进而实现与 Anthropic 和 OpenAI 的交互，如生成文本、评估假设等。这种结构化的客户端实例化方式提高了代码的可维护性和可重用性，确保了 API 密钥的安全管理，并为后续的功能扩展提供了基础。",
      "summary": "该文件用于初始化与 Anthropic 和 OpenAI API 的客户端连接，支持安全的 API 调用。",
      "raw": "import \"dotenv/config\";\nimport Anthropic from \"@anthropic-ai/sdk\";\nimport OpenAI from \"openai\";\n\nexport const anthropic = new Anthropic({\n    apiKey: process.env.ANTHROPIC_API_KEY,\n});\n\nexport const openai = new OpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n});\n"
    },
    {
      "path": "src\\services\\anthropic\\discordSplitter.ts",
      "detailed_description": "该文件实现了一个用于处理和切分 Markdown 文档以适应 Discord 消息限制的功能。具体而言，它定义了一个最大消息长度，并通过智能拆分机制，将 Markdown 内容分成多个块，以保证每个消息块不超过 1800 个字符。文件中包含多个辅助函数，例如：判断行是否为 Markdown 头部、列表项，并计算头部级别。核心的 `splitMarkdownForDiscord` 函数遍历每一行，维护当前消息块的长度和内容，并在必要时根据 Markdown 头部和列表项的特性进行拆分。最后，生成的消息块通过 `formatForDiscord` 函数格式化为适合 Discord 输出的字符串。文件还支持将消息块写入输出文件。此外，错误处理机制确保在处理过程中能够捕获并记录任何异常，提供了较为健壮的处理能力。整体而言，该文件的主要用途是确保 Markdown 文档能够正确地被分割和发送至 Discord，以满足其消息长度限制。",
      "summary": "该文件用于智能切分 Markdown 文档，以便将其内容适配 Discord 的消息限制。",
      "raw": "import fs from \"fs/promises\";\n\n// Discord has a 2000 character limit per message, but we'll use a slightly lower value\n// to account for formatting and to provide some safety margin\nconst MAX_MESSAGE_LENGTH = 1800;\n\ninterface MessageChunk {\n    content: string;\n    number: number;\n}\n\n/**\n * Determines if a line is a Markdown header\n */\nfunction isHeader(line: string): boolean {\n    return /^#{1,6}\\s+/.test(line);\n}\n\n/**\n * Determines if a line is a list item (numbered or bullet)\n */\nfunction isListItem(line: string): boolean {\n    return /^(\\d+\\.|\\*|-)\\s+/.test(line);\n}\n\n/**\n * Counts the header level (number of # characters)\n */\nfunction getHeaderLevel(line: string): number {\n    const match = line.match(/^(#{1,6})\\s+/);\n    return match ? match[1].length : 0;\n}\n\n/**\n * Intelligently splits a Markdown document into chunks for Discord\n */\nasync function splitMarkdownForDiscord(\n    content: string\n): Promise<MessageChunk[]> {\n    // Split the content into lines\n    const lines = content.split(\"\\n\");\n\n    // Initialize chunks array\n    const chunks: MessageChunk[] = [];\n    let currentChunk: string[] = [];\n    let messageNumber = 1;\n    let currentChunkLength = 0;\n\n    // Process each line\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n        const lineLength = line.length + 1; // +1 for the newline\n\n        // Start a new chunk for major headers if the current chunk isn't empty\n        if (\n            isHeader(line) &&\n            getHeaderLevel(line) <= 2 &&\n            currentChunkLength > 0\n        ) {\n            chunks.push({\n                content: currentChunk.join(\"\\n\"),\n                number: messageNumber++,\n            });\n            currentChunk = [line];\n            currentChunkLength = lineLength;\n            continue;\n        }\n\n        // If adding this line would exceed the limit\n        if (\n            currentChunkLength + lineLength > MAX_MESSAGE_LENGTH &&\n            currentChunkLength > 0\n        ) {\n            // Special case: always break at headers regardless of level\n            if (isHeader(line)) {\n                chunks.push({\n                    content: currentChunk.join(\"\\n\"),\n                    number: messageNumber++,\n                });\n                currentChunk = [line];\n                currentChunkLength = lineLength;\n                continue;\n            }\n\n            // Special case: try to keep list items together when possible\n            if (isListItem(line)) {\n                let listEndIndex = i;\n                // Look ahead to find where this list section ends\n                while (\n                    listEndIndex < lines.length &&\n                    (isListItem(lines[listEndIndex]) ||\n                        lines[listEndIndex].trim() === \"\")\n                ) {\n                    listEndIndex++;\n                }\n\n                // If the whole list section can fit in the next chunk, start a new chunk\n                const listText = lines.slice(i, listEndIndex).join(\"\\n\");\n                if (listText.length < MAX_MESSAGE_LENGTH / 2) {\n                    // Using half to be conservative\n                    chunks.push({\n                        content: currentChunk.join(\"\\n\"),\n                        number: messageNumber++,\n                    });\n                    currentChunk = [];\n                    currentChunkLength = 0;\n                    continue; // Don't increment i, we'll process this line again\n                }\n            }\n\n            // Default: start a new chunk\n            chunks.push({\n                content: currentChunk.join(\"\\n\"),\n                number: messageNumber++,\n            });\n            currentChunk = [line];\n            currentChunkLength = lineLength;\n        } else {\n            // Add line to current chunk\n            currentChunk.push(line);\n            currentChunkLength += lineLength;\n        }\n    }\n\n    // Add the final chunk if there's content left\n    if (currentChunk.length > 0) {\n        chunks.push({\n            content: currentChunk.join(\"\\n\"),\n            number: messageNumber,\n        });\n    }\n\n    return chunks;\n}\n\n/**\n * Format message chunks for Discord output\n */\nfunction formatForDiscord(chunks: MessageChunk[]): string[] {\n    return chunks.map(\n        (chunk) => `<message num=${chunk.number}>\\n${chunk.content}\\n</message>`\n    );\n}\n\n/**\n * Main function to process a markdown file\n */\nasync function processMarkdownFile(\n    filePath: string,\n    outputFilePath?: string\n): Promise<void> {\n    try {\n        const chunks = await splitMarkdownForDiscord(filePath);\n        const formattedChunks = formatForDiscord(chunks);\n\n        // Log to console\n        formattedChunks.forEach((chunk) => console.log(chunk + \"\\n\"));\n\n        // Write to output file if specified\n        if (outputFilePath) {\n            await fs.writeFile(\n                outputFilePath,\n                formattedChunks.join(\"\\n\\n\"),\n                \"utf8\"\n            );\n            console.log(`Output written to ${outputFilePath}`);\n        }\n    } catch (error) {\n        console.error(\"Error processing markdown file:\", error);\n        if (error instanceof Error) {\n            console.error(error.stack);\n        }\n    }\n}\n\n// Export for use as a module\nexport { processMarkdownFile, splitMarkdownForDiscord };\n"
    },
    {
      "path": "src\\services\\anthropic\\errors.ts",
      "detailed_description": "文件 src/services/anthropic/errors.ts 定义了两个自定义错误类，SparqlError 和 FileError。这些类扩展了 JavaScript 的内置 Error 类，允许开发者在处理特定类型的错误时，提供更清晰的错误信息和上下文。SparqlError 类用于处理与 SPARQL 查询相关的错误，构造函数接受一个错误消息和可选的原因参数，以便在捕获和处理 SPARQL 相关的异常时，能够提供详细的错误信息。FileError 类则用于捕获与文件操作相关的错误，类似地，通过构造函数传递消息和原因，使得在文件处理过程中出现问题时，能够提供更具可读性和诊断性的错误信息。这种自定义错误处理方式有助于提高代码的可维护性和可调试性，允许开发者在处理不同类型的错误时，采取适当的响应措施。",
      "summary": "该文件定义了用于处理 SPARQL 查询和文件操作错误的自定义错误类。",
      "raw": "export class SparqlError extends Error {\n  constructor(\n    message: string,\n    public cause?: unknown,\n  ) {\n    super(message);\n    this.name = 'SparqlError';\n  }\n}\n\nexport class FileError extends Error {\n  constructor(\n    message: string,\n    public cause?: unknown,\n  ) {\n    super(message);\n    this.name = 'FileError';\n  }\n}\n"
    },
    {
      "path": "src\\services\\anthropic\\evaluateHypothesis.ts",
      "detailed_description": "该文件定义了 evaluateHypothesis 函数和 sendEvaluationToDiscord 函数，用于评估科学假设并将结果发送到 Discord 渠道。evaluateHypothesis 函数首先通过 OpenAI 的 GPT 模型执行两步评估：第一步进行互联网研究，从而获取与假设相关的信息；第二步使用该信息对假设进行评估，并生成相应的评价文本。在这两步中，函数分别调用了不同的模型，生成的研究结果和评价文本随后用于提取一个数值评分。最后，该函数返回一个包含研究结果、评估文本和评分的对象。sendEvaluationToDiscord 函数则负责将评估结果格式化并发送到指定的 Discord 频道，确保研究和评估结果可以实时与团队共享。此文件在科学研究中起到了连接大型语言模型和团队沟通平台的桥梁作用，有助于提高假设评估的效率和透明度。",
      "summary": "该文件实现了科学假设的评估和结果发送至 Discord 渠道的功能。",
      "raw": "import { openai } from \"./client\";\nimport {\n  stepOnePrompt,\n  stepTwoPrompt,\n} from \"../../evaluators/evaluationPrompt\";\nimport { EvaluationResult } from \"../../evaluators/types\";\nimport { IAgentRuntime } from \"@elizaos/core\";\nimport { anthropic } from \"./client\";\n\nexport async function evaluateHypothesis(\n  hypothesis: string\n): Promise<EvaluationResult> {\n  // Step 1: Internet Research\n  const stepOneCompletion = await openai.chat.completions.create({\n    model: \"gpt-4o-search-preview\",\n    messages: [\n      {\n        role: \"system\",\n        content: stepOnePrompt,\n      },\n      {\n        role: \"user\",\n        content: `Here is the Hypothesis to research: ${hypothesis}`,\n      },\n    ],\n  });\n\n  const research = stepOneCompletion.choices[0].message.content;\n\n  // Step 2: Hypothesis Evaluation\n  const stepTwoCompletion = await openai.chat.completions.create({\n    model: \"o1\",\n    messages: [\n      {\n        role: \"system\",\n        content: stepTwoPrompt,\n      },\n      {\n        role: \"user\",\n        content: `Here is the Hypothesis to evaluate: ${hypothesis}\\n\\nHere are the research findings from Step 1: ${research}`,\n      },\n    ],\n  });\n\n  const evaluation = stepTwoCompletion.choices[0].message.content;\n\n  const scoreCompletion = await anthropic.messages.create({\n    model: \"claude-3-5-haiku-latest\",\n    messages: [\n      {\n        role: \"user\",\n        content: `You are a scientific hypothesis score extractor. Extract the numerical score (0-100) from the evaluation below. The score should already be present in the text. Output ONLY the integer score, no other text.\\n\\nEvaluation: ${evaluation}`,\n      },\n    ],\n    max_tokens: 100,\n  });\n\n  const score =\n    scoreCompletion.content[0].type === \"text\"\n      ? scoreCompletion.content[0].text\n      : \"0\";\n\n  return {\n    stepOne: {\n      research: research || \"\",\n      timestamp: new Date().toISOString(),\n    },\n    stepTwo: {\n      evaluation: evaluation || \"\",\n      timestamp: new Date().toISOString(),\n    },\n    score,\n  };\n}\n\n/**\n * Sends the evaluation to the Discord channel\n * @param agentRuntime The agent runtime\n * @param hypothesis The hypothesis\n * @param hypothesisMessageId The message id of the hypothesis\n */\nexport async function sendEvaluationToDiscord(\n  agentRuntime: IAgentRuntime,\n  hypothesis: string,\n  hypothesisMessageId: string\n) {\n  const channel = await agentRuntime\n    .getService(\"discord\")\n    // @ts-ignore\n    .client.channels.fetch(process.env.DISCORD_CHANNEL_ID);\n  const evaluationResult = await evaluateHypothesis(hypothesis);\n  const research = evaluationResult.stepOne.research.split(\"\\n\\n\");\n  const evaluation = evaluationResult.stepTwo.evaluation.split(\"\\n\\n\");\n  await channel.send({\n    content: \"# Research\",\n    reply: { messageReference: hypothesisMessageId },\n  });\n  research.forEach(async (paragraph) => {\n    await channel.send(paragraph);\n  });\n  await channel.send({\n    content: \"# Evaluation\",\n    reply: { messageReference: hypothesisMessageId },\n  });\n  evaluation.forEach(async (paragraph) => {\n    await channel.send(paragraph);\n  });\n}\n"
    },
    {
      "path": "src\\services\\anthropic\\generateHypothesis.ts",
      "detailed_description": "该文件 `generateHypothesis.ts` 主要实现了生成科学假设的功能，通过与 SPARQL 数据库交互和调用 Anthropic API 来生成与生物医学研究相关的新假设。文件中首先定义了一系列辅助函数，用于加载 SPARQL 查询、获取关键词、摘要和研究发现等。生成假设的过程包括：从数据库中获取随机选择的研究发现，提取与这些发现相关的关键词，以及获取相关的论文摘要。接着，函数根据这些信息创建一个包含假设结构和背景的提示（prompt），该提示被发送到 Anthropic API 以生成假设文本。生成的假设通过 Discord 通道发送，且该文件还处理了可能出现的错误，如 SPARQL 查询错误或文件读取错误。整体逻辑设计确保从多种数据源中提取相关信息，以生成结构化且具有科学依据的假设，从而推动生物医学领域的研究。",
      "summary": "该文件用于通过与 SPARQL 数据库交互和调用 Anthropic API 生成生物医学研究的新假设。",
      "raw": "import fs from \"fs/promises\";\nimport { splitMarkdownForDiscord } from \"./discordSplitter\";\nimport {\n  chooseTwoRelevantFindings,\n  chooseTwoRelevantKeywords,\n} from \"./chooseTwoRelevant\";\nimport { Binding, Abstract, Finding, FindingResult, Hypothesis } from \"./types\";\nimport { sparqlRequest } from \"./sparql/makeRequest\";\nimport { FileError, SparqlError } from \"./errors\";\nimport { anthropic } from \"./client\";\nimport { logger, IAgentRuntime } from \"@elizaos/core\";\nimport {\n  getKeywordsQuery,\n  getAbstractsQuery,\n  getFindingsQuery,\n  getAbstractsForPapersQuery,\n  getPreviousHypothesesForKeywordsQuery,\n} from \"./sparql/queries\";\n/**\n * Loads a SPARQL query from a file\n * @param path Path to the query file\n * @returns The query as a string\n */\nexport async function loadQuery(path: string): Promise<string> {\n  try {\n    return await fs.readFile(path, \"utf8\");\n  } catch (error) {\n    throw new FileError(`Failed to load query from ${path}`, error);\n  }\n}\n\n/**\n * Fetches keywords from the graph database\n */\nasync function fetchKeywords(): Promise<string[]> {\n  const data = await sparqlRequest(getKeywordsQuery);\n  return data.results.bindings.map((binding: Binding) => binding.obj.value);\n}\n\n/**\n * Fetches previous hypotheses for an array of keywords\n */\nasync function fetchPreviousHypotheses(keywords: string[]): Promise<string[]> {\n  const data = await sparqlRequest(\n    getPreviousHypothesesForKeywordsQuery(keywords)\n  );\n  return data.results.bindings.map(\n    (binding: Hypothesis) => binding.hypothesis.value\n  );\n}\n\n/**\n * Fetches abstracts for a given keyword\n */\nasync function fetchAbstracts(\n  keyword: string\n): Promise<{ abstract: string; paper: string }[]> {\n  const data = await sparqlRequest(\n    getAbstractsQuery.replace(\"{{keyword}}\", `\"${keyword}\"`)\n  );\n  return data.results.bindings.map((binding: Abstract) => ({\n    abstract: binding.abstract.value,\n    paper: binding.sub.value,\n  }));\n}\n\n/**\n * Fetches findings from the graph database\n */\nasync function fetchFindings(): Promise<FindingResult[]> {\n  const data = await sparqlRequest(getFindingsQuery);\n  return data.results.bindings.map((binding: Finding) => ({\n    finding: binding.description.value,\n    paper: binding.paper.value,\n  }));\n}\n\n/**\n * Fetches abstracts for list of papers\n */\nasync function fetchAbstractsForPapers(papers: string[]): Promise<string[]> {\n  const data = await sparqlRequest(getAbstractsForPapersQuery(papers));\n  return data.results.bindings.map(\n    (binding: Abstract) => binding.abstract.value\n  );\n}\n\n/**\n * Ensures the chosen keywords are in array format\n */\nfunction ensureKeywordsArray(chosen: string | string[]): string[] {\n  if (Array.isArray(chosen)) {\n    return chosen;\n  }\n\n  return chosen\n    .slice(1, -1) // Remove brackets\n    .split(\",\")\n    .map((s) => s.trim());\n}\n\n/**\n * Ensures the chosen keywords are in array format\n */\nfunction ensureFindingsArray(chosen: string | string[]): string[] {\n  if (Array.isArray(chosen)) {\n    return chosen;\n  }\n\n  return chosen\n    .slice(1, -1) // Remove brackets\n    .split(\";;;\")\n    .map((s) => s.trim());\n}\n\n/**\n * Creates a hypothesis generation prompt\n */\nfunction createHypothesisPrompt(\n  findings: string[],\n  keywords: string[],\n  abstractsFindings: string[],\n  abstractKeywords: string[],\n  previousHypotheses: string[]\n): string {\n  return `\nYou are a biomedical scientist specializing in generating novel, testable hypotheses that connect seemingly disparate research areas.\n\n## Task\nDevelop a mechanistic hypothesis that connects findings ${findings[0]} and ${\n    findings[1]\n  } through specific biological pathways, molecular mechanisms, or cellular processes that could plausibly link these fields.\n\n## Hypothesis Structure\n1. Background: Briefly summarize the current state of knowledge for each topic (2-3 sentences per topic)\n2. Knowledge Gap: Identify a specific gap in understanding that your hypothesis addresses\n3. Central Hypothesis: Clearly state your proposed mechanistic connection in one concise sentence\n4. Proposed Mechanism: Detail the step-by-step biological pathway or mechanism (3-5 steps)\n5. Testable Predictions: Provide 2-3 specific, falsifiable predictions that would validate your hypothesis\n6. Potential Experimental Approaches: Suggest 1-2 experimental methods to test your hypothesis\n\n## Evaluation Criteria\n- Novelty: Prioritize non-obvious connections that aren't explicitly stated in the abstracts\n- Biological Plausibility: Ensure the mechanism adheres to known biological principles\n- Parsimony: Favor simpler explanations that require fewer assumptions\n- Falsifiability: Ensure the hypothesis can be tested and potentially disproven\n\n## Research Abstracts for the findings:\n${abstractsFindings\n  .map((abstract, index) => `Abstract ${index + 1}:\\n${abstract}`)\n  .join(\"\\n\\n\")}\n\n## Relevant keywords to the findings:\n${keywords.join(\", \")}\n\n## Research Abstracts for the keywords relevant to the findings:\n${abstractKeywords\n  .map((abstract, index) => `Abstract ${index + 1}:\\n${abstract}`)\n  .join(\"\\n\\n\")}\n\n## Previous Hypotheses for the relevant keywords:\n${\n  previousHypotheses.length\n    ? previousHypotheses\n        .map((hypothesis, index) => `Hypothesis ${index + 1}:\\n${hypothesis}`)\n        .join(\"\\n\\n\")\n    : \"There are no previous hypotheses.\"\n}\n\n## Additional Guidelines\n- Cite specific abstracts when drawing information (e.g., \"As shown in Abstract 3...\")\n- Acknowledge conflicting evidence if present in the abstracts\n- Consider alternative mechanisms if multiple plausible pathways exist\n- Focus on mechanisms that could reasonably be investigated with current technology\n  `.trim();\n}\n\n/**\n * Generates a hypothesis connecting two randomly chosen keywords\n */\nexport async function generateHypothesis(\n  agentRuntime: IAgentRuntime\n): Promise<{ hypothesis: string; hypothesisMessageId: string }> {\n  const channel = await agentRuntime\n    .getService(\"discord\")\n    // @ts-ignore\n    .client.channels.fetch(process.env.DISCORD_CHANNEL_ID);\n  logger.info(\"Generating hypothesis...\");\n  try {\n    // Fetch and choose findings\n    const findings = await fetchFindings();\n    const chosen = await chooseTwoRelevantFindings(\n      findings.map((f) => f.finding)\n    );\n    const chosenFindings = ensureFindingsArray(chosen);\n    logger.info(\"Chosen findings:\", chosenFindings);\n\n    // TODO: more ideal to get both the paper and the finding from the LLM prompt rather than searching like this\n    const findingPapers = findings\n      .filter((f) => chosenFindings.includes(f.finding))\n      .map((f) => f.paper);\n\n    const abstracts = await fetchAbstractsForPapers(findingPapers);\n\n    logger.info(\"Abstracts:\", abstracts);\n\n    // makes sense to use both old keyword search + the finding search and combine the results\n    // Some papers are in new form with BIO ontologies, and some are in old format with regular keywords\n    // imo best way to do this in the future is to add the keywords to the BIO ontology format, since it looks like a useful way to search\n    // then first the keywords could be used to filter the papers, and then the findings could be used for actual hypotheses\n    // since im doing the task timeboxed for 1 day, im going to implement in the following way:\n\n    // for now, let's use the other papers which are in the other format to extract additional information from the abstracts\n    // let's get two relevant keywords, that are connected to the selected findings\n    const keywords = await fetchKeywords();\n\n    const keywordsString = await chooseTwoRelevantKeywords(\n      keywords,\n      chosenFindings\n    );\n    logger.info(\"Keywords:\", keywordsString);\n    const chosenKeywords = ensureKeywordsArray(keywordsString);\n    logger.info(\"Chosen keywords:\", chosenKeywords);\n\n    // get additional abstracts for the hypotheses generation (sourcing from minimum 4 papers instead of 2)\n    const abstractsKeywordOne = await fetchAbstracts(chosenKeywords[0]);\n    const abstractsKeywordTwo = await fetchAbstracts(chosenKeywords[1]);\n\n    // combine the abstracts from the findings and the keywords\n    const keywordAbstractsResult = [\n      ...abstractsKeywordOne,\n      ...abstractsKeywordTwo,\n    ];\n    const keywordAbstracts = keywordAbstractsResult.map((a) => a.abstract);\n    const keywordPapers = keywordAbstractsResult.map((a) => a.paper);\n    const usedPapers = [...new Set([...findingPapers, ...keywordPapers])];\n    const usedAbstracts = [\n      ...abstracts,\n      ...keywordAbstractsResult.map((a) => a.abstract),\n    ];\n\n    const previousHypotheses = await fetchPreviousHypotheses(chosenKeywords);\n    logger.info(\"Previous hypotheses:\", previousHypotheses);\n\n    const hypothesisGenerationPrompt = createHypothesisPrompt(\n      chosenFindings,\n      chosenKeywords,\n      abstracts,\n      keywordAbstracts,\n      previousHypotheses\n    );\n\n    logger.info(\"Generating hypothesis...\");\n    const response = await anthropic.messages.create({\n      model: \"claude-3-7-sonnet-latest\",\n      messages: [{ role: \"user\", content: hypothesisGenerationPrompt }],\n      max_tokens: 5001,\n      thinking: {\n        type: \"enabled\",\n        budget_tokens: 5000,\n      },\n    });\n\n    const hypothesis =\n      response.content[1]?.type === \"text\"\n        ? response.content[1].text\n        : \"No hypothesis generated\";\n\n    logger.info(\"Generated Hypothesis:\");\n    logger.info(hypothesis);\n    logger.info(\n      \"For verifiability, here are the papers to cross check the hypotheses:\"\n    );\n    logger.info(usedPapers);\n\n    const randomId = Math.random().toString(36).substring(2, 15);\n\n    // produce a hypothesis, citing the papers that were used to generate it, as well as the concrete findings and keywords\n    const hypothesisJSONLD = {\n      \"@context\": {\n        dcterms: \"http://purl.org/dc/terms/\",\n        cito: \"http://purl.org/spar/cito/\",\n        deo: \"http://purl.org/spar/deo/\",\n      },\n      \"@id\": `https://hypothesis.bioagent.ai/${randomId}`,\n      \"@type\": \"deo:FutureWork\",\n      \"cito:usesDataFrom\": usedPapers,\n      \"dcterms:references\": [hypothesis],\n      \"dcterms:subject\": chosenKeywords,\n      \"dcterms:source\": chosenFindings,\n      \"dcterms:abstract\": usedAbstracts,\n    };\n\n    const chunks = await splitMarkdownForDiscord(hypothesis);\n    const messageIds = [];\n    for (const chunk of chunks) {\n      const message = await channel.send(chunk.content);\n      messageIds.push(message.id);\n    }\n    return { hypothesis, hypothesisMessageId: messageIds[0] }; // return the first message id, which is the hypothesis heading\n  } catch (error) {\n    if (error instanceof SparqlError) {\n      console.error(\"SPARQL Error:\", error.message);\n    } else if (error instanceof FileError) {\n      console.error(\"File Error:\", error.message);\n    } else {\n      console.error(\"Unexpected error:\", error);\n    }\n  }\n}\n"
    },
    {
      "path": "src\\services\\anthropic\\hypGenEvalLoop.ts",
      "detailed_description": "该文件定义了一个名为 hypGenEvalLoop 的异步函数，该函数用于定期生成科学假设并将其评估结果发送至 Discord 渠道。具体而言，函数使用 setInterval 方法每 150000 毫秒（即每 2 分钟半）调用 generateHypothesis 函数，该函数生成一个新的假设，并返回假设及其对应的消息 ID。生成的假设会通过 elizaLogger 记录到日志中。随后，sendEvaluationToDiscord 函数被调用，用于将生成的假设及其消息 ID 发送到 Discord 渠道。该文件还定义了 stopHypGenEvalLoop 函数，用于清除定时器并停止假设生成的过程，确保资源的有效管理。整个过程使得该模块可以持续生成和评估科学假设，适合需要实时更新和监控的应用场景。",
      "summary": "该文件实现了定期生成科学假设并将评估结果发送至 Discord 渠道的功能。",
      "raw": "import { logger, IAgentRuntime, elizaLogger } from \"@elizaos/core\";\nimport { generateHypothesis } from \"./generateHypothesis\";\nimport { sendEvaluationToDiscord } from \"./evaluateHypothesis\";\n\nexport const hypGenEvalLoop = async (agentRuntime: IAgentRuntime) => {\n  logger.info(\"Starting hypothesis generation interval\");\n\n  const interval = setInterval(async () => {\n    const { hypothesis, hypothesisMessageId } =\n      await generateHypothesis(agentRuntime);\n\n    elizaLogger.log(hypothesis);\n    await sendEvaluationToDiscord(\n      agentRuntime,\n      hypothesis,\n      hypothesisMessageId\n    );\n  }, 150000);\n  return interval;\n};\n\nexport const stopHypGenEvalLoop = (interval: NodeJS.Timeout) => {\n  logger.info(\"Stopping hypothesis generation interval\");\n  clearInterval(interval);\n};\n"
    },
    {
      "path": "src\\services\\anthropic\\types.ts",
      "detailed_description": "文件 src/services/anthropic/types.ts 定义了一组 TypeScript 类型，用于描述与科学研究相关的数据结构。这些类型主要包括 Binding、Abstract、Finding、FindingResult 和 Hypothesis。每个类型都具有特定的字段，描述了不同的科学信息。例如，Binding 类型表示与某个对象的绑定关系，包含类型和对应值；Abstract 类型用于表示论文的摘要和其下属信息；Finding 类型描述研究发现，包括其描述和相关论文；FindingResult 则用于表示发现的结果与相关论文的关联；Hypothesis 类型表示科学假设及其相关实体。这些类型为后续的功能模块提供了结构化的数据模型，确保了数据一致性和类型安全，有助于在与外部 API（如 Anthropic 和 OpenAI）交互时有效地处理和传递信息。",
      "summary": "该文件定义了一组用于描述科学研究相关数据的 TypeScript 类型，确保数据结构的类型安全和一致性。",
      "raw": "export type Binding = {\n  obj: {\n    type: string;\n    value: string;\n  };\n};\n\nexport type Abstract = {\n  abstract: {\n    type: string;\n    value: string;\n  };\n  sub: {\n    type: string;\n    value: string;\n  };\n};\n\nexport type Finding = {\n  description: {\n    type: string;\n    value: string;\n  };\n  paper: {\n    type: string;\n    value: string;\n  };\n};\n\nexport type FindingResult = {\n  finding: string;\n  paper: string;\n};\n\nexport type Hypothesis = {\n  hypothesis: {\n    type: string;\n    value: string;\n  };\n  hypothesisEntity: {\n    type: string;\n    value: string;\n  };\n};\n"
    },
    {
      "path": "src\\services\\anthropic\\sparql\\makeRequest.ts",
      "detailed_description": "该文件定义了一个名为 sparqlRequest 的异步函数，用于向 SPARQL 端点发送查询请求。函数使用 axios 库发送 HTTP POST 请求到本地的 SPARQL 服务器（http://localhost:7878/query），并附带查询字符串和适当的请求头，以指示请求内容为 SPARQL 查询。成功时，函数将返回从 SPARQL 服务器获取的数据；如果请求失败，它会捕获错误并检查错误是否为 AxiosError 类型。如果是，则抛出一个自定义的 SparqlError，包含失败信息，这样可以在调用该函数的地方进行更好的错误处理。此文件的主要用途是提供一个统一的接口来处理与 SPARQL 数据库的交互，并封装错误处理逻辑，以提高代码的可维护性和可读性。",
      "summary": "该文件实现了一个用于向 SPARQL 端点发送查询请求的函数，并处理相关的错误情况。",
      "raw": "import axios, { AxiosError } from \"axios\";\nimport { SparqlError } from \"../errors\";\n\nexport async function sparqlRequest(query: string) {\n    try {\n        const { data } = await axios.post(\n            \"http://localhost:7878/query\",\n            query,\n            {\n                headers: {\n                    \"Content-Type\": \"application/sparql-query\",\n                    Accept: \"application/sparql-results+json\",\n                },\n            }\n        );\n        return data;\n    } catch (error) {\n        if (error instanceof AxiosError) {\n            throw new SparqlError(\n                `SPARQL request failed: ${error.message}`,\n                error\n            );\n        }\n        throw error;\n    }\n}\n"
    },
    {
      "path": "src\\services\\anthropic\\sparql\\queries.ts",
      "detailed_description": "文件 src/services/anthropic/sparql/queries.ts 定义了多个用于 SPARQL 查询的常量，这些查询主要用于从知识图谱中提取与科学论文相关的信息。具体而言，文件中包含的查询包括获取关键词、摘要、研究结果及与特定关键词相关的假设。每个查询使用 RDF 和 RDFS 命名空间，通过 SELECT 语句从知识图谱中检索数据。文件提供了灵活的查询构建能力，例如 getAbstractsForPapersQuery 函数能够根据传入的论文列表动态生成获取论文摘要的查询，而 getPreviousHypothesesForKeywordsQuery 函数则可以根据给定的关键词查找相关的假设。这些查询为后续的数据处理和分析提供了基础，使得科学研究和假设生成过程更为高效和自动化。",
      "summary": "该文件定义了一系列 SPARQL 查询，用于从知识图谱中提取与科学论文相关的关键词、摘要和假设信息。",
      "raw": "export const getKeywordsQuery = `PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nSELECT ?obj WHERE {\n  ?sub <https://schema.org/keywords> ?obj .\n}\nGROUP BY ?obj`;\n\nexport const getAbstractsQuery = `PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nSELECT ?sub ?abstract WHERE {\n  ?sub <https://schema.org/keywords> {{keyword}} .\n  ?sub <http://purl.org/dc/terms/abstract> ?abstract\n}`;\n\n// instead of using keywords, we will get findings from the GO/CHEBI/ATC ontologies and assay/output fields\nexport const getFindingsQuery = `PREFIX fabio: <http://purl.org/spar/fabio/>\nPREFIX dcterms: <http://purl.org/dc/terms/>\nPREFIX obi: <http://purl.obolibrary.org/obo/>\n\nSELECT DISTINCT ?description ?paper\nWHERE {\n  VALUES ?predicate {\n    obi:OBI_0000299\n    obi:OBI_0000070\n    obi:OBI_0200000\n  }\n  \n  ?paper a fabio:ResearchPaper;\n  ?predicate ?obj .\n  \n  {\n    ?obj dcterms:description ?description .\n  }\n  UNION\n  {\n    ?obj ?p ?innerObj .\n    ?innerObj dcterms:description ?description .\n  }\n}`;\n\n// query searches for abstract first, and defaults to LLM generated summary if abstract was not extracted\nexport const getAbstractsForPapersQuery = (papers: string[]): string => {\n  const valuesBlock = papers.map((paper) => `<${paper}>`).join(\" \");\n\n  return `\n    PREFIX fabio: <http://purl.org/spar/fabio/>\n    PREFIX dcterms: <http://purl.org/dc/terms/>\n\n    SELECT DISTINCT ?paper ?abstract\n    WHERE {\n      VALUES ?paper { ${valuesBlock} }\n      ?paper a fabio:ResearchPaper .\n\n      OPTIONAL { ?paper dcterms:abstract ?abs . }\n      OPTIONAL { ?paper dcterms:hasPart ?part . }\n\n      BIND(COALESCE(?abs, ?part) AS ?abstract)\n    }\n  `;\n};\n\nexport const getPreviousHypothesesForKeywordsQuery = (\n  keywords: string[]\n): string => {\n  const valuesBlock = keywords.map((keyword) => `\"${keyword}\"`).join(\" \");\n\n  return `\n    PREFIX dcterms: <http://purl.org/dc/terms/>\n    PREFIX deo: <http://purl.org/spar/deo/>\n\n    SELECT DISTINCT ?hypothesisEntity ?hypothesis\n    WHERE {\n      ?hypothesisEntity a deo:FutureWork ;\n                        dcterms:subject ?subject ;\n                        dcterms:references ?hypothesis .\n\n      VALUES ?subject { ${valuesBlock} }\n    }\n  `;\n};\n"
    },
    {
      "path": "src\\services\\gdrive\\buildQuery.ts",
      "detailed_description": "该文件定义了一个用于构建 Google Drive 文件查询的策略模式，提供了针对共享文件夹和共享驱动的具体实现。通过接口 `ListFilesQueryStrategy`，文件定义了几个方法，包括 `buildQuery`、`getStartPageTokenParams`、`getDriveType`、`getDriveId` 和 `getWatchFolderParams`，这些方法被具体的策略类 `SharedDriveFolderStrategy` 和 `SharedDriveStrategy` 实现。每个策略类在构造函数中接受相应的 ID 参数，并实现查询构建逻辑，确保从 Google Drive 中检索 PDF 文件的需求。`ListFilesQueryContext` 类负责根据输入的 ID 选择合适的策略，并提供一个统一的接口来构建查询，获取开始页面令牌参数、驱动类型、驱动 ID 和监视文件夹参数。该文件的主要作用是根据不同的 Google Drive 类型灵活构建用于文件检索的查询条件，确保在不同的上下文中能够有效地获取文件信息。",
      "summary": "该文件实现了基于策略模式的 Google Drive 文件查询构建器，支持处理共享文件夹和共享驱动的查询逻辑。",
      "raw": "import { logger } from \"@elizaos/core\";\n\ninterface ListFilesQueryStrategy {\n  buildQuery(): Record<string, any>;\n  getStartPageTokenParams(): Record<string, any>;\n  getDriveType(): DriveType;\n  getDriveId(): string;\n  getWatchFolderParams(): Record<string, any>;\n}\n\ntype DriveType = \"shared_folder\" | \"shared_drive\";\n\nclass SharedDriveFolderStrategy implements ListFilesQueryStrategy {\n  constructor(private sharedDriveFolderId: string) {}\n\n  buildQuery(): Record<string, any> {\n    return {\n      q: `'${this.sharedDriveFolderId}' in parents and mimeType='application/pdf' and trashed=false`,\n      fields: \"files(id, name, md5Checksum, size)\",\n      orderBy: \"name\",\n    };\n  }\n\n  getStartPageTokenParams(): Record<string, any> {\n    return {};\n  }\n\n  getDriveType(): DriveType {\n    return \"shared_folder\";\n  }\n\n  getDriveId(): string {\n    return this.sharedDriveFolderId;\n  }\n  getWatchFolderParams(): Record<string, any> {\n    return {};\n  }\n}\n\nclass SharedDriveStrategy implements ListFilesQueryStrategy {\n  constructor(private sharedDriveId: string) {}\n\n  buildQuery(): Record<string, any> {\n    return {\n      q: `'${this.sharedDriveId}' in parents and mimeType='application/pdf' and trashed=false`,\n      orderBy: \"name\",\n      fields: \"files(id, name, md5Checksum, size)\",\n      supportsAllDrives: true,\n      includeItemsFromAllDrives: true,\n      driveId: this.sharedDriveId,\n      corpora: \"drive\",\n    };\n  }\n\n  getStartPageTokenParams(): Record<string, any> {\n    return {\n      driveId: this.sharedDriveId,\n      supportsAllDrives: true,\n    };\n  }\n\n  getDriveType(): DriveType {\n    return \"shared_drive\";\n  }\n\n  getDriveId(): string {\n    return this.sharedDriveId;\n  }\n  getWatchFolderParams(): Record<string, any> {\n    return {\n      includeItemsFromAllDrives: true,\n      supportsAllDrives: true,\n      corpora: \"drive\",\n    };\n  }\n}\n\nexport class ListFilesQueryContext {\n  private strategy: ListFilesQueryStrategy;\n\n  constructor(mainFolderId?: string, sharedDriveId?: string) {\n    if (mainFolderId && sharedDriveId) {\n      logger.error(\n        \"You cannot populate both GOOGLE_DRIVE_FOLDER_ID and SHARED_DRIVE_ID.\"\n      );\n      process.exit(1);\n    } else if (sharedDriveId) {\n      this.strategy = new SharedDriveStrategy(sharedDriveId);\n    } else if (mainFolderId) {\n      this.strategy = new SharedDriveFolderStrategy(mainFolderId);\n    } else {\n      logger.error(\n        \"Either GOOGLE_DRIVE_FOLDER_ID or SHARED_DRIVE_ID must be defined.\"\n      );\n      process.exit(1);\n    }\n  }\n\n  buildQuery(): Record<string, any> {\n    return this.strategy.buildQuery();\n  }\n\n  getStartPageTokenParams(): Record<string, any> {\n    return this.strategy.getStartPageTokenParams();\n  }\n\n  getDriveType(): DriveType {\n    return this.strategy.getDriveType();\n  }\n\n  getDriveId(): string {\n    return this.strategy.getDriveId();\n  }\n\n  getWatchFolderParams(): Record<string, any> {\n    return this.strategy.getWatchFolderParams();\n  }\n}\n"
    },
    {
      "path": "src\\services\\gdrive\\client.ts",
      "detailed_description": "该文件负责初始化 Google Drive 客户端并提供与 Google Drive API 交互的功能。它首先从环境变量中加载 Google Cloud 项目的 JSON 凭据，并使用这些凭据设置 OAuth 认证。通过调用 `initDriveClient` 函数，可以创建一个配置好的 Google Drive 客户端，该客户端默认请求只读权限。文件还定义了 `FOLDERS` 常量对象，用于存储与共享驱动相关的文件夹和驱动 ID，这些信息同样通过环境变量进行配置。此外，该文件提供了两个辅助函数：`getListFilesQuery` 和 `getStartPageTokenParams`，它们利用 `ListFilesQueryContext` 类构建查询参数，用于检索共享驱动中的文件列表和获取开始页面令牌。这些功能为后续与 Google Drive 文件操作的实现提供了基础。",
      "summary": "该文件用于初始化 Google Drive 客户端并提供与 Google Drive API 交互的相关功能。",
      "raw": "// https://developers.google.com/workspace/drive/api/reference/rest/v3\n\nimport { google, drive_v3 } from \"googleapis\";\nimport { ListFilesQueryContext } from \"./buildQuery\";\nimport \"dotenv/config\";\n/**\n * Initialize and return a Google Drive client\n * @param scopes - The OAuth scopes to request\n * @returns The initialized Google Drive client\n */\nexport async function initDriveClient(\n  scopes: string[] = [\"https://www.googleapis.com/auth/drive.readonly\"]\n): Promise<drive_v3.Drive> {\n  let credentials: any;\n  try {\n    // Load credentials\n    credentials = JSON.parse(process.env.GCP_JSON_CREDENTIALS || \"\");\n    // Set up authentication\n    const auth = new google.auth.GoogleAuth({\n      credentials,\n      scopes,\n    });\n\n    // Create and return drive client\n    return google.drive({ version: \"v3\", auth });\n  } catch (error) {\n    console.error(\"Error initializing Google Drive client:\", error);\n    throw error;\n  }\n}\n\nexport const FOLDERS = {\n  SHARED_DRIVE_FOLDER: process.env.GOOGLE_DRIVE_FOLDER_ID,\n  SHARED_DRIVE_ID: process.env.SHARED_DRIVE_ID,\n};\n\nexport function getListFilesQuery() {\n  const context = new ListFilesQueryContext(\n    FOLDERS.SHARED_DRIVE_FOLDER,\n    FOLDERS.SHARED_DRIVE_ID\n  );\n  return context.buildQuery();\n}\n\nexport function getStartPageTokenParams() {\n  const context = new ListFilesQueryContext(\n    FOLDERS.SHARED_DRIVE_FOLDER,\n    FOLDERS.SHARED_DRIVE_ID\n  );\n  return context.getStartPageTokenParams();\n}\n"
    },
    {
      "path": "src\\services\\gdrive\\index.ts",
      "detailed_description": "文件 src/services/gdrive/index.ts 的主要功能是集中导出与 Google Drive 相关的模块和功能，包括 Google Drive 客户端的初始化和监控文件变化的功能。通过使用 export * 语法，该文件将 client.ts 和 watchFiles.ts 中定义的所有导出内容整合到一个模块中，从而简化了其他模块对这些功能的引用。此外，该文件还定义了 pdf2PicOptions，这些选项用于配置将 PDF 文件转换为 PNG 图片时的参数，包括密度、格式、宽度和高度等。这些选项为处理 PDF 到图片的转换提供了必要的设置，可能在文件处理或预览时使用。整体而言，该文件旨在提高代码的模块化和可维护性，使得与 Google Drive 相关的功能更加容易访问和使用。",
      "summary": "该文件集中管理 Google Drive 相关功能的导出，并定义 PDF 转图片时的配置选项。",
      "raw": "export * from \"./client\";\nexport * from \"./watchFiles\";\n\nexport const pdf2PicOptions = {\n  density: 100,\n  format: \"png\",\n  width: 595,\n  height: 842,\n};\n"
    },
    {
      "path": "src\\services\\gdrive\\storeJsonLdToKg.ts",
      "detailed_description": "该文件包含一个名为 `storeJsonLd` 的异步函数，其主要功能是接收一个 JSON-LD 对象，解析其内容并将解析后的数据存储到 Oxigraph 数据库中。代码逻辑首先创建一个 N3 Store 实例和一个 JSON-LD 解析器实例。通过事件监听，解析器会在接收到数据时将生成的四元组添加到存储中。如果解析过程中出现错误，会捕获并记录这些错误。解析完成后，函数将 Store 中的数据转换为 N-Triples 格式，并通过 Axios 库发送 POST 请求到 Oxigraph 的存储端点。函数在成功存储数据后返回 `true`，若存储失败则抛出错误并提供相关信息。该功能对于在知识图谱中存储结构化数据至关重要，特别是在处理科学数据和元信息时。",
      "summary": "该文件实现了将 JSON-LD 对象解析并存储到 Oxigraph 数据库的功能。",
      "raw": "import { Store, Quad } from \"n3\";\nimport { JsonLdParser } from \"jsonld-streaming-parser\";\nimport axios from \"axios\";\n\n/**\n * Accepts a JSON-LD object, parses it, and stores the resulting data in Oxigraph.\n * @param jsonLd A JSON-LD object to be parsed.\n * @returns A promise that resolves with `true` if successful.\n */\nexport async function storeJsonLd(jsonLd: object): Promise<boolean> {\n  const store = new Store();\n  const parser = new JsonLdParser();\n\n  return new Promise((resolve, reject) => {\n    // Attach stream listeners\n    parser.on(\"data\", (quad: Quad) => {\n      store.addQuad(quad);\n    });\n\n    parser.on(\"error\", (error: Error) => {\n      console.error(\"Parsing error:\", error);\n      reject(error);\n    });\n\n    parser.on(\"end\", async () => {\n      try {\n        console.log(`Parsed ${store.size} quads`);\n\n        // Convert store data to N-Triples\n        const ntriples = store\n          .getQuads(null, null, null, null)\n          .map(\n            (quad) =>\n              `<${quad.subject.value}> <${quad.predicate.value}> ${\n                quad.object.termType === \"Literal\"\n                  ? `\"${quad.object.value}\"`\n                  : `<${quad.object.value}>`\n              }.`\n          )\n          .join(\"\\n\");\n\n        // Send N-Triples to Oxigraph\n        const response = await axios.post(\n          \"http://localhost:7878/store\",\n          ntriples,\n          {\n            headers: {\n              \"Content-Type\": \"application/n-quads\",\n            },\n          }\n        );\n\n        if (response.status === 204) {\n          console.log(\"Successfully stored JSON-LD in Oxigraph\");\n          resolve(true);\n        } else {\n          reject(new Error(`Unexpected response status: ${response.status}`));\n        }\n      } catch (error: any) {\n        console.error(\"Error storing JSON-LD in Oxigraph:\", error);\n        reject(error);\n      }\n    });\n\n    // Begin parsing\n    try {\n      parser.write(JSON.stringify(jsonLd));\n      parser.end();\n    } catch (error: any) {\n      console.error(\"Error during parser execution:\", error);\n      reject(error);\n    }\n  });\n}\n"
    },
    {
      "path": "src\\services\\gdrive\\watchFiles.ts",
      "detailed_description": "该文件实现了一个监视 Google Drive 文件夹变化的功能，利用 Google Drive API 定期检查文件的变化并下载新文件。在函数 `watchFolderChanges` 中，首先初始化 DKG 客户端和已处理文件的哈希集，然后通过 `getFilesInfo` 函数获取当前文件的信息。利用定时器每 10 秒检查一次新的文件变化，如果检测到新文件，文件会被下载并转换为图像格式。下载后的图像会被用作生成知识资产（KA），并存储为 JSON-LD 格式到 Oxigraph 数据库。此外，文件中还包含错误处理逻辑，以确保在发生问题时能够记录相关信息。整体上，该文件负责实现与 Google Drive 的交互，以自动化文件监视、下载和知识资产的生成。",
      "summary": "该文件用于监视 Google Drive 文件夹的变化，自动下载新文件并生成知识资产。",
      "raw": "import { IAgentRuntime, logger } from \"@elizaos/core\";\nimport { initDriveClient, FOLDERS, getListFilesQuery } from \"./client.js\";\nimport { drive_v3 } from \"googleapis\";\nimport { fileURLToPath } from \"url\";\nimport { dirname } from \"path\";\nimport DKG from \"dkg.js\";\nimport { fromBuffer } from \"pdf2pic\";\nimport { pdf2PicOptions } from \"./index.js\";\nimport { OpenAIImage } from \"./extract/types.js\";\nimport { generateKa } from \"./extract\";\nimport { storeJsonLd } from \"./storeJsonLdToKg.js\";\nimport { db, fileMetadataTable } from \"src/db\";\n\ntype Schema$File = drive_v3.Schema$File;\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = dirname(__filename);\n\ntype DKGClient = typeof DKG | null;\nlet DkgClient: DKGClient = null;\n\ninterface FileInfo {\n  id: string;\n  name: string;\n  md5Checksum: string;\n  size: number;\n}\n\nasync function downloadFile(\n  drive: drive_v3.Drive,\n  file: FileInfo\n): Promise<Buffer> {\n  const res = await drive.files.get(\n    {\n      fileId: file.id,\n      alt: \"media\",\n    },\n    {\n      responseType: \"arraybuffer\",\n      params: {\n        supportsAllDrives: true,\n        acknowledgeAbuse: true,\n      },\n      headers: {\n        Range: \"bytes=0-\",\n      },\n    }\n  );\n\n  return Buffer.from(res.data as ArrayBuffer);\n}\n\nasync function getFilesInfo(): Promise<FileInfo[]> {\n  const drive = await initDriveClient();\n  const query = getListFilesQuery();\n  const response = await drive.files.list(query);\n\n  return (response.data.files || [])\n    .filter(\n      (\n        f\n      ): f is Schema$File & {\n        id: string;\n        name: string;\n        md5Checksum: string;\n        size: number;\n      } =>\n        f.id != null &&\n        f.name != null &&\n        f.md5Checksum != null &&\n        f.size != null\n    )\n    .map((f) => ({\n      id: f.id,\n      name: f.name,\n      md5Checksum: f.md5Checksum,\n      size: f.size,\n    }));\n}\n\nexport async function watchFolderChanges(runtime: IAgentRuntime) {\n  logger.info(\"Watching folder changes\");\n  DkgClient = new DKG({\n    environment: runtime.getSetting(\"DKG_ENVIRONMENT\"),\n    endpoint: runtime.getSetting(\"DKG_HOSTNAME\"),\n    port: runtime.getSetting(\"DKG_PORT\"),\n    blockchain: {\n      name: runtime.getSetting(\"DKG_BLOCKCHAIN_NAME\"),\n      publicKey: runtime.getSetting(\"DKG_PUBLIC_KEY\"),\n      privateKey: runtime.getSetting(\"DKG_PRIVATE_KEY\"),\n    },\n    maxNumberOfRetries: 300,\n    frequency: 2,\n    contentType: \"all\",\n    nodeApiVersion: \"/v1\",\n  });\n  let knownHashes = new Set<string>();\n  let processedFilesId = new Set<string>();\n  let response = await db\n    .select({ hash: fileMetadataTable.hash, id: fileMetadataTable.id })\n    .from(fileMetadataTable);\n  for (const file of response) {\n    knownHashes.add(file.hash);\n    processedFilesId.add(file.id);\n  }\n  const drive = await initDriveClient();\n  let intervalId: NodeJS.Timeout | null = null;\n  let isRunning = true;\n\n  const checkForChanges = async () => {\n    if (!isRunning) return;\n\n    try {\n      const files = await getFilesInfo();\n      logger.info(`Found ${files.length} files`);\n      const currentHashes = new Set(files.map((f) => f.md5Checksum));\n\n      // Check for new files by hash that we haven't processed yet\n      const newFiles = files.filter(\n        (f) => !knownHashes.has(f.md5Checksum) && !processedFilesId.has(f.id)\n      );\n\n      if (newFiles.length > 0) {\n        logger.info(\n          \"New files detected:\",\n          newFiles.map((f) => `${f.name} (${f.md5Checksum})`)\n        );\n\n        // Download new files\n        for (const file of newFiles) {\n          logger.info(`Downloading ${file.name}...`);\n          const pdfBuffer = await downloadFile(drive, file);\n          logger.info(`Successfully downloaded ${file.name}`);\n\n          // Mark as processed immediately after download\n          processedFilesId.add(file.id);\n\n          const converter = fromBuffer(pdfBuffer, pdf2PicOptions);\n          const storeHandler = await converter.bulk(-1, {\n            responseType: \"base64\",\n          });\n          const images: OpenAIImage[] = storeHandler\n            .filter((page) => page.base64)\n            .map((page) => ({\n              type: \"image_url\",\n              image_url: {\n                url: `data:image/png;base64,${page.base64}`,\n              },\n            }));\n\n          const ka = await generateKa(images);\n          const res = await storeJsonLd(ka);\n          if (!res) {\n            continue;\n          } else {\n            logger.info(\"Successfully stored JSON-LD to Oxigraph\");\n          }\n\n          try {\n          } catch (error) {\n            logger.error(\n              \"Error occurred while publishing message to DKG:\",\n              error.message\n            );\n\n            if (error.stack) {\n              logger.error(\"Stack trace:\", error.stack);\n            }\n            if (error.response) {\n              logger.error(\n                \"Response data:\",\n                JSON.stringify(error.response.data, null, 2)\n              );\n            }\n          }\n        }\n      }\n\n      knownHashes = currentHashes;\n    } catch (error) {\n      logger.error(\"Error checking files:\", error.stack);\n    }\n  };\n\n  // Start the interval\n  checkForChanges();\n  intervalId = setInterval(checkForChanges, 10000); // Check every 10 seconds\n\n  // Return a function to stop watching\n  return {\n    stop: () => {\n      isRunning = false;\n      if (intervalId) {\n        clearInterval(intervalId);\n        intervalId = null;\n      }\n    },\n  };\n}\n"
    },
    {
      "path": "src\\services\\gdrive\\extract\\README.md",
      "detailed_description": "该文件提供了有关如何提取科学论文的逻辑的基本实现，重点关注如何创建示例 JSON-LD 数据。文件指出了当前实现与可用的 Google Drive 文件监视逻辑（watchFiles.ts）之间的集成需求，并提到了一些过时的代码，包括 evaluateHypothesis 函数。文件特别强调了处理关键词字段的挑战，因为不同论文可能会使用不同的术语来描述相同的主题，这可能导致不一致性。为了提高关键词的标准化，文件建议使用医学主题词（MeSH）或统一医学语言系统（UMLS）进行模糊匹配，以便能够更好地处理这些术语的多样性。该文件为后续的开发提供了指导，确保提取逻辑能够有效整合到 BioAgent 插件中，从而提升科学论文的处理和分析能力。",
      "summary": "该文件描述了提取科学论文的逻辑，并提出了处理关键词标准化的挑战和解决方案。",
      "raw": "# Logic for extracting the papers\n\nThis is a very basic implementaion of how the [sampleJsonLds](../../sampleJsonLds/20250214_085206_s41586_023_06801_2.json) were created. This needs to be integrated into the [watchFiles.ts](../../src/bioagentPlugin/services/gdrive/watchFiles.ts). Also a lot of the code in that file is outdated, eg. the `evaluateHypothesis` function. Just the logic for watching the GDrive for changes is still relevant.\n\nThe challenge is the `keywords` field needs to be handled properly. Basically, when processing different papers, the LLM might generate inconsistent terminology- like using \"Alzheimer's disease\" for one paper and \"AD\" for another. To fix this, we'll need post-processing that maps these varied terms to standardized keywords. This can get tricky given the inconsistent answers LLMs produce.\n\nI'm currently exploring solutions using either MeSH (Medical Subject Headings) or UMLS (Unified Medical Language System) to handle this using fuzzy matching. Once that is figured out, this is can be safely integrated into the bioagentPlugin\n"
    },
    {
      "path": "src\\services\\gdrive\\extract\\config.ts",
      "detailed_description": "该文件定义了一个单例模式的 Config 类，用于管理与 Anthropic、OpenAI 及 Instructor 的 API 客户端连接和配置。它实现了多个静态属性，比如 _anthropicClient 和 _openaiClient，用于存储客户端实例，确保它们在整个应用程序中只被初始化一次。该类中还包括用于设置和获取模型类型（如 ANTHROPIC_MODEL 和 OPENAI_MODEL）及论文存储目录的 getter 和 setter 方法。此外，它提供了 pdf2PicOptions 用于定义 PDF 转图片的配置选项。initialize 方法负责检查和创建存储论文的目录，确保应用程序运行时所需的资源都已准备就绪。该文件的设计目的是为了集中管理与多个 AI 模型相关的配置，提高代码的可维护性和灵活性。",
      "summary": "该文件管理与 AI 模型 API 客户端的配置和实例化，确保单例模式下的资源管理。",
      "raw": "import \"dotenv/config\";\nimport Anthropic from \"@anthropic-ai/sdk\";\nimport OpenAI from \"openai\";\nimport Instructor, { InstructorClient as IC } from \"@instructor-ai/instructor\";\nimport path from \"path\";\nimport fs from \"fs\";\n\nconst __dirname = path.resolve();\n\nexport default class Config {\n  private static _instance: Config;\n  private static _anthropicClient: Anthropic;\n  private static _openaiClient: OpenAI;\n  private static _instructorOai: IC<OpenAI>;\n  private static _instructorAnthropic: IC<Anthropic>;\n  private static _anthropicModel: string =\n    process.env.ANTHROPIC_MODEL || \"claude-3-7-sonnet-latest\";\n  private static _openaiModel: string = process.env.OPENAI_MODEL || \"gpt-4o\";\n  private static _papersDirectory: string = path.join(__dirname, \"papers\");\n  private static _pdf2PicOptions: any = {\n    density: 100,\n    format: \"png\",\n    width: 595,\n    height: 842,\n  };\n\n  private constructor() {}\n\n  private static initialize() {\n    if (!this._anthropicClient) {\n      this._anthropicClient = new Anthropic({\n        apiKey: process.env.ANTHROPIC_API_KEY,\n      });\n    }\n    if (!this._openaiClient) {\n      this._openaiClient = new OpenAI({\n        apiKey: process.env.OPENAI_API_KEY,\n      });\n    }\n\n    if (!this._instructorOai) {\n      this._instructorOai = Instructor({\n        client: this._openaiClient,\n        mode: \"JSON\",\n      });\n    }\n\n    // TODO: Anthropic not yet supported\n    // if (!this._instructorAnthropic) {\n    //   this._instructorAnthropic = Instructor({\n    //     client: this._anthropicClient,\n    //     mode: \"JSON\",\n    //   });\n    // }\n\n    if (!fs.existsSync(this._papersDirectory)) {\n      fs.mkdirSync(this._papersDirectory, { recursive: true });\n    } else if (!fs.lstatSync(this._papersDirectory).isDirectory()) {\n      throw new Error(\n        `The specified papers path \"${this._papersDirectory}\" is not a directory.`\n      );\n    }\n  }\n\n  private static getInstance(): Config {\n    if (!this._instance) {\n      this._instance = new Config();\n      this.initialize();\n    }\n    return this._instance;\n  }\n\n  public static get anthropicClient(): Anthropic {\n    this.getInstance();\n    return this._anthropicClient;\n  }\n\n  public static get openaiClient(): OpenAI {\n    this.getInstance();\n    return this._openaiClient;\n  }\n\n  public static get anthropicModel(): string {\n    this.getInstance();\n    return this._anthropicModel;\n  }\n\n  public static set anthropicModel(model: string) {\n    this.getInstance();\n    this._anthropicModel = model;\n  }\n\n  public static get openaiModel(): string {\n    this.getInstance();\n    return this._openaiModel;\n  }\n\n  public static set openaiModel(model: string) {\n    this.getInstance();\n    this._openaiModel = model;\n  }\n\n  public static get papersDirectory(): string {\n    this.getInstance();\n    return this._papersDirectory;\n  }\n\n  public static set papersDirectory(directory: string) {\n    this.getInstance();\n    this._papersDirectory = directory;\n  }\n\n  public static get pdf2PicOptions() {\n    this.getInstance();\n    return this._pdf2PicOptions;\n  }\n\n  public static set pdf2PicOptions(options: any) {\n    this.getInstance();\n    this._pdf2PicOptions = options;\n  }\n\n  public static get instructorOai() {\n    this.getInstance();\n    return this._instructorOai;\n  }\n\n  public static get instructorAnthropic() {\n    this.getInstance();\n    return this._instructorAnthropic;\n  }\n}\n"
    },
    {
      "path": "src\\services\\gdrive\\extract\\index.ts",
      "detailed_description": "该文件实现了从图像中提取科学论文和本体论信息的功能，通过与 OpenAI 的 API 进行交互完成知识提取。文件首先导入必要的配置、路径管理模块以及数据模式定义。然后定义了三个异步函数：extractPaper，extractOntologies 和 generateKa。extractPaper 函数接受一组图像，并通过调用 OpenAI 的聊天接口，使用指定的提示（extractionPrompt）提取论文内容，返回一个包含论文信息的对象；extractOntologies 类似，负责从图像中提取本体论信息，使用不同的提示（ontologiesExtractionPrompt）。generateKa 函数则是整合这两个提取功能，它并行调用 extractPaper 和 extractOntologies 函数，将两个结果合并，并返回最终的知识提取结果。整个过程包含错误重试机制，确保在面对临时问题时能够成功获取所需数据。这使得该文件在科学研究中极为重要，尤其是在处理视觉信息转化为结构化知识时。",
      "summary": "该文件负责从图像中提取科学论文和本体论信息，并整合提取结果以生成知识资产。",
      "raw": "import Config from \"./config\";\nimport path from \"path\";\nimport { PaperSchema, OntologiesSchema } from \"./z\";\nimport { ontologiesExtractionPrompt, extractionPrompt } from \"./prompts\";\nimport { OpenAIImage } from \"./types\";\n\nconst __dirname = path.resolve();\n\nasync function extractPaper(images: OpenAIImage[]) {\n  console.log(\n    `[extractPaper] Starting paper extraction with ${images.length} images`\n  );\n  const client = Config.instructorOai;\n\n  const { _meta, ...paper } = await client.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"system\",\n        content: extractionPrompt,\n      },\n      {\n        role: \"user\",\n        content: [...images],\n      },\n    ],\n    response_model: { schema: PaperSchema, name: \"Paper\" },\n    max_retries: 3,\n  });\n  console.log(`[extractPaper] Paper extraction completed successfully`);\n  return paper;\n}\n\nasync function extractOntologies(images: OpenAIImage[]) {\n  console.log(\n    `[extractOntologies] Starting ontologies extraction with ${images.length} images`\n  );\n  const client = Config.instructorOai;\n\n  const { _meta, ...ontologies } = await client.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"system\",\n        content: ontologiesExtractionPrompt,\n      },\n      {\n        role: \"user\",\n        content: [...images],\n      },\n    ],\n    response_model: { schema: OntologiesSchema, name: \"Ontologies\" },\n    max_retries: 3,\n  });\n  console.log(\n    `[extractOntologies] Ontologies extraction completed successfully`\n  );\n  return ontologies;\n}\n\nexport async function generateKa(images: OpenAIImage[]) {\n  console.log(\n    `[generateKa] Starting knowledge extraction with ${images.length} images`\n  );\n  const res = await Promise.all([\n    extractPaper(images),\n    extractOntologies(images),\n  ]);\n  console.log(`[generateKa] All extractions completed, combining results`);\n  res[0][\"ontologies\"] = res[1];\n  console.log(`[generateKa] Knowledge extraction successfully completed`);\n  return res[0];\n}\n"
    },
    {
      "path": "src\\services\\gdrive\\extract\\prompts.ts",
      "detailed_description": "该文件定义了用于从科学论文生成 JSON-LD 对象的提示（prompts），以确保提取的信息符合特定的结构化格式。`extractionPrompt` 提供了一个系统提示，指导生成器如何从论文中提取关键信息，如标题、作者、摘要、出版日期等，并确保输出满足预定义的 `PaperSchema` 要求。它还强调了在生成过程中不应使用虚构的占位符，确保信息的真实性和准确性。`ontologiesExtractionPrompt` 则专注于生成与论文相关的本体术语，要求从生物医学和书目本体中提取真实的、可验证的术语，并规定了选择术语的优先级和覆盖范围。这些提示的设计旨在提高提取的质量和一致性，确保生成的 JSON-LD 对象能够准确反映科学论文的内容，并在学术和机器可读的环境中得到有效使用。",
      "summary": "该文件提供了用于生成符合 PaperSchema 的 JSON-LD 对象和相关本体术语的提示，以确保从科学论文中提取的信息准确且结构化。",
      "raw": "export const extractionPrompt = `## **System Prompt**\n\nYou are a specialized assistant that generates JSON-LD objects for scientific papers. The output must strictly validate against the **PaperSchema** described below.\n\n1. **PaperSchema Requirements**  \n   - **\\`@context\\`**: Must be the fixed prefixes object.  \n   - **\\`@id\\`**: A string identifier (preferably a DOI) for the paper.  \n   - **\\`@type\\`**: Usually \\`\"bibo:AcademicArticle\"\\` or \\`\"schema:ScholarlyArticle\"\\`.  \n   - **\\`dcterms:title\\`**: Title of the paper.  \n   - **\\`dcterms:creator\\`**: Array of creator objects (authors). Each must have an \\`@id\\`, \\`@type\\`, and \\`\"foaf:name\"\\`.  \n   - **\\`dcterms:abstract\\`**: Abstract text.  \n   - **\\`schema:datePublished\\`**: Publication date in ISO 8601 format.  \n   - **\\`schema:keywords\\`**: Array of free-text keywords.  \n   - **\\`fabio:hasPublicationVenue\\`**: Metadata about the publication venue (journal or conference).  \n   - **\\`fabio:hasPart\\`**: Array of sections, each with an ID, type, title, and content.  \n   - **\\`cito:cites\\`**: Array of citations (with an ID, type, title, and DOI).  \n\n2. **No Made-Up Placeholders**  \n   - Provide accurate information based on the paper content.\n   - If you aren't sure of exact details, provide a concise approximation based on available information.  \n\n3. **Output Format**  \n   - Return exactly one JSON object.  \n   - No additional commentary.  \n   - No markdown fences.  \n   - Must parse successfully under the PaperSchema (Zod validator).  \n\n4. **Quality & Realism**  \n   - Provide realistic but concise bibliographic fields (title, authors, abstract, etc.).  \n   - Extract main sections and their content from the paper.\n   - Include multiple \\`cito:cites\\` references with real or plausible DOIs.  \n\n**Your Role**:  \n- Generate a single valid JSON-LD object that captures the key information from the scientific paper.\n- Structure the content according to the PaperSchema requirements.\n- Ensure all extracted information accurately represents the paper's content.\n\nThat's it. Output only the JSON object, nothing more.`;\n\nexport const ontologiesExtractionPrompt = `# Comprehensive Ontology Term Generation Prompt\n\nGenerate a rich, diverse set of ontology terms for my scientific paper JSON-LD using the following structure:\n\n\\`\\`\\`json\n{\n  \"@id\": \"[PREFIX]:[ID_NUMBER]\",\n  \"schema:name\": \"[OFFICIAL_LABEL]\"\n}\n\\`\\`\\`\n\n## Required Ontologies:\n\nInclude terms from these ontologies (3-5 terms from each where relevant):\n\n### Biomedical Ontologies:\n- **Gene Ontology (GO)**: Include biological processes, molecular functions, and cellular components\n- **Human Disease Ontology (DOID)**: For disease classifications and relationships\n- **Chemical Entities of Biological Interest (ChEBI)**: For relevant chemical compounds\n- **Anatomical Therapeutic Chemical (ATC)**: For drug classifications\n- **Monarch Disease Ontology (MONDO)**: For unified disease representation\n- **Pathway Ontology (PW)**: For biological pathways and interactions\n- **Evidence & Conclusion Ontology (ECO)**: For evidence types supporting scientific claims\n- **MeSH**: For medical subject headings, especially chemical terms\n\n### Bibliographic Ontologies:\n- **BIBO**: For bibliographic resource modeling\n- **SPAR Ontologies**:\n  - CiTO: For citation typing and relationships\n  - BiRO: For bibliographic references\n  - FaBiO: For scholarly works\n\n## Term Selection Requirements:\n\n1. **ALL terms MUST be real and verifiable** with:\n   - Correct prefix format (e.g., \"GO:0008150\", \"DOID:14330\")\n   - Genuine numeric ID (never use placeholders like \"xxxx\")\n   - Accurate official label as \"schema:name\"\n\n2. **Prioritize terms that are:**\n   - Domain-specific rather than overly general\n   - Current and not deprecated\n   - Relevant to my research topic\n   - Include a mix of granularity levels\n\n3. **Coverage requirements:**\n   - Represent biological entities, processes, and functions\n   - Include relevant diseases and conditions\n   - Cover chemical compounds and drugs where applicable\n   - Include pathway and interaction terms\n   - Provide evidence classification terms\n\n4. **DO NOT use any of the example ontology terms listed below unless they are specifically relevant to my research paper topic.** Select terms that actually apply to my research:\n\n\\`\\`\\`json\n\"cito:discusses\": [\n  {\"@id\": \"GO:0006915\", \"schema:name\": \"apoptotic process\"},\n  {\"@id\": \"GO:0007154\", \"schema:name\": \"cell communication\"},\n  {\"@id\": \"DOID:14330\", \"schema:name\": \"lung cancer\"},\n  {\"@id\": \"MONDO:0005148\", \"schema:name\": \"breast cancer\"},\n  {\"@id\": \"CHEBI:15377\", \"schema:name\": \"water\"},\n  {\"@id\": \"CHEBI:27732\", \"schema:name\": \"morphine\"},\n  {\"@id\": \"ATC:N02AA01\", \"schema:name\": \"morphine\"},\n  {\"@id\": \"PW:0000013\", \"schema:name\": \"citric acid cycle pathway\"},\n  {\"@id\": \"ECO:0000006\", \"schema:name\": \"experimental evidence\"},\n  {\"@id\": \"MESH:D015179\", \"schema:name\": \"Pre-Eclampsia\"}\n]\n\\`\\`\\`\n\nProvide a minimum of 25-30 diverse, relevant ontology terms that would appear in the \"cito:discusses\" section of my scientific paper. Ensure ALL terms are genuine entries that exist in their respective ontologies and are directly related to my specific research topic.`;\n"
    },
    {
      "path": "src\\services\\gdrive\\extract\\types.ts",
      "detailed_description": "该文件定义了与图像处理相关的 TypeScript 类型，主要用于与不同 AI 模型（如 Anthropic 和 OpenAI）进行交互时的数据结构规范。它包含了三个类型的定义：AnthropicImage、OpenAIImage 和 InstructorClient。在这些定义中，AnthropicImage 类型描述了一个包含图像数据的对象，图像数据以 base64 编码的 PNG 格式存储，方便在网络应用中使用；而 OpenAIImage 类型则定义了一个图像对象，该对象通过 URL 访问，便于从外部资源加载图像。InstructorClient 类型则是用来表示与 AI 模型互动的客户端，可以是与 OpenAI 或 Anthropic 进行交互的实例。通过这些类型的定义，开发者能够确保在处理图像数据时的一致性和类型安全，从而减少运行时错误，提升代码的可维护性和可读性。",
      "summary": "该文件定义了与图像处理相关的 TypeScript 类型，以确保与不同 AI 模型交互时的数据结构一致性和类型安全。",
      "raw": "import Anthropic from \"@anthropic-ai/sdk\";\nimport OpenAI from \"openai\";\nimport { InstructorClient as IC } from \"@instructor-ai/instructor\";\n\nexport type AnthropicImage = {\n  type: \"image\";\n  source: {\n    type: \"base64\";\n    media_type: \"image/png\";\n    data: string;\n  };\n};\n\nexport type OpenAIImage = {\n  type: \"image_url\";\n  image_url: {\n    url: string;\n  };\n};\n\nexport type InstructorClient = IC<OpenAI> | IC<Anthropic>;\n"
    },
    {
      "path": "src\\services\\gdrive\\extract\\z.ts",
      "detailed_description": "该文件定义了用于描述科学论文的 JSON-LD 结构化模式，利用 Zod 库进行类型验证和模式推导。它首先设定了一个默认的上下文，包含多个生物医学本体的前缀，确保在生成 JSON-LD 时可以容易地引用这些本体。接着，定义了多个模式，分别描述了作者、出版场所、论文的部分、引用等信息的结构。每个模式都包含详细的属性描述，确保信息的完整性和一致性。文件还包括一个完整的 'PaperSchema'，该模式整合了所有子模式，提供了一个完整的科学论文的 JSON-LD 表达方式，包含作者的身份、论文的标题、摘要、出版日期、关键词、出版场所、章节及引用等信息。最后，该文件导出了 'PaperSchema' 和其推导出的类型，便于其他模块使用和引用。",
      "summary": "该文件定义了科学论文的 JSON-LD 结构化模式及其相关类型，旨在提供一套标准化的方式来描述论文的各个组成部分。",
      "raw": "import { z } from \"zod\";\nimport crypto from \"crypto\";\n\n/*\n * Initial standardized schema for the scientific paper KA\n * with expanded descriptions for each field\n */\n\n// TODO: add IPFS\n\n// fixed default context with additional biomedical ontologies\nconst defaultContext = {\n  schema: \"https://schema.org/\",\n  fabio: \"http://purl.org/spar/fabio/\",\n  cito: \"http://purl.org/spar/cito/\",\n  dcterms: \"http://purl.org/dc/terms/\",\n  foaf: \"http://xmlns.com/foaf/0.1/\",\n  bibo: \"http://purl.org/ontology/bibo/\",\n  go: \"http://purl.obolibrary.org/obo/GO_\",\n  doid: \"http://purl.org/obo/DOID_\",\n  chebi: \"http://purl.org/obo/CHEBI_\",\n  atc: \"http://purl.org/obo/ATC_\",\n  pw: \"http://purl.org/obo/PW_\",\n  eco: \"http://purl.org/obo/ECO_\",\n  mondo: \"http://purl.org/obo/MONDO_\",\n  comptox: \"https://comptox.epa.gov/\",\n  mesh: \"http://id.nlm.nih.gov/mesh/\",\n} as const;\n\nconst ContextSchema = z\n  .object({\n    schema: z.literal(\"https://schema.org/\"),\n    fabio: z.literal(\"http://purl.org/spar/fabio/\"),\n    cito: z.literal(\"http://purl.org/spar/cito/\"),\n    dcterms: z.literal(\"http://purl.org/dc/terms/\"),\n    foaf: z.literal(\"http://xmlns.com/foaf/0.1/\"),\n    bibo: z.literal(\"http://purl.org/ontology/bibo/\"),\n    go: z.literal(\"http://purl.obolibrary.org/obo/GO_\"),\n    doid: z.literal(\"http://purl.org/obo/DOID_\"),\n    chebi: z.literal(\"http://purl.org/obo/CHEBI_\"),\n    atc: z.literal(\"http://purl.org/obo/ATC_\"),\n    pw: z.literal(\"http://purl.org/obo/PW_\"),\n    eco: z.literal(\"http://purl.org/obo/ECO_\"),\n    mondo: z.literal(\"http://purl.org/obo/MONDO_\"),\n    comptox: z.literal(\"https://comptox.epa.gov/\"),\n    mesh: z.literal(\"http://id.nlm.nih.gov/mesh/\"),\n  })\n  .default(defaultContext)\n  .describe(\n    \"Context prefixes for JSON-LD, mapping short prefixes (e.g. go:) to full IRIs.\"\n  );\n\n// creator (author) schema\nconst CreatorSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Unique identifier for the creator, typically an ORCID URI. Defaults to a kebab-case of the creator's name.\"\n    )\n    .default(`https://orcid.org/${crypto.randomUUID()}`),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type of the creator (e.g. foaf:Person). Identifies the class of this entity in Linked Data.\"\n    ),\n  \"foaf:name\": z\n    .string()\n    .describe(\n      \"Full display name of the creator, e.g. 'Alice Smith' or 'Alice B. Smith'.\"\n    ),\n});\n\n// publication venue schema\nconst PublicationVenueSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Primary identifier (e.g. DOI) of the publication venue (journal, conference, repository).\"\n    ),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type of the publication venue (e.g. fabio:Journal, schema:Periodical).\"\n    ),\n  \"schema:name\": z\n    .string()\n    .describe(\n      \"Human-readable name of the publication venue, e.g. 'Nature' or 'Proceedings of XYZ Conference'.\"\n    ),\n});\n\n// section schema\nconst SectionSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Short ID or local identifier for this section, often used as a fragment or anchor.\"\n    ),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type for the section, e.g. 'fabio:Section' or similar to define its role in the paper.\"\n    ),\n  \"dcterms:title\": z\n    .string()\n    .describe(\n      \"Heading or title of this section, e.g. 'Methods', 'Results', 'Discussion'.\"\n    ),\n  \"fabio:hasContent\": z\n    .string()\n    .describe(\n      \"Full textual content of the section (may include paragraphs of text, tables, etc.).\"\n    ),\n});\n\n// citation schema\nconst CitationSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"A unique identifier (often a DOI) for the cited work being referenced.\"\n    ),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type of the cited resource, e.g. 'bibo:AcademicArticle' or 'schema:ScholarlyArticle'.\"\n    ),\n  \"dcterms:title\": z.string().describe(\"Title of the cited work or resource.\"),\n  \"bibo:doi\": z\n    .string()\n    .describe(\n      \"Explicit DOI string of the cited work, e.g. '10.1038/s41586-020-XXXXX'.\"\n    ),\n});\n\n// ontology schema\nexport const OntologySchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Compact or full IRI of the ontology term discussed in the paper (e.g. GO, DOID, CHEBI, ATC, etc.) or 'http://purl.obolibrary.org/obo/xxx'.\"\n    ),\n  \"schema:name\": z\n    .string()\n    .describe(\n      \"Human-readable label of the ontology concept discussed in the paper\"\n    ),\n});\n\nexport const OntologiesSchema = z.object({\n  ontologies: z.array(OntologySchema),\n});\n\n// research paper schema\nexport const PaperSchema = z\n  .object({\n    \"@context\": ContextSchema,\n    \"@id\": z\n      .string()\n      .describe(\"Top-level identifier for the paper, typically a DOI.\")\n      .default(`https://doi.org/10.1234/${crypto.randomInt(10000, 99999)}`),\n    \"@type\": z\n      .string()\n      .describe(\n        \"Type of the paper, typically 'bibo:AcademicArticle' or 'schema:ScholarlyArticle'.\"\n      ),\n    \"dcterms:title\": z\n      .string()\n      .describe(\"Title of the paper, e.g. 'A Study on ...'.\"),\n    \"dcterms:creator\": z\n      .array(CreatorSchema)\n      .describe(\n        \"List of creators (authors). Each entry follows CreatorSchema, containing @id, @type, foaf:name.\"\n      ),\n    \"dcterms:abstract\": z\n      .string()\n      .describe(\"Abstract text summarizing the paper's content and findings.\"),\n    \"schema:datePublished\": z\n      .string()\n      .describe(\"Publication date, usually an ISO 8601 string (YYYY-MM-DD).\"),\n    \"schema:keywords\": z\n      .array(z.string())\n      .describe(\n        \"List of keywords or key phrases describing the paper's topics.\"\n      ),\n    \"fabio:hasPublicationVenue\": PublicationVenueSchema.describe(\n      \"Metadata about where this paper was published (journal, conference, etc.).\"\n    ),\n    \"fabio:hasPart\": z\n      .array(SectionSchema)\n      .describe(\n        \"Sections that compose the paper. Each section has an @id, @type, title, and content.\"\n      ),\n    \"cito:cites\": z\n      .array(CitationSchema)\n      .describe(\n        \"References/citations this paper includes. Each entry has an identifier, type, title, and DOI.\"\n      ),\n  })\n  .describe(\n    \"Complete JSON-LD schema for a scientific paper, including authors, venue, sections, citations, and ontology references.\"\n  );\n\nexport type Paper = z.infer<typeof PaperSchema>;\n"
    },
    {
      "path": "src\\services\\kaService\\anthropicClient.ts",
      "detailed_description": "该文件定义了与 Anthropic API 交互的客户端功能。首先，它通过 dotenv 库加载环境变量，提取 ANTHROPIC_API_KEY，以便安全地进行 API 调用。文件中包含两个主要函数：getClient 和 generateResponse。getClient 函数创建并返回一个 Anthropic 客户端实例，使用提取的 API 密钥进行认证。generateResponse 函数用于生成基于给定提示的响应。它接收 Anthropic 客户端、提示文本、模型名称（默认为 'claude-3-5-sonnet-20241022'）和最大令牌数（默认为 1500）作为参数，调用客户端的 messages.create 方法与模型进行交互，并返回生成的文本响应。如果响应中没有有效的文本内容，则抛出错误。这使得开发者可以轻松地调用 Anthropic 的语言模型生成文本，适用于需要自然语言处理的场景。",
      "summary": "该文件实现了与 Anthropic API 交互的功能，包括创建客户端和生成文本响应。",
      "raw": "import \"dotenv/config\";\nimport { Anthropic } from \"@anthropic-ai/sdk\";\n\nconst apiKey: string | undefined = process.env.ANTHROPIC_API_KEY;\n\nexport function getClient(): Anthropic {\n    return new Anthropic({ apiKey });\n}\n\nexport async function generateResponse(\n    client: Anthropic,\n    prompt: string,\n    model: string = \"claude-3-5-sonnet-20241022\",\n    maxTokens: number = 1500\n): Promise<string> {\n    const response = await client.messages.create({\n        model: model,\n        max_tokens: maxTokens,\n        messages: [{ role: \"user\", content: prompt }],\n    });\n\n    if (\n        response.content &&\n        response.content.length > 0 &&\n        response.content[0].type === \"text\"\n    ) {\n        return response.content[0].text;\n    } else {\n        throw new Error(\"No response received from Claude.\");\n    }\n}\n"
    },
    {
      "path": "src\\services\\kaService\\biologyApi.ts",
      "detailed_description": "该文件实现了与生物本体相关的多个 API 交互功能，主要用于从不同的生物医学本体（如基因本体、疾病本体、化合物本体和药物本体）中搜索相关术语。文件中定义了多个异步函数，每个函数都通过 Axios 库向特定的 API 发送请求，以获取与输入术语相匹配的候选项，并使用 Anthropic API 生成新的术语。具体而言，`searchGo`、`searchDoid`、`searchChebi` 和 `searchAtc` 函数分别处理基因本体、疾病本体、化合物本体和药物本体的搜索请求，解析 API 返回的数据并进行相应的格式化，最后通过生成的响应更新术语。文件还包含一些辅助函数，如 `extractAtcId` 用于提取 ATC ID，以及一系列更新函数，如 `updateGoTerms`、`updateDoidTerms`、`updateChebiTerms` 和 `updateAtcTerms`，这些函数用于在数据集中更新相关术语的 ID，确保数据的准确性和一致性。整体而言，该文件在生物医学数据处理和本体术语标准化方面起着重要作用。",
      "summary": "该文件用于通过与生物本体 API 交互，搜索和更新生物医学术语，以确保数据的一致性和准确性。",
      "raw": "// searchOntology.ts\n\nimport { logger } from \"@elizaos/core\";\nimport axios, { AxiosResponse } from \"axios\";\nimport \"dotenv/config\";\nimport { generateResponse } from \"./anthropicClient\";\nimport {\n  get_go_api_prompt,\n  get_doid_api_prompt,\n  get_chebi_api_prompt,\n  get_atc_api_prompt,\n} from \"./llmPrompt\";\nimport Anthropic from \"@anthropic-ai/sdk\";\n\nconst bioontologyApiKey: string | undefined = process.env.BIONTOLOGY_KEY;\n\n/**\n * Extract the last part of an ATC URL as the ATC ID.\n * E.g., if the URL ends with \"/A12BC51\", returns \"A12BC51\".\n */\nexport function extractAtcId(url: string): string | null {\n  const match = url.match(/\\/([^/]+)$/);\n  return match ? match[1] : null;\n}\n\n/**\n * Search for a term in the Gene Ontology via QuickGO API.\n */\nexport async function searchGo(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://www.ebi.ac.uk/QuickGO/services/ontology/go/search\";\n  const params = { query: term, limit: 5, page: 1 };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n    if (apiResponse.status === 200) {\n      const goCandidates = apiResponse.data.results?.slice(0, 4) || [];\n      const promptGoApi = get_go_api_prompt(term, goCandidates);\n\n      newTerm = await generateResponse(client, promptGoApi, modelIdentifier);\n\n      // Replace \"GO:\" with \"GO_\"\n      if (newTerm.includes(\"GO:\")) {\n        newTerm = newTerm.replace(\"GO:\", \"GO_\");\n      }\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.info(`EBI API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Search for a term in the DOID Ontology via EBI API.\n */\nexport async function searchDoid(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://www.ebi.ac.uk/ols/api/search\";\n  const params = {\n    q: term,\n    ontology: \"doid\",\n  };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n\n    if (apiResponse.status === 200) {\n      const data = apiResponse.data;\n      const found = data.response?.numFound || 0;\n      const doidCandidates =\n        found > 0\n          ? data.response.docs.slice(0, 4).map((candidate) => ({\n              short_form: candidate.short_form,\n              description: candidate.description,\n              label: candidate.label,\n            }))\n          : [];\n\n      const promptDoidApi = get_doid_api_prompt(term, doidCandidates);\n      newTerm = await generateResponse(client, promptDoidApi, modelIdentifier);\n\n      // Replace \"DOID:\" with \"DOID_\"\n      if (newTerm.includes(\"DOID:\")) {\n        newTerm = newTerm.replace(\"DOID:\", \"DOID_\");\n      }\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.error(`EBI API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Search for a term in the ChEBI Ontology via EBI API.\n */\nexport async function searchChebi(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://www.ebi.ac.uk/ols/api/search\";\n  const params = {\n    q: term,\n    ontology: \"chebi\",\n  };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n\n    if (apiResponse.status === 200) {\n      const data = apiResponse.data;\n      const found = data.response?.numFound || 0;\n      const chebiCandidates =\n        found > 0\n          ? data.response.docs.slice(0, 4).map((candidate) => ({\n              short_form: candidate.short_form,\n              description: candidate.description,\n              label: candidate.label,\n            }))\n          : [];\n\n      const promptChebiApi = get_chebi_api_prompt(term, chebiCandidates);\n      newTerm = await generateResponse(client, promptChebiApi, modelIdentifier);\n\n      // Replace \"CHEBI:\" with \"CHEBI_\"\n      if (newTerm.includes(\"CHEBI:\")) {\n        newTerm = newTerm.replace(\"CHEBI:\", \"CHEBI_\");\n      }\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.error(`EBI API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Search for a term in the ATC Ontology via BioOntology API.\n */\nexport async function searchAtc(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://data.bioontology.org/search\";\n  const params = {\n    q: term,\n    ontologies: \"ATC\",\n    apikey: bioontologyApiKey,\n  };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n\n    if (apiResponse.status === 200) {\n      const data = apiResponse.data;\n      let atcCandidates = [];\n      if (data.collection && data.collection.length > 0) {\n        atcCandidates = data.collection.map((candidate) => ({\n          short_form: extractAtcId(candidate[\"@id\"]),\n          description: \"\",\n          label: candidate[\"prefLabel\"],\n        }));\n      }\n      const promptAtcApi = get_atc_api_prompt(term, atcCandidates);\n      newTerm = await generateResponse(client, promptAtcApi, modelIdentifier);\n\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.error(`ATC API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Update subject and object fields in GO data with best-matching GO terms.\n */\nexport async function updateGoTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const subjectResult = await searchGo(entry.subject, client);\n    entry.subject = { term: entry.subject, id: subjectResult };\n\n    const objectResult = await searchGo(entry.object, client);\n    entry.object = { term: entry.object, id: objectResult };\n  }\n\n  return data.filter(\n    (entry) => entry.subject !== \"None\" && entry.object !== \"None\"\n  );\n}\n\n/**\n * Update disease fields in DOID data with best-matching DOID terms.\n */\nexport async function updateDoidTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const diseaseResult = await searchDoid(entry.disease, client);\n    entry.disease_id = diseaseResult;\n  }\n\n  return data.filter((entry) => entry.disease_id !== \"None\");\n}\n\n/**\n * Update compound fields in ChEBI data with best-matching ChEBI terms.\n */\nexport async function updateChebiTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const compoundResult = await searchChebi(entry.compound, client);\n    entry.compound_id = compoundResult;\n  }\n\n  return data.filter((entry) => entry.compound_id !== \"None\");\n}\n\n/**\n * Update drug fields in ATC data with best-matching ATC terms.\n */\nexport async function updateAtcTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const drugResult = await searchAtc(entry.drug, client);\n    entry.drug_id = drugResult;\n  }\n\n  return data.filter((entry) => entry.drug_id !== \"None\");\n}\n"
    },
    {
      "path": "src\\services\\kaService\\downloadPaper.ts",
      "detailed_description": "该文件定义了一个名为 `downloadPaperAndExtractDOI` 的异步函数，旨在从给定的论文 URL 下载 PDF 文件并提取数字对象标识符（DOI）。该函数支持 bioRxiv 和 arXiv 的论文链接。函数首先检查 URL 来确定对应的 PDF 地址；对于 bioRxiv，直接在 URL 后附加 '.full.pdf'，而对于 arXiv，则通过替换 '/abs/' 为 '/pdf/' 并附加 '.pdf' 来构建 PDF URL。接着，函数使用 axios 获取论文的 HTML 页面，并利用 cheerio 解析 HTML 内容以提取 DOI。若初始方法未能成功提取 DOI，函数会尝试从包含 DOI 的链接中进行提取。完成 DOI 提取后，函数会下载对应的 PDF 文件并将其存储为 'biorxiv_paper.pdf' 或 'arxiv_paper.pdf'。如果在获取 DOI 或下载 PDF 的过程中发生错误，函数会记录错误信息并返回 null。整体上，该文件为学术研究提供了便捷的文献获取工具，简化了下载和管理科学论文的过程。",
      "summary": "该文件实现了从指定论文 URL 下载 PDF 文件并提取 DOI 的功能，支持 bioRxiv 和 arXiv 链接。",
      "raw": "import { logger } from \"@elizaos/core\";\nimport axios from \"axios\";\nimport * as cheerio from \"cheerio\";\nimport * as fs from \"fs\";\n\n/**\n * Downloads the PDF and extracts the DOI for a given paper URL.\n * Supports bioRxiv and arXiv URLs.\n *\n * @param paperUrl - The URL of the paper's abstract page.\n */\nexport async function downloadPaperAndExtractDOI(paperUrl: string) {\n  let pdfUrl: string;\n  let fileName: string;\n\n  if (paperUrl.includes(\"biorxiv.org\")) {\n    // For bioRxiv, the PDF URL is the paper URL appended with '.full.pdf'\n    pdfUrl = paperUrl + \".full.pdf\";\n    fileName = \"biorxiv_paper.pdf\";\n  } else if (paperUrl.includes(\"arxiv.org\")) {\n    // For arXiv, replace '/abs/' with '/pdf/' and append '.pdf'\n    pdfUrl = paperUrl.replace(\"/abs/\", \"/pdf/\") + \".pdf\";\n    fileName = \"arxiv_paper.pdf\";\n  } else {\n    logger.error(\"Unsupported URL. Only bioRxiv and arXiv URLs are supported.\");\n    return;\n  }\n\n  try {\n    const htmlResponse = await axios.get(paperUrl); // raw html\n    const htmlData: string = htmlResponse.data;\n    const $ = cheerio.load(htmlData);\n\n    let doi: string | undefined = $('meta[name=\"citation_doi\"]').attr(\n      \"content\"\n    );\n\n    // fallback for arXiv\n    if (!doi && paperUrl.includes(\"arxiv.org\")) {\n      const doiAnchor = $('a[href*=\"doi.org\"]').first();\n      if (doiAnchor.length > 0) {\n        const doiHref = doiAnchor.attr(\"href\");\n        if (doiHref) {\n          doi = doiHref.replace(/^https?:\\/\\/doi\\.org\\//, \"\");\n        }\n      }\n    }\n\n    if (doi) {\n      logger.info(\"DOI:\", doi);\n    } else {\n      logger.info(\"DOI not found using any method.\");\n    }\n\n    const pdfResponse = await axios.get(pdfUrl, {\n      responseType: \"arraybuffer\",\n    });\n    const pdfBuffer: Buffer = Buffer.from(pdfResponse.data, \"binary\");\n\n    // fs.writeFileSync(fileName, new Uint8Array(pdfBuffer));\n    logger.info(`PDF file downloaded successfully as \"${fileName}\".`);\n    return { pdfBuffer, doi };\n  } catch (error) {\n    logger.error(\"An error occurred during the process:\", error);\n    return { pdfBuffer: null, doi: null };\n  }\n}\n\n// downloadPaperAndExtractDOI(\n//   \"https://www.biorxiv.org/content/10.1101/2025.02.19.639050v1\"\n// );\n// downloadPaperAndExtractDOI(\"https://arxiv.org/abs/2412.21154\");\n"
    },
    {
      "path": "src\\services\\kaService\\exampleForPrompts.ts",
      "detailed_description": "The file `src/services/kaService/exampleForPrompts.ts` serves as a collection of example inputs and outputs for various data structures related to scientific papers and their metadata. It provides structured examples that illustrate how to format inputs for different processing tasks, such as extracting basic information from a paper, generating citations, and creating subgraphs for Gene Ontology (GO), Disease Ontology (DOID), Chemical Entities of Biological Interest (ChEBI), and Anatomical Therapeutic Chemical (ATC) classifications. Each example is defined as a constant string or an array of objects that represent the expected JSON structure for both input and output. This file is crucial for developers to understand how to interact with the system and ensures consistency in data processing, as it provides clear templates for what the data should look like at various stages of handling scientific information.",
      "summary": "The file provides structured examples of input and output formats for processing scientific paper metadata and related data.",
      "raw": "export const basic_info_example_input = `\n    [{\n        \"element_id\": \"XXXX\",\n        \"metadata\": {\n        \"filename\": \"file.pdf\",\n        \"filetype\": \"application/pdf\",\n        \"languages\": [\"eng\"],\n        \"page_number\": 1,\n        \"parent_id\": \"XXXX\"\n        },\n        \"text\": \"Title of the paper in XYZ journal https://doi.org/XX.XXXX/XX.XXXX\",\n        \"type\": \"NarrativeText\"\n    },\n    {\n        \"element_id\": \"XXXX\",\n        \"metadata\": {\n        \"filename\": \"file.pdf\",\n        \"filetype\": \"application/pdf\",\n        \"languages\": [\"eng\"],\n        \"page_number\": 1,\n        \"parent_id\": \"XXXX\"\n        },\n        \"text\": \"AuthorX, AuthorY, AuthorZ\",\n        \"type\": \"NarrativeText\"\n    }]\n`;\n\nexport const basic_info_example_output = `\n{\n\"title\": \"Title of the paper\",\n\"authors\": [\"AuthorX\", \"AuthorY\", \"AuthorZ\"],\n\"abstract\": \"\",\n\"publication_date\": \"\",\n\"publisher\": \"\",\n\"volume\": \"\",\n\"issue\": \"\",\n\"page_numbers\": \"\",\n\"doi\": \"https://doi.org/XX.XXXX/XX.XXXX\",\n\"conflict_of_interest\": \"\",\n\"obi_details\": {\n    \"has_specified_output\": [\n        {\n        \"description\": \"Generated data on the ...\"\n        }\n    ],\n    \"instrument\": [\n        {\n        \"name\": \"Instrument X\",\n        \"description\": \"Description of Instrument X\"\n        }\n    ],\n    \"data_transformation\": [\n        {\n        \"name\": \"Example Transformation\",\n        \"description\": \"Transformation used in the experiment\"\n        }\n    ],\n    \"recruitment_status\": [\n        {\n        \"description\": \"Recruitment status of the experiment\"\n        }\n    ],\n    \"assay\": [\n        {\n        \"description\": \"Description of the assay used\"\n        }\n    ]\n}\n}\n`;\n\nexport const citations_example_input = `\nDoe John, Jane Doe, Paper title example 1, https://doi.org/random-doi-identifier1\nDoe Peter, Maria Doe, Paper title example 2, https://doi.org/random-doi-identifier2\nSmith Smith, Bob Smith, Paper title example 3, https://doi.org/random-doi-identifier3\n`;\n\nexport const citations_example_output = `\n\"citations\": \"Doe John, Paper title example 1 - https://doi.org/random-doi-identifier1\nDoe Peter, Paper title example 2 - https://doi.org/random-doi-identifier2\nSmith Smith, Paper title example 3 - https://doi.org/random-doi-identifier3\"\n`;\n\nexport const subgraph_go_example_input = `\n[\n{\n    \"text\": \"Description of a biological process involving X and Y.\"\n}\n]\n`;\n\nexport const subgraph_go_example_output = `\n[\n{\n    \"subject\": \"example biological term 1\", // Make sure to use the name of the subject, not the Gene Ontology ID (GO_...)\n    \"predicate\": \"example Gene Ontology relationship\",\n    \"object\": \"example biological term 1\", // Make sure to use the name of the object, not the Gene Ontology ID (GO_...)\n    \"explanation\": \"example explanation ...\"\n}\n]\n`;\n\nexport const subgraph_doid_example_input = `\n[\n{\n    \"text\": \"Description of Disease X with symptoms Y and Z.\"\n}\n]\n`;\n\nexport const subgraph_doid_example_output = `\n[\n{\n    \"disease\": \"Disease X\",\n    \"findings\": \"Disease X is characterized by symptoms Y and Z.\"\n}\n]\n`;\n\nexport const subgraph_chebi_example_input = `\n[\n{\n    \"text\": \"Description of Chemical Compound X.\"\n}\n]\n`;\n\nexport const subgraph_chebi_example_output = `\n[\n{\n    \"compound\": \"Chemical Compound X\",\n    \"findings\": \"Chemical Compound X is known for its properties Y and Z.\"\n}\n]\n`;\n\nexport const subgraph_atc_example_input = `\n[\n{\n    \"text\": \"Description of Drug X classified under ATC code Y.\"\n}\n]\n`;\n\nexport const subgraph_atc_example_output = `\n[\n{\n    \"drug\": \"Drug X\",\n    \"findings\": \"Drug X, classified under ATC code Y, is used for treating Z.\"\n}\n]\n`;\n\nexport const gene_ontology_example_input = `\n[\n    {\n        \"subject\": {\"term\": \"GO term example subject name\", \"id\": \"GO_XXXXXX\"},\n        \"predicate\": \"some of the gene ontology relationships\",\n        \"object\": {\"term\": \"GO term example object name\", \"id\": \"GO_XXXXXX\"},\n        \"explanation\": \"example explanation ...\"\n    }\n]\n`;\n\nexport const doid_ontology_example_input = `\n[\n    {\n        \"disease\": \"DOID_XXX\",\n        \"title\": \"Disease Y\",\n        \"findings\": \"Disease Y is a condition characterized by specific symptoms and causes. Patients with Disease Y often exhibit symptoms such as A, B, and C. Recent research indicates that genetic mutations in Gene1 and Gene2 may contribute to the pathogenesis of Disease Y.\"\n    }\n]\n`;\n\nexport const chebi_ontology_example_input = `\n[\n    {\n        \"compound_id\": \"CHEBI_XXXX\",\n        \"compound\": \"Chemical X\",\n        \"findings\": \"Chemical X (Formula) is a compound that at room temperature is characterized by certain properties. It is widely studied and is known for its ability to perform Function Y.\"\n    }\n]\n`;\n\nexport const example_basic_info = `\n    {\n    \"title\": \"Preliminary Study on X\",\n    \"authors\": [\"Author A\", \"Author B\"],\n    \"abstract\": \"This study investigates...\",\n    \"publication_date\": \"\",\n    \"publisher\": \"\",\n    \"volume\": \"\",\n    \"issue\": \"\",\n    \"page_numbers\": \"\",\n    \"doi\": \"\",\n    \"conflict_of_interest\": \"The authors are also employed by the organization which funded the research.\",\n    \"citations\": \"Author A, Author B. Paper title example 1. https://doi.org/XXXXX\n    Author C, Author D. Paper title example 2. https://doi.org/YYYYY\"\n}\n`;\n\nexport const example_spar_output = `\n    {\n        \"@context\": {\n            \"fabio\": \"http://purl.org/spar/fabio/\",\n            \"dcterms\": \"http://purl.org/dc/terms/\",\n            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n            \"cito\": \"http://purl.org/spar/cito/\",\n            \"doco\": \"http://purl.org/spar/doco/\",\n            \"pro\": \"http://purl.org/spar/pro/\",\n            \"obi\": \"http://purl.obolibrary.org/obo/\",\n            \"schema\": \"http://schema.org/\"\n        },\n        \"@type\": \"fabio:ResearchPaper\",\n        \"dcterms:title\": \"Preliminary Study on X\",\n        \"dcterms:creator\": [\n            {\"@type\": \"foaf:Person\", \"foaf:name\": \"Dr. A\"},\n            {\"@type\": \"foaf:Person\", \"foaf:name\": \"Prof. B\"}\n        ],\n        \"dcterms:abstract\": \"This study investigates...\",\n        \"dcterms:date\": \"\", // Use the release date/date of issueing here in \"yyyy-mm-dd\" format. You shouldn't include the attribute in case it's not found or empty\n        \"dcterms:publisher\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"fabio:hasJournalVolume\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"fabio:hasJournalIssue\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"fabio:hasPageNumbers\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"dcterms:identifier\": \"https://doi.org/ID\", // Use the full DOI identifier here. You shouldn't include the attribute in case it's not found or empty\n        \"dcterms:rights\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"doco:hasPart\": [], // You shouldn't include the attribute in case it's not found or empty\n        \"pro:roleIn\": [],  // You shouldn't include the attribute in case it's not found or empty\n        \"obi:OBI_0000968\": [ \n            {\n                \"foaf:name\": \"\", // Instrument used\n                \"dcterms:description\": \"\"\n            }\n        ],\n        \"obi:OBI_0200000\": [  \n            {\n                \"dcterms:description\": \"\" // Data Transformation, for example which programming language was used to sort the data\n            }\n        ],\n        \"obi:OBI_0000070\": [  \n            {\n                \"dcterms:description\": \"\" // Assay found in the science paper\n            }\n        ],\n        \"obi:OBI_0000251\": [  \n            {\n                \"dcterms:description\": \"\" // Recruitment status of the paper, e.g. how many participants in the experiment\n            }\n        ],\n        \"obi:IAO_0000616\": [  \n            {\n                \"dcterms:description\": \"\" // Conflict of interest of the authors\n            }\n        ]\n    }\n`;\n\nexport const example_go_output = `\n[\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/GO_XXXXX\", // Subject ID\n            \"dcterms:name\": \"Subject name\",\n            \"obi:RO_XXXXX\": {  /* Note for model: corresponds to a specific relationship */\n                \"@id\": \"http://purl.obolibrary.org/obo/GO_YYYYY\",  // Object ID\n                \"dcterms:description\": \"Process X positively regulates Process Y in cell type Z. This involves the modulation of Factor A and Factor B, affecting outcome C as indicated by the experimental results.\"\n                \"dcterms:name\": \"Object name\",\n            }\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/GO_AAAAA\", // Subject ID\n            \"obi:BFO_XXXXX\": {  /* Note for model: corresponds to a specific relationship */\n                \"@id\": \"http://purl.obolibrary.org/obo/GO_BBBBB\", // Object ID\n                \"dcterms:description\": \"Description of the process or relationship goes here.\"\n            }\n        }\n        /* Other GO entries would be similarly structured */\n]\n`;\n\nexport const example_doid_output = `\n[\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/CHEBI_XXXXXX\",\n            \"dcterms:title\": \"Chemical X\",                \n            \"dcterms:description\": \"Chemical X (Formula) is a compound with certain properties. It is widely studied and known for Function Y.\"\n        }\n        /* Other DOID entries would be similarly structured */\n]\n`;\n\nexport const example_chebi_output = `\n[\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/DOID_XXXXX\",\n            \"dcterms:title\": \"Compound Y\",                \n            \"dcterms:description\": \"Compound Y (Formula) is a chemical with properties A and B. It is essential in biological processes and known for characteristic C.\"\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/DOID_YYYYY\",\n            \"dcterms:title\": \"Disease X\",\n            \"dcterms:description\": \"Disease X is a disorder characterized by symptoms A, B, and C. It is linked to genetic factors such as mutations in Gene1, Gene2, and Gene3.\"\n        }\n        /* Other DOID entries would be similarly structured */\n]\n`;\n\nexport const example_json_citations = [\n  {\n    \"@id\": \"https://doi.org/10.1234/another-article\",\n    \"dcterms:title\": \"Related Work on Y\",\n  },\n  {\n    \"@id\": \"https://doi.org/10.5678/related-work\",\n    \"dcterms:title\": \"Further Discussion on Z\",\n  },\n];\n\nexport const example_graph = `\n{\n    \"@context\": {\n        \"fabio\": \"http://purl.org/spar/fabio/\",\n        \"dcterms\": \"http://purl.org/dc/terms/\",\n        \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n        \"cito\": \"http://purl.org/spar/cito/\",\n        \"doco\": \"http://purl.org/spar/doco/\",\n        \"pro\": \"http://purl.org/spar/pro/\",\n        \"obi\": \"http://purl.obolibrary.org/obo/\"\n    },\n    \"@type\": \"fabio:ResearchPaper\",\n    \"dcterms:title\": \"KSR1 Knockout Mouse Model Demonstrates MAPK Pathway's Key Role in Cisplatin- and Noise-induced Hearing Loss\",\n    \"dcterms:creator\": [\n        {\n            \"@type\": \"foaf:Person\",\n            \"foaf:name\": \"Maria A. Ingersoll\"\n        },\n        {\n            \"@type\": \"foaf:Person\",\n            \"foaf:name\": \"Jie Zhang\"\n        },\n        {\n            \"@type\": \"foaf:Person\",\n            \"foaf:name\": \"Tal Teitz\"\n        }\n    ],\n    \"dcterms:abstract\": \"Hearing loss is a major disability in everyday life and therapeutic interventions to protect hearing would bene\\ufb01t a large portion of the world population. Here we found that mice devoid of the protein kinase suppressor of RAS 1 (KSR1) in their tissues (germline KO mice) exhibit resistance to both cisplatin- and noise-induced permanent hearing loss compared with their wild-type KSR1 litter- mates. KSR1 is a scaffold protein that brings in proximity the mitogen-activated protein kinase (MAPK) proteins BRAF, MEK1/2 and ERK1/2 and assists in their activation through a phosphorylation cascade induced by both cisplatin and noise insults in the cochlear cells. KSR1, BRAF, MEK1/2, and ERK1/2 are all ubiquitously expressed in the cochlea. Deleting the KSR1 protein tempered down the MAPK phosphorylation cascade in the cochlear cells following both cisplatin and noise insults and conferred hearing protection of up to 30 dB SPL in three tested frequencies in male and female mice. Treatment with dabrafenib, an FDA-approved oral BRAF inhibitor, protected male and female KSR1 wild-type mice from both cisplatin- and noise-induced hearing loss. Dabrafenib treatment did not enhance the protection of KO KSR1 mice, providing evidence dabrafenib works primarily through the MAPK pathway. Thus, either elimination of the KSR1 gene expression or drug inhibition of the MAPK cellular pathway in mice resulted in profound protection from both cisplatin- and noise-induced hearing loss. Inhibition of the MAPK pathway, a cel- lular pathway that responds to damage in the cochlear cells, can prove a valuable strategy to protect and treat hearing loss.\",\n    \"dcterms:date\": \"2024-03-21\",\n    \"dcterms:publisher\": \"Neurobiology of Disease\",\n    \"dcterms:identifier\": \"https://doi.org/10.1523/JNEUROSCI.2174-23.2024\",\n    \"dcterms:rights\": \"T.T. and J.Z. are inventors on a patent for the use of dabrafenib in hearing protection (US 2020-0093923 A1 and US Patent no 11,433,073, 18794717.1 / EP 3618807, Japan 2022-176126, China 201880029618.7) and are cofounders of Ting Therapeutics. All other authors declare that they have no competing \\ufb01nancial interests.\",\n    \"obi:OBI_0000299\": [\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/GO_0004672\",\n            \"obi:RO_0002335\": {\n                \"@id\": \"http://purl.obolibrary.org/obo/GO_0000165\",\n                \"dcterms:description\": \"Knockout of the KSR1 gene reduces activation of the MAPK signaling pathway, which is involved in cellular stress and death in response to cisplatin and noise exposure.\"\n            }\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/DOID_0050563\",\n            \"dcterms:title\": \"Noise-induced hearing loss\",\n            \"dcterms:description\": \"Genetic knockout of the KSR1 gene in mice confers resistance to noise-induced permanent hearing loss compared to wild-type littermates. KSR1 knockout mice had significantly less hearing loss, as demonstrated by auditory brainstem response, distortion product otoacoustic emission, outer hair cell counts, and synaptic puncta staining, compared to wild-type mice following noise exposure. Inhibition of the MAPK pathway, which includes BRAF, MEK, and ERK, was shown to be the mechanism by which KSR1 knockout mice were protected from noise-induced hearing loss.\"\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/CHEBI_75048\",\n            \"dcterms:title\": \"Dabrafenib\",\n            \"dcterms:description\": \"Dabrafenib is a BRAF inhibitor that protects against both cisplatin-induced and noise-induced hearing loss in mice by inhibiting the MAPK pathway. Dabrafenib is an FDA-approved drug, making it a promising compound to repurpose for the prevention of hearing loss.\"\n        },\n        {\n            \"@id\": \"http://purl.bioontology.org/ontology/ATC/L01XA01\",\n            \"dcterms:title\": \"Cisplatin\",\n            \"dcterms:description\": \"Cisplatin is a widely used chemotherapeutic agent that can cause permanent hearing loss as a side effect. Cisplatin-induced hearing loss is associated with activation of the MAPK pathway, which leads to cellular stress and damage in the inner ear. Genetic knockout of the KSR1 gene, which is involved in the MAPK pathway, conferred resistance to cisplatin-induced hearing loss in mice. Additionally, treatment with the BRAF inhibitor dabrafenib, which inhibits the MAPK pathway, also protected against cisplatin-induced hearing loss.\"\n        }\n    ],\n    \"obi:OBI_0000968\": [\n        {\n            \"foaf:name\": \"Not specified\",\n            \"dcterms:description\": \"Various instruments and equipment were used in this study, but specific details were not provided.\"\n        }\n    ],\n    \"obi:OBI_0000293\": [\n        {\n            \"dcterms:description\": \"Utilized the KSR1 knockout mouse model and wild-type littermates as the study subjects.\"\n        }\n    ],\n    \"obi:OBI_0200000\": [\n        {\n            \"dcterms:description\": \"Analyzed single-cell RNA sequencing data from postnatal day 28 C57BL/6 mice to examine the expression of MAPK genes in the cochlea.\"\n        },\n        {\n            \"dcterms:description\": \"Performed statistical analysis to compare hearing outcomes and MAPK signaling between KSR1 knockout and wild-type mice.\"\n        }\n    ],\n    \"obi:OBI_0000070\": [\n        {\n            \"dcterms:description\": \"Evaluated hearing function in mice using auditory assessments.\"\n        },\n        {\n            \"dcterms:description\": \"Measured MAPK pathway activation in the cochlea through biochemical assays.\"\n        }\n    ]\n}\n`;\n\nexport const incorrect_json_example = `\n[\n  {\n    \"@id\": \"https://doi.org/XXXXX\",\n    \"dcterms:title\": \"Title of Paper X\"\n  },\n  {\n    \"@id\": \"https://doi.org/YYYYY\",\n    \"dcterms:title\": \"Title of Paper Y\"\n  },\n  {\n    \"@id\": \"https://doi.org/ZZZZZ\",\n    \"dcterms:title\": \"Title of Paper Z\"\n  },\n  {\n    \"@id\": \"https://doi.org/AAAAA\",\n    \"dcterms:title\": \"Title of Paper A\"\n  },\n  {\n    \"@id\": \"https://doi.org/BBBBB\",\n    \"dcterms:title\": \"Title of Paper B\"\n  },\n  {\n    \"@id\": \"https://doi.org/CCCCC\",\n    \"dcterms:title\": \"Title of Paper C\"\n  },\n  {\n    \"@id\": \"https://doi.org/DDDDD\",\n    \"dcterms:title\": \"Title of Paper D\"\n  },\n  {\n    \"@id\": \"https://doi.org/EEEEE\",\n    \"dcterms:title\": \"Title of Paper E\"\n  },\n  {\n    \"@id\": \"https://doi.org/FFFFF\",\n    \"dcterms:title\": \"Title of Paper F\"\n  },\n  {\n    \"@id\": \"https://doi.org/\n`;\n\nexport const correct_json_example = `\n[\n  {\n    \"@id\": \"https://doi.org/XXXXX\",\n    \"dcterms:title\": \"Title of Paper X\"\n  },\n  {\n    \"@id\": \"https://doi.org/YYYYY\",\n    \"dcterms:title\": \"Title of Paper Y\"\n  },\n  {\n    \"@id\": \"https://doi.org/ZZZZZ\",\n    \"dcterms:title\": \"Title of Paper Z\"\n  },\n  {\n    \"@id\": \"https://doi.org/AAAAA\",\n    \"dcterms:title\": \"Title of Paper A\"\n  },\n  {\n    \"@id\": \"https://doi.org/BBBBB\",\n    \"dcterms:title\": \"Title of Paper B\"\n  },\n  {\n    \"@id\": \"https://doi.org/CCCCC\",\n    \"dcterms:title\": \"Title of Paper C\"\n  },\n  {\n    \"@id\": \"https://doi.org/DDDDD\",\n    \"dcterms:title\": \"Title of Paper D\"\n  },\n  {\n    \"@id\": \"https://doi.org/EEEEE\",\n    \"dcterms:title\": \"Title of Paper E\"\n  },\n  {\n    \"@id\": \"https://doi.org/FFFFF\",\n    \"dcterms:title\": \"Title of Paper F\"\n  }\n]\n`;\n"
    },
    {
      "path": "src\\services\\kaService\\kaService.ts",
      "detailed_description": "该文件定义了一个名为 kaService 的服务，负责从科学论文的 PDF 文件生成知识组装（Knowledge Assembly），并与去中心化知识图谱（DKG）进行交互。主要功能包括：从 PDF 文件中提取 DOI，下载论文并提取相关元数据，生成基于提取信息的语义图，以及与 DKG 进行查询以检查论文是否已存在。文件中的核心函数 jsonArrToKa 处理输入的 JSON 数组，提取基本信息和引用，生成知识图谱，并确保其格式符合结构化要求。此外，提供的辅助函数 removeColonsRecursively 用于递归清理数据中的冒号，以满足特定的格式要求。该文件还实现了从 PDF 文件生成图像，提取 DOI、分类到 DAO（去中心化自治组织），并最终返回清理后的知识组装。通过与外部 API 交互，这个服务在生物医学研究中帮助自动化知识获取和管理，提升了研究效率。",
      "summary": "该文件实现了从 PDF 文件生成知识组装并与去中心化知识图谱交互的服务。",
      "raw": "import \"dotenv/config\";\nimport { getClient } from \"./anthropicClient\";\nimport { downloadPaperAndExtractDOI } from \"./downloadPaper\";\nimport { paperExists } from \"./sparqlQueries\";\nimport { logger } from \"@elizaos/core\";\nimport { makeUnstructuredApiRequest } from \"./unstructuredPartitioning\";\n\nimport { processJsonArray, process_paper, create_graph } from \"./processPaper\";\nimport { getSummary } from \"./vectorize\";\nimport { fromPath } from \"pdf2pic\";\nimport fs from \"fs\";\nimport { categorizeIntoDAOsPrompt } from \"./llmPrompt\";\nimport DKG from \"dkg.js\";\nconst unstructuredApiKey = process.env.UNSTRUCTURED_API_KEY;\n\ntype DKGClient = typeof DKG | null;\n\n// const jsonArr = JSON.parse(fs.readFileSync('arxiv_paper.json', 'utf8'));\n\ninterface PaperArrayElement {\n  metadata: {\n    page_number: number;\n    [key: string]: unknown;\n  };\n  text: string;\n  [key: string]: unknown;\n}\n\ninterface TaskInstance {\n  xcom_push(key: string, value: string): void;\n}\n\ninterface GeneratedGraph {\n  \"@context\": Record<string, string>;\n  \"@id\"?: string;\n  \"dcterms:hasPart\"?: string;\n  \"cito:cites\"?: unknown;\n  [key: string]: unknown;\n}\n\n/**\n * Takes an array of JSON elements representing the paper's text\n * and returns a \"knowledge assembly\" (semantic graph) that includes\n * extracted metadata, citation info, subgraphs, and a summary.\n */\nexport async function jsonArrToKa(jsonArr: PaperArrayElement[], doi: string) {\n  const client = getClient();\n\n  const paperArrayDict = await processJsonArray(jsonArr, client);\n\n  const [\n    generatedBasicInfo,\n    generatedCitations,\n    generatedGoSubgraph,\n    generatedDoidSubgraph,\n    generatedChebiSubgraph,\n    generatedAtcSubgraph,\n  ] = await process_paper(client, paperArrayDict);\n\n  const generatedGraph = await create_graph(\n    client,\n    generatedBasicInfo,\n    generatedCitations,\n    {\n      go: generatedGoSubgraph,\n      doid: generatedDoidSubgraph,\n      chebi: generatedChebiSubgraph,\n      atc: generatedAtcSubgraph,\n    }\n  );\n\n  generatedGraph[\"dcterms:hasPart\"] = await getSummary(client, generatedGraph);\n\n  generatedGraph[\"@id\"] = `https://doi.org/${doi}`; // the doi that we extracted from the paper\n\n  // Update citations, if they exist\n  // if (\n  //   generatedGraph['cito:cites'] &&\n  //   Array.isArray(generatedGraph['cito:cites']) &&\n  //   generatedGraph['cito:cites'].length > 0\n  // ) {\n  //   generatedGraph['cito:cites'] = getFinalCitations(\n  //     generatedGraph['cito:cites'],\n  //   );\n  // }\n\n  // Ensure @context has schema entry\n  const context = generatedGraph[\"@context\"] as Record<string, string>;\n  if (!(\"schema\" in context)) {\n    context[\"schema\"] = \"http://schema.org/\";\n    logger.info(\"Added 'schema' to @context in KA\");\n  }\n\n  return generatedGraph;\n  // console.log(generatedGraph);\n}\n\n// jsonArrToKa(jsonArr, {\n//   xcom_push: (key: string, value: string) => {\n//     console.log(`${key}: ${value}`);\n//   },\n// });\n/**\n/**\n/**\n * Recursively remove all colons (\":\") from string values in an object or array,\n * except for certain cases:\n *   1) Skip the entire \"@context\" object (do not remove colons from any values inside it).\n *   2) Skip any string where the key is \"@type\".\n *   3) Skip any string that appears to be a URL (starting with \"http://\", \"https://\", or \"doi:\").\n * @param data - The input data which can be an object, array, or primitive.\n * @param parentKey - The key of the parent property (used to check exceptions).\n * @returns A new object, array, or primitive with colons removed from allowed string values.\n */\nfunction removeColonsRecursively<T>(data: T, parentKey?: string): T {\n  // 1) If the parent key is \"@context\", return the data as-is (skip processing entirely)\n  if (parentKey === \"@context\") {\n    return data;\n  }\n\n  // Handle arrays\n  if (Array.isArray(data)) {\n    return data.map((item) =>\n      removeColonsRecursively(item, parentKey)\n    ) as unknown as T;\n  }\n\n  // Handle objects\n  if (data !== null && typeof data === \"object\") {\n    const newObj: Record<string, unknown> = {};\n    for (const key in data) {\n      if (Object.prototype.hasOwnProperty.call(data, key)) {\n        newObj[key] = removeColonsRecursively(\n          (data as Record<string, unknown>)[key],\n          key\n        );\n      }\n    }\n    return newObj as T;\n  }\n\n  // Handle strings\n  if (typeof data === \"string\") {\n    // 2) If this is the value of \"@type\", skip removing colons.\n    if (parentKey === \"@type\") {\n      return data as unknown as T;\n    }\n\n    // 3) If it's a URL/DOI (starts with http://, https://, or doi:), skip removing colons.\n    if (/^(https?:\\/\\/|doi:)/i.test(data)) {\n      return data as unknown as T;\n    }\n\n    // Otherwise, remove all colons\n    return data.replace(/:/g, \"\") as unknown as T;\n  }\n\n  // For numbers, booleans, null, etc., just return as is\n  return data;\n}\nconst daoUals = {\n  VitaDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101956\",\n  AthenaDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101957\",\n  PsyDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101958\",\n  ValleyDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101959\",\n  HairDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101961\",\n  CryoDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101962\",\n  \"Cerebrum DAO\":\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101963\",\n  Curetopia:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101964\",\n  \"Long Covid Labs\":\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101965\",\n  \"Quantum Biology DAO\":\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101966\",\n};\n\nexport async function generateKaFromUrls(urls: [string]) {\n  for (const url of urls) {\n    const { pdfBuffer, doi } = await downloadPaperAndExtractDOI(url);\n    if (!pdfBuffer) {\n      throw new Error(\"Failed to download paper\");\n    }\n    if (!doi) {\n      throw new Error(\"Failed to extract DOI\");\n    }\n    const paperArray = await makeUnstructuredApiRequest(\n      pdfBuffer,\n      \"paper.pdf\",\n      unstructuredApiKey\n    );\n    const ka = await jsonArrToKa(paperArray, doi);\n    const cleanedKa = removeColonsRecursively(ka);\n    return cleanedKa;\n  }\n}\nexport interface Image {\n  type: \"image\";\n  source: {\n    type: \"base64\";\n    media_type: \"image/png\";\n    data: string;\n  };\n}\nasync function extractDOIFromPDF(images: Image[]) {\n  const client = getClient();\n  const response = await client.messages.create({\n    model: \"claude-3-5-haiku-20241022\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          ...images,\n          {\n            type: \"text\",\n            text: \"Extract the DOI from the paper. Only return the DOI, no other text.\",\n          },\n        ],\n      },\n    ],\n    max_tokens: 50,\n  });\n  return response.content[0].type === \"text\"\n    ? response.content[0].text\n    : undefined;\n}\n\nasync function categorizeIntoDAOs(images: Image[]) {\n  const client = getClient();\n  const response = await client.messages.create({\n    model: \"claude-3-7-sonnet-20250219\",\n    system: categorizeIntoDAOsPrompt,\n    messages: [\n      {\n        role: \"user\",\n        content: [...images],\n      },\n    ],\n    max_tokens: 50,\n  });\n  return response.content[0].type === \"text\"\n    ? response.content[0].text\n    : undefined;\n}\n\nexport async function generateKaFromPdf(pdfPath: string, dkgClient: DKGClient) {\n  const options = {\n    density: 100,\n    format: \"png\",\n    width: 595,\n    height: 842,\n  };\n  const convert = fromPath(pdfPath, options);\n  logger.info(`Converting ${pdfPath} to images`);\n\n  const storeHandler = await convert.bulk(-1, { responseType: \"base64\" });\n\n  const imageMessages = storeHandler\n    .filter((page) => page.base64)\n    .map((page) => ({\n      type: \"image\" as const,\n      source: {\n        type: \"base64\" as const,\n        media_type: \"image/png\" as const,\n        data: page.base64!,\n      },\n    }));\n  logger.info(`Extracting DOI`);\n  const doi = await extractDOIFromPDF(imageMessages);\n  if (!doi) {\n    throw new Error(\"Failed to extract DOI\");\n  }\n  const paperExistsResult = await dkgClient.graph.query(\n    paperExists(doi),\n    \"SELECT\"\n  );\n  if (paperExistsResult.data) {\n    logger.info(`Paper ${pdfPath} already exists in DKG, skipping`);\n    return;\n  } else {\n    logger.info(`Paper ${pdfPath} does not exist in DKG, creating`);\n  }\n  const pdfBuffer = fs.readFileSync(pdfPath);\n  const paperArray = await makeUnstructuredApiRequest(\n    pdfBuffer,\n    \"paper.pdf\",\n    unstructuredApiKey\n  );\n  const ka = await jsonArrToKa(paperArray, doi);\n  const cleanedKa = removeColonsRecursively(ka);\n  const relatedDAOsString = await categorizeIntoDAOs(imageMessages);\n\n  const daos = JSON.parse(relatedDAOsString);\n\n  const daoUalsMap = daos.map((dao) => {\n    const daoUal = daoUals[dao];\n    return {\n      \"@id\": daoUal,\n      \"@type\": \"schema:Organization\",\n      \"schema:name\": dao,\n    };\n  });\n  cleanedKa[\"schema:relatedTo\"] = daoUalsMap;\n\n  return cleanedKa;\n}\n"
    },
    {
      "path": "src\\services\\kaService\\llmPrompt.ts",
      "detailed_description": "该文件定义了一系列函数，用于生成与生物学相关的提示，主要用于从科学论文中提取信息并分类。每个函数接收一个输入（如词汇、候选项或论文数组），并返回一个结构化的提示字符串，这些提示字符串包含指示模型如何选择最合适的生物学术语（如基因本体GO、疾病本体DOID、化学实体ChEBI、解剖治疗化学ATC等）或提取信息（如基本信息、引用信息等）的说明。文件中的每个提示都遵循特定格式，确保输出符合预期，使得AI模型可以有效地处理输入数据并提供相应的科学信息。该文件通过提供标准化的提示格式，帮助研究人员从复杂的科学文本中提取结构化知识，支持进一步的分析和研究。整体上，该文件增强了数据处理的自动化和准确性，使得与生物医学相关的研究工作更加高效。",
      "summary": "该文件用于生成与生物学相关的提示，以提取和分类科学论文中的信息。",
      "raw": "// prompts.ts\n\nimport {\n    basic_info_example_input,\n    basic_info_example_output,\n    citations_example_input,\n    citations_example_output,\n    subgraph_go_example_input,\n    subgraph_go_example_output,\n    subgraph_doid_example_input,\n    subgraph_doid_example_output,\n    subgraph_chebi_example_input,\n    subgraph_chebi_example_output,\n    subgraph_atc_example_input,\n    subgraph_atc_example_output,\n    example_basic_info,\n    example_spar_output,\n    example_go_output,\n    example_doid_output,\n    example_chebi_output,\n    gene_ontology_example_input,\n    doid_ontology_example_input,\n    chebi_ontology_example_input,\n    example_json_citations,\n    example_graph,\n    incorrect_json_example,\n    correct_json_example,\n} from \"./exampleForPrompts\";\n\n/**\n * Returns a prompt for choosing the most appropriate Gene Ontology (GO) term\n * from a list of GO candidates.\n */\nexport function get_go_api_prompt(term: string, go_candidates): string {\n    return `\n    Given the biological context, which of the following Gene Ontology (GO) terms best matches the description for '${term}'? Please select the most appropriate GO term or indicate if none apply by replying 'None'.\n\n    GO Candidates in JSON format: ${JSON.stringify(go_candidates)}\n\n    You must output the GO candidate which is the most suitable by replying with its id (e.g. 'GO_0043178'). If there are no suitable candidates output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'. THE ID MUST BE IN FORMAT \"GO_NUMBER\" - USE \"_\" ALWAYS. DO NOT OUTPUT ANYTHING ELSE\n    `;\n}\n\n/**\n * Returns a prompt for choosing the most appropriate Disease Ontology (DOID) term\n * from a list of DOID candidates.\n */\nexport function get_doid_api_prompt(term: string, doid_candidates): string {\n    return `\n    Given the biological context, which of the following Disease Ontology (DOID) terms best matches the description for '${term}'? Please select the most appropriate DOID term or indicate if none apply by replying 'None'.\n\n    DOID Candidates in JSON format: ${JSON.stringify(doid_candidates)}\n\n    You must output the DOID candidate which is the most suitable by replying with its id (e.g. 'DOID_14330'). If there are no suitable candidates output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'. THE ID MUST BE IN FORMAT \"DOID_NUMBER\" - USE \"_\" ALWAYS. DO NOT OUTPUT ANYTHING ELSE\n    `;\n}\n\n/**\n * Returns a prompt for choosing the most appropriate ChEBI term\n * from a list of ChEBI candidates.\n */\nexport function get_chebi_api_prompt(term: string, chebi_candidates): string {\n    return `\n    Given the biological context, which of the following Chemical Entities of Biological Interest (ChEBI) terms best matches the description for '${term}'? Please select the most appropriate ChEBI term or indicate if none apply by replying 'None'.\n\n    ChEBI Candidates in JSON format: ${JSON.stringify(chebi_candidates)}\n\n    You must output the ChEBI candidate which is the most suitable by replying with its id (e.g. 'CHEBI_15377'). If there are no suitable candidates output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'. THE ID MUST BE IN FORMAT \"CHEBI_NUMBER\" - USE \"_\" ALWAYS. DO NOT OUTPUT ANYTHING ELSE.\n    `;\n}\n\n/**\n * Returns a prompt for choosing the most appropriate ATC term\n * from a list of ATC candidates.\n */\nexport function get_atc_api_prompt(term: string, atc_candidates): string {\n    return `\n    Given the biological context, which of the following Anatomical Therapeutic Chemical (ATC) terms best matches the description for '${term}'? Please select the most appropriate ATC term or indicate if none apply by replying 'None'.\n\n    ATC Candidates in JSON format: ${JSON.stringify(atc_candidates)}\n\n    You must output the ATC candidate which is the most suitable by replying with its id (e.g. 'A14AA04'). If there are no suitable candidates, output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'.\n    `;\n}\n\n/**\n * Returns a prompt for extracting basic paper info (title, authors, abstract, etc.)\n * from an array of paper JSON chunks.\n */\nexport function get_prompt_basic_info(paper_array): string {\n    return `**Prompt**:\n    You are provided with chunks of a scientific paper in the form of JSON array elements, each containing parts of the paper such as potential titles, authors, and abstracts. Your task is to analyze these chunks incrementally to update and output the information listed below. If an element contains relevant information that improves upon or adds to the current data, update the respective fields; otherwise.\n\n    **Task**\n    1. Capture the title of the paper if a more accurate title is found in the chunk.\n    2. Identify the authors list, refining or appending to the current list based on the new information found in the chunk. MAKE SURE TO USE FULL NAMES OF THE AUTHORS AND INCLUDE THEM ALL!\n    3. Identify the abstract if more detailed or relevant information is provided in the chunk.\n    4. Identify the publication date if found in any of the chunks.\n    5. Identify the publisher or journal name if it can be extracted from the given data.\n    6. Identify the volume and issue number of the journal in which the paper is published.\n    7. Identify the page numbers that indicate where the paper is located within the journal.\n    8. Identify the DOI (Digital Object Identifier) which provides a persistent link to the paper's online location.\n    9. Capture key experimental details such as:\n        OBI_0000299 'has_specified_output': Describe the types of data or results produced by the research.\n        OBI_0000968 'instrument': Specify the instruments or equipment used in the research.\n        OBI_0000293 'has_specified_input': Identify inputs such as samples or data sets utilized in the study.\n        OBI_0200000 'data transformation': Explain any computational or analytical methods applied to raw data.\n        OBI_0000251 'recruitment status': For clinical studies, provide details on the status of participant recruitment.\n        OBI_0000070 'assay': Describe the specific assays used in the study to measure or observe biological, chemical, or physical processes, essential for validating the experimental hypothesis and ensuring reproducibility.\n        IAO_0000616 'conflict of interest': If there's a conflict of interest mentioned in the paper, describe it here.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${basic_info_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${basic_info_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO INCLUDE THE TITLE, FULL AUTHOR LIST WITH THEIR FULL NAMES, ABSTRACT AND ALL OTHER INFORMATION ABOUT THE PAPER IN A JSON OBJECT, DO NOT INCLUDE ANY EXPLANATIONS OR ADDITIONAL CONTENT **\n    `;\n}\n\n/**\n * Returns a prompt for extracting citation info from the last pages of a paper.\n */\nexport function get_prompt_citations(paper_array): string {\n    return `**Prompt**:\n    Analyze the provided chunks of the final few pages of a scientific paper formatted as JSON array elements. Each element contains potential citations, likely preceded by the term 'References'.\n\n    **Task**:\n    1. Carefully examine each citation to ensure that none are omitted. Every citation found in the input must be included.\n    2. Extract and return each citation, splitting each by a new line. Each citation should include the first author's name, the title and DOI URL identifier of the cited paper. \n    3. Confirm completeness by ensuring that every potential citation found in the input is included.\n\n    **Instructions**:\n    - Begin your examination from the section likely marked by 'References', as this is where citations typically start.\n    - Ensure completeness by including every citation identified in the input. Do not skip any citations.\n    - Output should consist only of the first author of the paper, the title and DOI URL of each citation, formatted as 'First author, title - DOI'.\n    - Provide the citations as a simple list, each on a new line, adhering strictly to the format provided below. Do not include any other comments or remarks.\n\n    **Example Input Format (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**:\n    ${citations_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**:\n    ${citations_example_output}\n\n    **Actual Input to analyze**:\n    ${JSON.stringify(paper_array)}\n\n    **Final instruction**\n    Provide me with the final output of citations, making sure to include the author and title of the cited paper and DOI in 'author, cited paper title - DOI' format, separating each citation by a new line.\n    `;\n}\n\n/**\n * Returns a prompt for extracting Gene Ontology (GO) relationships from a parsed paper.\n */\nexport function get_prompt_go_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract relationships using Gene Ontology (GO) terms and identifiers based on the scientific analysis conducted within the paper. Utilize only the recognized relationships in the Gene Ontology, which include: \"is_a\", \"part_of\", \"regulates\", \"positively_regulates\", \"negatively_regulates\", \"occurs_in\", \"capable_of\", \"capable_of_part_of\", \"has_part\", \"has_input\", \"has_output\", \"derives_from\", and \"derives_into\". Each extracted relationship should be accompanied by a brief explanation that clarifies the relationship within the context of the scientific findings.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    subject: The GO term or identifier that acts or is described.\n    predicate: The relationship from the Gene Ontology, only choosing from the following: \"is_a\", \"part_of\", \"regulates\", \"positively_regulates\", \"negatively_regulates\", \"occurs_in\", \"capable_of\", \"capable_of_part_of\", \"has_part\", \"has_input\", \"has_output\", \"derives_from\", and \"derives_into\".\n    object: The GO term or identifier that is acted upon or described.\n    explanation: A brief explanation of the relationship, indicating its relevance and context.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_go_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_go_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE GENE ONTOLOGY TERMS THAT ACTUAL EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE RELATIONSHIPS THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE GENE ONTOLOGY IDENTIFIERS AND RELATIONSHIPS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    `;\n}\n\n/**\n * Returns a prompt for extracting disease relationships from a parsed paper\n * using DOID (Disease Ontology).\n */\nexport function get_prompt_doid_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract diseases and findings about them using Human Disease Ontology (DOID) terms and identifiers based on the scientific analysis conducted within the paper.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    disease: Name of the disease, or group of diseases that you extracted\n    findings: Description of the disease and findings in the paper about the disease or group of diseases.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_doid_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_doid_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE DOID ONTOLOGY TERMS AND FINDINGS ABOUT THEM THAT ACTUAL EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE FORMAT THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE DOID DISEASE NAMES AND FINDINGS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    ** MAKE SURE TO ONLY ONLY DOUBLE QUOTES INSIDE OF THE JSON ARRAY, NOT SINGLE QUOTES **\n    `;\n}\n\n/**\n * Returns a prompt for extracting chemical compound relationships from a parsed paper\n * using ChEBI (Chemical Entities of Biological Interest).\n */\nexport function get_prompt_chebi_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract chemical compounds and findings about them using Chemical Entities of Biological Interest (ChEBI) terms and identifiers based on the scientific analysis conducted within the paper.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    compound: Name of the chemical compound, or group of compounds that you extracted\n    findings: Description of the compound and findings in the paper about the compound or group of compounds.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_chebi_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_chebi_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE ChEBI ONTOLOGY TERMS AND FINDINGS ABOUT THEM THAT ACTUALLY EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE FORMAT THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE ChEBI COMPOUND NAMES AND FINDINGS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    ** MAKE SURE TO ONLY ONLY DOUBLE QUOTES INSIDE OF THE JSON ARRAY, NOT SINGLE QUOTES **\n     `;\n}\n\n/**\n * Returns a prompt for extracting medication relationships from a parsed paper\n * using the ATC (Anatomical Therapeutic Chemical) classification.\n */\nexport function get_prompt_atc_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract medications and findings about them using Anatomical Therapeutic Chemical (ATC) terms and identifiers based on the scientific analysis conducted within the paper.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    drug: Name of the medication, or group of medications that you extracted\n    findings: Description of the medication and findings in the paper about the medication or group of medications.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_atc_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_atc_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE ATC ONTOLOGY TERMS AND FINDINGS ABOUT THEM THAT ACTUALLY EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE FORMAT THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY ONLY DOUBLE QUOTES INSIDE OF THE JSON ARRAY, NOT SINGLE QUOTES **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE ATC MEDICATION NAMES AND FINDINGS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    `;\n}\n\n/**\n * Returns a prompt to transform citation info into SPAR-compliant JSON-LD.\n */\nexport function get_prompt_spar_citations(citations: string): string {\n    return `\n    **Task**\n    Transform the provided citation information about a scientific paper into a JSON array of citations following the format I provide you.\n    One citation should be represented as one object, with an \"@id\" field which represents the DOI URL and \"dcterms:title\" field which represents the title, and only the title, author name can be removed in the \"dcterms:title\" field.\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Here's an example of the JSON output object you should output. Pay specific attention that there are no authors named in the \"dcterms:title\" field:\n    ${example_json_citations}\n\n    **Actual Input**\n    ${citations}\n\n    **Final instruction**\n    Output the JSON array in the specified format and make sure to use double quotes (\") and not single quotes (') in the outputted JSON. Include only the DOI URLs and paper titles, all mentioned authors can be omitted. DO NOT INCLUDE ANY ADDITIONAL REMARKS OR COMMENTS, JUST THE JSON ARRAY.\n    `;\n}\n\n/**\n * Returns a prompt to transform basic paper info into a SPAR/OBI-based JSON-LD.\n */\nexport function get_prompt_spar_ontology(basic_info_text: string): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON-LD object using appropriate elements from the SPAR Ontologies. The input includes key metadata such as title, authors, abstract, and other publication details. Your task is to utilize the FaBiO, CiTO, DoCO, and PRO ontologies to create a rich, semantically detailed representation of the paper.\n\n    ** Input **\n    A JSON object with basic metadata about a scientific paper.\n\n    ** Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT) **\n    A JSON-LD formatted object using SPAR Ontologies and OBI ontology to structure and link the provided information in a semantic web-friendly manner. Exclude attributes with no tangible values.\n\n    **Explanation of Key OBI Elements to Include:**\n    - **OBI:0000299 'has_specified_output'**: Use this to describe the types of data or results produced by the research.\n    - **OBI_0000968 'instrument'**: Detail the instruments or equipment employed in the research.\n    - **OBI_0000293 'has_specified_input'**: Identify inputs such as samples or data sets used in the study.\n    - **OBI_0200000 'data transformation'**: Describe any computational or analytical methods applied to raw data.\n    - **OBI:0000251 'recruitment status'**: Relevant for clinical studies, detail the status of participant recruitment.\n    - **OBI:0000070 'assay'**: Represents the specific assays used in the study to measure or observe biological, chemical, or physical processes. Assays are fundamental in validating the experimental hypothesis and are essential for the reproducibility of the results.\n    ** Note for OBI Elements ** MAKE SURE TO ONLY INCLUDE THE OBI ELEMENTS IF THEY ARE FOUND INSIDE THE SCIENCE PAPER. IF THEY ARE NOT, YOU CAN OMIT THEM.\n\n    ** Example Input JSON **\n    Basic paper info example: ${example_basic_info}\n\n    ** Note ** Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT) **\n    ${example_spar_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values (e.g. )\n\n    **Explanation:**\n    - **@context:** Includes namespaces for a broader range of SPAR ontologies and the OBI ontology\n    - **@type:** Changed to \\`fabio:ResearchPaper\\` to better match academic publications.\n    - **DOI:** Use the dcterms:identifier field to include the DOI (Digital Object Identifier) that you found in the paper.\n    - **Metadata Fields:** Extended to potentially include roles and document components.\n    - **Use of PRO and DoCO:** Added placeholders for document parts (\\`doco:hasPart\\`) and roles (\\`pro:roleIn\\`).\n    - **Condition on Non-Empty Values:** Fields with empty strings, empty lists, or other unspecified values are not included in the output.\n    - **Flexibility in Attribute Selection:** While the example output provides a baseline, additional SPAR attributes should be considered and included if they provide further context or detail to the representation of the paper.\n\n    ** Actual Input JSON **\n    Basic paper info (SPAR & OBI Ontology): ${basic_info_text}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON OBJECT WHICH REPRESENTS THE JSON LD REPRESENTATION OF THE PAPERS BASIC INFO. DO NOT INCLUDE ANY OTHER REMARKS - JUST THE JSON OBJECT. DO NOT INCLUDE ANY COMMENTS IN THE JSON OUTPUT (// notation) **\n    ** MAKE SURE TO ONLY INCLUDE OBI TERMS IN THE OUTPUT WHICH ARE INCLUDED IN THE BASIC PAPER INFO PASSED *\n    `;\n}\n\n/**\n * Returns a prompt to transform a GO subgraph into a JSON array using GO relationships.\n */\nexport function get_prompt_go_ontology(generated_go_subgraph): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON array using appropriate elements from the Gene Ontology (GO). The input includes Gene Ontology (GO) terms in a simple JSON format which you should transfer into an array format for an RDF graph. Your task is to utilize the GO ontology to create a rich, semantically detailed representation of terms and relationships described.\n\n    ** Input **\n    A JSON object with Gene Ontology terms and relationships from a scientific paper.\n\n    ** Output **\n    A JSON formatted array using Gene Ontology to structure and link the provided information in a semantic web-friendly manner.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Gene Ontology input example: ${gene_ontology_example_input}\n\n    ** Note **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_go_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values (e.g. )\n\n    **Explanation of how to build GO ontology array:**\n    Map the Gene Ontology relationships to their correspondands from the OBI (obi:) ontology.\n    enabled by <=> obi:RO_0002333\n    directly positively regulates <=> obi:RO_0002629\n    negatively regulated by <=> obi:RO_0002335\n    causally upstream of, positive effect <=> obi:RO_0002304\n    causally upstream of, negative effect <=> obi:RO_0002305\n    occurs in <=> obi:BFO_0000066\n    part of <=> obi:BFO_0000050\n    capable of <=> obi:RO_0002215\n    capable of part of <=> RO_0002216\n    has input <=> obi:RO_0002233\n    has output <=> obi:RO_0002234\n    derives from <=> obi:RO_0001000\n    derives into <=> obi:RO_0001001\n\n    ** Actual Input JSON **\n    Gene Ontology terms and relationships: ${JSON.stringify(\n        generated_go_subgraph,\n        null,\n        2\n    )}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data - the actual output should be in the same format of only the JSON array.\n    ** MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD. **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY WHICH REPRESENTS THE GO TERMS - NO OTHER REMARKS OR COMMENTS SHOULD BE INCLUDED **\n    `;\n}\n\n/**\n * Returns a prompt to transform a DOID subgraph into a JSON array using DOID relationships.\n */\nexport function get_prompt_doid_ontology(generated_doid_subgraph): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON array using appropriate elements from the Disease Ontology (DOID). The input includes Disease Ontology (DOID) terms in a simple JSON format which you should transfer into an array format for an RDF graph. Your task is to utilize the DOID ontology to create a rich, semantically detailed representation of terms and relationships described.\n\n    ** Input **\n    A JSON object with DOID terms and findings about the disease from a scientific paper.\n\n    ** Output **\n    A JSON formatted array using DOID ontology to structure and link the provided information in a semantic web-friendly manner.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Disease Ontology input example: ${doid_ontology_example_input}\n\n    ** Note **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_doid_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values \n\n    **Explanation of how to build DOID ontology array:**\n    Map the provided \"disease_id\" to the \"@id\" field.\n    Map the provided \"disease\" field to the \"dcterms:title\" field.\n    Map the provided \"findings\" field to the \"dcterms:description\" field.\n\n    ** Actual Input JSON **\n    Disease ontology terms and relationships: ${JSON.stringify(\n        generated_doid_subgraph,\n        null,\n        2\n    )}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data - the actual output should be in the same format of only the JSON array.\n    ** MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD. **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY WHICH REPRESENTS THE DOID DISEASES - NO OTHER REMARKS OR COMMENTS SHOULD BE INCLUDED **\n    `;\n}\n\n/**\n * Returns a prompt to transform a ChEBI subgraph into a JSON array using ChEBI relationships.\n */\nexport function get_prompt_chebi_ontology(generated_chebi_subgraph): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON array using appropriate elements from the Chemical Entities of Biological Interest (ChEBI). The input includes ChEBI terms in a simple JSON format which you should transfer into an array format for an RDF graph. Your task is to utilize the ChEBI ontology to create a rich, semantically detailed representation of terms and relationships described.\n\n    ** Input **\n    A JSON object with ChEBI terms and findings about the chemical compound from a scientific paper.\n\n    ** Output **\n    A JSON formatted array using ChEBI ontology to structure and link the provided information in a semantic web-friendly manner.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ChEBI input example: ${chebi_ontology_example_input}\n\n    ** Note **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_chebi_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values \n\n    **Explanation of how to build ChEBI ontology array:**\n    Map the provided \"compound_id\" to the \"@id\" field.\n    Map the provided \"compound\" field to the \"dcterms:title\" field.\n    Map the provided \"findings\" field to the \"dcterms:description\" field.\n\n    ** Actual Input JSON **\n    Disease ontology terms and relationships: ${JSON.stringify(\n        generated_chebi_subgraph,\n        null,\n        2\n    )}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data - the actual output should be in the same format of only the JSON array.\n    ** MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD. **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY WHICH REPRESENTS THE DOID DISEASES - NO OTHER REMARKS OR COMMENTS SHOULD BE INCLUDED **\n    `;\n}\n\n/**\n * Returns a prompt asking for start and stop pages of specified sections\n * given an array of page text data.\n */\nexport function get_prompt_section_page_numbers(\n    paper_array,\n    sections: string[]\n): string {\n    let prompt = `Given the following pages of a research paper, identify the start and stop pages for each one of the provided sections\\n\\n`;\n\n    paper_array.forEach((element) => {\n        const pageNumber = element.metadata?.page_number;\n        const text = element.text;\n        prompt += `Page ${pageNumber}:\\n${text}\\n\\n`;\n    });\n\n    prompt += `Please provide the start and stop pages for each section in the following format:\n    \n    ** Example input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Introduction, abstract\n\n    ** Example output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**:\n    Introduction 1, 2\n    Abstract, 4, 9\n\n    ** Actual input **\n    ${sections.join(\", \")}\n\n    ** Your output **\n    ${JSON.stringify(\n        sections.map((section) => `${section}, start, end`),\n        null,\n        2\n    )}\n\n    OUTPUT ONLY THE SECTIONS AND PAGE NUMBERS IN THE EXAMPLE FORMAT, ONLY FOR THE SECTIONS FROM THE INPUT. DO NOT CONSIDER OTHER SECTIONS OR ADD ANY OTHER COMMENTS, EXPLANATIONS ETC. \n    `;\n    return prompt;\n}\n\n/**\n * Returns a prompt for generating a textual summary for similarity search\n * from an RDF JSON-LD graph.\n */\nexport function get_prompt_vectorization_summary(graph): string {\n    // Shallow clone of the graph\n    const graphCopy = JSON.parse(JSON.stringify(graph));\n    if (graphCopy[\"cito:cites\"]) {\n        delete graphCopy[\"cito:cites\"];\n    }\n\n    return `\n    ** Task: **\n    Generate a comprehensive textual summary based on the provided RDF JSON-LD graph. The summary should include as much information as possible that would be useful for similarity search.\n\n    ** Input **\n\n    An RDF JSON-LD graph that contains various nodes and relationships.\n\n    ** Output **\n\n    A detailed textual summary that captures the key information, entities, and relationships in the graph, formatted in a way that maximizes its utility for similarity search.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_graph}\n\n    ** Example Output summary **\n\n    The research paper titled \"KSR1 Knockout Mouse Model Demonstrates MAPK Pathway's Key Role in Cisplatin- and Noise-induced Hearing Loss\" was authored by Maria A. Ingersoll, Jie Zhang, and Tal Teitz and published on March 21, 2024, in the Neurobiology of Disease. This study investigates the impact of the KSR1 gene on hearing loss. The abstract reveals that knockout mice devoid of the KSR1 protein exhibit resistance to cisplatin- and noise-induced permanent hearing loss compared to their wild-type counterparts. The KSR1 protein acts as a scaffold bringing MAPK pathway proteins (BRAF, MEK1/2, ERK1/2) in proximity for activation through phosphorylation cascades triggered by cisplatin and noise in cochlear cells. The knockout of KSR1 significantly reduces MAPK activation, thereby conferring hearing protection.\n\n    Key findings include the role of MAPK pathway inhibition in providing hearing protection, with dabrafenib (a BRAF inhibitor) effectively protecting KSR1 wild-type mice from hearing loss without additional benefits in KSR1 knockout mice. These findings suggest that dabrafenib primarily works through the MAPK pathway.\n\n    Further details include the involvement of several key entities:\n    - GO_0004672: Knockout of the KSR1 gene reduces activation of the MAPK signaling pathway, which is involved in cellular stress and death in response to cisplatin and noise exposure.\n    - DOID_0050563: Genetic knockout of the KSR1 gene in mice confers resistance to noise-induced permanent hearing loss compared to wild-type littermates. KSR1 knockout mice had significantly less hearing loss, as demonstrated by auditory brainstem response, distortion product otoacoustic emission, outer hair cell counts, and synaptic puncta staining, compared to wild-type mice following noise exposure. Inhibition of the MAPK pathway, which includes BRAF, MEK, and ERK, was shown to be the mechanism by which KSR1 knockout mice were protected from noise-induced hearing loss.\n    - CHEBI_75048: Dabrafenib is a BRAF inhibitor that protects against both cisplatin-induced and noise-induced hearing loss in mice by inhibiting the MAPK pathway. Dabrafenib is an FDA-approved drug, making it a promising compound to repurpose for the prevention of hearing loss.\n    - L01XA01: Cisplatin is a widely used chemotherapeutic agent that can cause permanent hearing loss as a side effect. Cisplatin-induced hearing loss is associated with activation of the MAPK pathway, which leads to cellular stress and damage in the inner ear. Genetic knockout of the KSR1 gene, which is involved in the MAPK pathway, conferred resistance to cisplatin-induced hearing loss in mice. Additionally, treatment with the BRAF inhibitor dabrafenib, which inhibits the MAPK pathway, also protected against cisplatin-induced hearing loss.\n\n    The study utilized various instruments and equipment (details not specified), and included the KSR1 knockout mouse model and wild-type littermates as subjects. Analytical methods involved single-cell RNA sequencing data from postnatal day 28 C57BL/6 mice to examine the expression of MAPK genes in the cochlea, and statistical analysis to compare hearing outcomes and MAPK signaling between KSR1 knockout and wild-type mice. Hearing function was evaluated using auditory assessments, and MAPK pathway activation in the cochlea was measured through biochemical assays.\n\n\n    ** Notes **\n    1. DO NOT USE ANY SPECIAL CHARACTERS IN THE SUMMARY. eg. :, \", newlines, etc.\n    2. Ensure the summary captures all key entities and relationships present in the RDF JSON-LD graph.\n    3. The summary should be formatted in a way that makes it easy to use for similarity search purposes - make sure to use specific term names and not pronouns such as \"it\", \"he\", \"they\".\n    4. Your output should be only the generated summary. No other comments or remarks will be tolerated.\n    ** Actual Input JSON **\n\n    ${JSON.stringify(graphCopy, null, 4)}\n    \n    ** FINAL NOTE - MAKE SURE TO ONLY OUTPUT THE GENERATED SUMMARY, WITHOUT EXTRA COMMENTS OR REMARKS **\n    `;\n}\n\n/**\n * Returns a prompt to convert incorrectly formatted text into valid JSON.\n */\nexport function get_prompt_convert_to_json(incorrect_json: string): string {\n    return `\n    ** Task: **\n    Convert the following incorrectly formatted text into a valid JSON format. Ensure that any incomplete or cut-off elements are properly removed, and the resulting JSON structure is correct.\n\n    ** Input: **\n    The incorrectly formatted text that is supposed to be JSON but has errors or missing/extra elements.\n\n    ** Output: **\n    A valid JSON structure with all elements properly formatted and completed. If the JSON array is too long and gets cut off, delete the last incomplete element and properly close the array.\n\n    ** Example Input JSON: **\n    ${incorrect_json_example}\n\n    ** Note: **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON: **\n    ${correct_json_example}\n\n    ** Note: **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE PROVIDED DATA INSTEAD.\n\n    ** Actual Input JSON: **\n    ${incorrect_json}\n\n    ** Important Notes: **\n    1. THE MOST IMPORTANT THING IS THAT THE JSON MUST BE CORRECT. IT IS OKAY TO REMOVE SOME OF THE CONTENT IN ORDER TO MAKE THE JSON FORMATTED WELL.\n    2. REMOVE ANY INCOMPLETE ELEMENTS FROM THE JSON ARRAY AND PROPERLY CLOSE THE ARRAY.\n    3. MAKE SURE TO ONLY OUTPUT THE JSON OBJECT, DO NOT INCLUDE ANY OTHER REMARKS OR COMMENTS.\n    `;\n}\n\n/**\n * Returns a prompt to generate three suggested research questions\n * based on the given RDF JSON-LD graph.\n */\nexport function get_prompt_suggested_questions(graph): string {\n    const graphCopy = JSON.parse(JSON.stringify(graph));\n    if (graphCopy[\"cito:cites\"]) {\n        delete graphCopy[\"cito:cites\"];\n    }\n\n    return `\n    ** Task: **\n    Generate three suggested research questions based on the provided RDF JSON-LD graph. The questions should be related to the key entities, assets, or topics in the graph and should be useful for exploring the content further.\n\n    ** Input **\n\n    An RDF JSON-LD graph that contains various nodes and relationships.\n\n    ** Output **\n\n    Three research questions related to the graph's entities and relationships. The questions should be designed to assist in exploring related assets (such as papers, entities, or authors) and can be based on the scientific content of the graph.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_graph}\n\n    ** Example Suggested Questions **\n\n    1. List all the science papers related to Alzheimer disease.\n    2. What can you tell me about Cisplatin from the recent science papers?\n    3. List me the authors of paper \"KIS kinase controls alternative splicing during neuronal differentiation\".\n\n    ** Example Output (Make sure to follow this exact format, with no additional text): **\n\n    List all the science papers related to Alzheimer disease.\n    What can you tell me about Cisplatin from the recent science papers?\n    List me the authors of paper \"KIS kinase controls alternative splicing during neuronal differentiation\".\n\n    ** Notes **\n    1. Ensure the questions are specific, well-formed, and directly related to the entities or relationships present in the RDF JSON-LD graph.\n    2. The questions should be in a format that can be easily understood and processed for further querying or retrieval tasks.\n    3. Your output should be the three suggested questions, each on its own line, without extra comments or remarks. Do not include any labels like 'Question:'.\n\n    ** Actual Input JSON **\n\n    ${JSON.stringify(graphCopy, null, 4)}\n\n    ** FINAL NOTE - MAKE SURE TO OUTPUT ONLY THE THREE QUESTIONS WITHOUT EXTRA COMMENTS OR REMARKS **\n    `;\n}\n\nexport const categorizeIntoDAOsPrompt = `JUST RETURN THE ARRAY OF DAO NAMES RELEVANT TO THE PAPER AND ANSWER VERY CONCISE AND TO THE POINT\n<dao_list>\nVitaDAO → Longevity, anti-aging, age-related diseases\nAthenaDAO → Women's health, reproductive health, gynecological research\nPsyDAO → Psychedelic science, mental health, psychedelic-assisted therapy\nValleyDAO → Synthetic biology, environmental biotech, climate solutions\nHairDAO → Hair loss treatment, regenerative medicine, dermatology\nCryoDAO → Cryopreservation, biostasis, organ/brain freezing technologies\nCerebrum DAO → Brain health, neurodegeneration, Alzheimer's research\nCuretopia → Rare disease research, genetic disorders, orphan drug discovery\nLong COVID Labs → Long COVID, post-viral syndromes, chronic illness research\nQuantum Biology DAO → Quantum biology, biophysics, quantum effects in biology\n</dao_list>\n\nReturn your output **only** as a JSON array of DAO names. If no DAOs are relevant, return an empty array.\n\nExample output format:\n[\"DAO1\", \"DAO2\", \"DAO3\"]\nor\n[]`;\n"
    },
    {
      "path": "src\\services\\kaService\\processPaper.ts",
      "detailed_description": "该文件 src/services/kaService/processPaper.ts 负责处理科学论文的各个部分，提取结构化信息并生成相应的知识图谱。其主要功能包括：提取论文的各个部分（如引言、摘要、方法、结果和讨论），并根据这些部分生成基本信息、引用和相关的子图（如基因本体GO、疾病本体DOID、化合物本体ChEBI和药物分类ATC）。文件中通过异步函数和并行处理来提升性能，利用Anthropic API生成响应，并通过不同的工具函数来构建和格式化数据，确保最终生成的JSON-LD结构符合知识图谱的需求。代码逻辑包括：提取文档各部分的页码范围、生成基本信息和引用、构建GO和DOID子图并更新相关术语，最后将所有生成的信息合并为一个综合图谱。该文件通过将复杂的科学信息转化为可用的知识资产，支持科学研究和数据共享。",
      "summary": "该文件用于处理科学论文，提取结构化信息并生成知识图谱。",
      "raw": "import {\n  get_prompt_basic_info,\n  get_prompt_citations,\n  get_prompt_go_subgraph,\n  get_prompt_doid_subgraph,\n  get_prompt_chebi_subgraph,\n  get_prompt_atc_subgraph,\n  get_prompt_spar_ontology,\n  get_prompt_spar_citations,\n  get_prompt_section_page_numbers,\n  get_prompt_go_ontology,\n  get_prompt_suggested_questions,\n} from \"./llmPrompt\";\n\nimport { generateResponse, getClient } from \"./anthropicClient\";\nimport {\n  updateGoTerms,\n  updateDoidTerms,\n  updateChebiTerms,\n  updateAtcTerms,\n} from \"./biologyApi\";\n\nimport { extractBracketContent, isEmptyArray } from \"./regex\";\nimport { logger } from \"@elizaos/core\";\nimport Anthropic from \"@anthropic-ai/sdk\";\n\n/** Generic JSON-like type for representing arbitrary structures. */\ntype JSONValue =\n  | string\n  | number\n  | boolean\n  | { [x: string]: JSONValue }\n  | JSONValue[];\n\n/** One page's text and metadata. */\ninterface PaperArrayElement {\n  metadata: {\n    page_number: number;\n  };\n  text: string;\n}\n\n/** Object storing arrays of text for each major section. */\ninterface PaperDict {\n  introduction: string[];\n  abstract: string[];\n  methods: string[];\n  results: string[];\n  discussion: string[];\n  citations: string[];\n}\n\n/** Key-value for label => [startPage, endPage]. */\ninterface LabelPageRanges {\n  [label: string]: [number, number];\n}\n\n/** Subgraph object typically returned from the extraction steps. */\ninterface SubgraphSet {\n  go: string;\n  doid: string;\n  chebi: string;\n  atc: string;\n}\n\n/** Result of processPaper: a 6-tuple of strings. */\ntype PaperProcessResult = [string, string, string, string, string, string];\n\nconst CITATIONS_OFFSET = 6;\n\n/**\n * Extract page ranges for known sections (Abstract, Introduction, Methods, etc.).\n */\nexport async function extractSections(\n  client: Anthropic,\n  paper_array: PaperArrayElement[]\n): Promise<LabelPageRanges> {\n  const originalLabels = [\n    \"Abstract\",\n    \"Introduction\",\n    \"Methods\",\n    \"Materials and methods\",\n    \"Material and methods\",\n    \"Results\",\n    \"Discussion\",\n  ];\n\n  const labels = originalLabels.map((label) => label.toLowerCase());\n  const label_page_numbers: Record<string, number[]> = {};\n  const label_mapping: Record<string, string> = {};\n\n  for (const label of labels) {\n    label_page_numbers[label] = [];\n    const originalLabel = originalLabels.find(\n      (orig) => orig.toLowerCase() === label\n    );\n    label_mapping[label] = originalLabel ? originalLabel : label;\n  }\n\n  for (const element of paper_array) {\n    const page_number = element.metadata.page_number;\n    const textLower = element.text.toLowerCase();\n\n    for (const label of labels) {\n      if (textLower.includes(label)) {\n        // Skip table of contents on page 1 for all but \"Abstract\" and \"Introduction\"\n        if (\n          page_number === 1 &&\n          label !== \"abstract\" &&\n          label !== \"introduction\"\n        ) {\n          continue;\n        }\n        label_page_numbers[label].push(page_number);\n      }\n    }\n  }\n\n  // If \"methods\" array is empty, try \"materials and methods\" or \"material and methods\"\n  const methodAliases = [\"materials and methods\", \"material and methods\"];\n  if (label_page_numbers[\"methods\"].length === 0) {\n    for (const alias of methodAliases) {\n      if (label_page_numbers[alias] && label_page_numbers[alias].length > 0) {\n        label_page_numbers[\"methods\"] = label_page_numbers[alias];\n        break;\n      }\n    }\n  }\n\n  // Remove the separate method aliases from the dictionary\n  for (const alias of methodAliases) {\n    if (alias in label_page_numbers) {\n      delete label_page_numbers[alias];\n      const idx = labels.indexOf(alias);\n      if (idx >= 0) {\n        labels.splice(idx, 1);\n      }\n      const origAlias = label_mapping[alias];\n      const origAliasIdx = originalLabels.indexOf(origAlias);\n      if (origAliasIdx >= 0) {\n        originalLabels.splice(origAliasIdx, 1);\n      }\n    }\n  }\n\n  // Map each label => first page or undefined\n  const first_appearance: Record<string, number | undefined> = {};\n  for (const [label, pages] of Object.entries(label_page_numbers)) {\n    first_appearance[label] = pages.length > 0 ? pages[0] : undefined;\n  }\n  // Remove undefined\n  for (const key of Object.keys(first_appearance)) {\n    if (first_appearance[key] === undefined) {\n      delete first_appearance[key];\n    }\n  }\n\n  const sorted_labels = Object.entries(first_appearance).sort((a, b) => {\n    const aVal = a[1] === undefined ? Infinity : a[1];\n    const bVal = b[1] === undefined ? Infinity : b[1];\n    return (aVal as number) - (bVal as number);\n  });\n\n  const label_page_ranges: LabelPageRanges = {};\n\n  // Build up the start/stop pages for each discovered label\n  for (let i = 0; i < sorted_labels.length; i++) {\n    const [label, startPageRaw] = sorted_labels[i];\n    if (startPageRaw == null) continue;\n    let start_page = startPageRaw;\n    let end_page: number;\n\n    // If first label, set to page 1\n    if (i === 0) {\n      start_page = 1;\n    }\n\n    // If last label => end on final page\n    if (i === sorted_labels.length - 1) {\n      end_page = paper_array[paper_array.length - 1].metadata.page_number;\n    } else {\n      const [_, nextStart] = sorted_labels[i + 1];\n      if (nextStart != null && nextStart >= start_page) {\n        end_page = nextStart;\n      } else {\n        end_page = start_page;\n      }\n    }\n    label_page_ranges[label] = [start_page, end_page];\n  }\n\n  // Some labels not found => request pages from the LLM\n  const labels_with_no_range: string[] = labels.filter(\n    (label) => !(label in label_page_ranges)\n  );\n\n  if (labels_with_no_range.length > 0) {\n    const prompt = get_prompt_section_page_numbers(\n      paper_array.map((el) => ({\n        metadata: { page_number: el.metadata.page_number },\n        text: el.text,\n      })),\n      labels_with_no_range.map((lbl) => label_mapping[lbl])\n    );\n\n    const answer = await generateResponse(\n      client,\n      prompt,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n\n    const answerLines = answer.split(\"\\n\");\n\n    const additional_label_page_ranges: LabelPageRanges = {};\n\n    for (const line of answerLines) {\n      // e.g. \"Introduction, 2, 5\"\n      const match = line.trim().match(/^(\\w+),\\s*(\\d+),\\s*(\\d+)/);\n      if (match) {\n        const labelLower = match[1].toLowerCase();\n        const startPage = parseInt(match[2], 10);\n        const endPage = parseInt(match[3], 10);\n        additional_label_page_ranges[labelLower] = [startPage, endPage];\n      }\n    }\n\n    // Merge additional ranges in\n    for (const [label, range] of Object.entries(additional_label_page_ranges)) {\n      label_page_ranges[label] = range;\n    }\n  }\n\n  // Build final list of extracted labels\n  const extractedLabels = Object.keys(label_page_ranges)\n    .map((lbl) => label_mapping[lbl])\n    .filter((lbl2) => lbl2 != null);\n\n  const skippedLabels = originalLabels.filter(\n    (lbl) => !extractedLabels.includes(lbl)\n  );\n\n  if (skippedLabels.length > 0) {\n    logger.info(`Skipped sections: ${skippedLabels.join(\", \")}`);\n  } else {\n    logger.info(`All sections were extracted.`, label_page_ranges);\n  }\n\n  // Make introduction start at page 0 if it exists\n  if (\"introduction\" in label_page_ranges) {\n    const [_, endPage] = label_page_ranges[\"introduction\"];\n    label_page_ranges[\"introduction\"] = [0, endPage];\n  }\n\n  return label_page_ranges;\n}\n\n/**\n * Return a string containing basic info about the paper, or \"\" on error.\n */\nexport async function getGeneratedBasicInfoText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  const spar_array = Array.from(\n    new Set([...paper_dict.introduction, ...paper_dict.abstract])\n  );\n  try {\n    const prompt_basic_info = get_prompt_basic_info(spar_array);\n    const generated_basic_info_text = await generateResponse(\n      client,\n      prompt_basic_info\n    );\n    logger.info(\n      `Generated basic text from Claude: ${generated_basic_info_text}`\n    );\n    return generated_basic_info_text;\n  } catch (e) {\n    logger.error(\"Generated basic info text exception\", e);\n    return \"\";\n  }\n}\n\n/**\n * Return a string containing citations, or \"\" on error.\n */\nexport async function getGeneratedCitations(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const prompt_citations = get_prompt_citations(paper_dict.citations);\n    const generated_citations = await generateResponse(\n      client,\n      prompt_citations,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(`Generated citations from Claude: ${generated_citations}`);\n    return generated_citations;\n  } catch (e) {\n    logger.error(\"Generated citations exception\", e);\n    return \"\";\n  }\n}\n\n/**\n * Generate a GO subgraph from relevant sections, then refine it via update_go_terms.\n */\nexport async function getGoGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const go_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.methods,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_go_subgraph(go_array);\n    let generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated GO subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = {};\n    }\n\n    // The below function presumably returns a new structure or modifies in place:\n    const updated_subgraph = await updateGoTerms(generated_subgraph, client);\n    generated_subgraph_text = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using GO API: ${generated_subgraph_text}`);\n    return generated_subgraph_text;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"{}\";\n  }\n}\n\n/**\n * Generate a DOID subgraph from relevant sections, then refine it via update_doid_terms.\n */\nexport async function getDoidGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const doid_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.abstract,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_doid_subgraph(doid_array);\n    const generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated DOID subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = [];\n    }\n    const updated_subgraph = await updateDoidTerms(generated_subgraph, client);\n    const finalText = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using DOID API: ${finalText}`);\n    return finalText;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"[]\";\n  }\n}\n\n/**\n * Generate a ChEBI subgraph from relevant sections, then refine it via update_chebi_terms.\n */\nexport async function getChebiGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const chebi_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.abstract,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_chebi_subgraph(chebi_array);\n    const generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated ChEBI subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = [];\n    }\n    const updated_subgraph = await updateChebiTerms(generated_subgraph, client);\n    const finalText = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using CHEBI API: ${finalText}`);\n    return finalText;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"[]\";\n  }\n}\n\n/**\n * Generate an ATC subgraph from relevant sections, then refine it via update_atc_terms.\n */\nexport async function getAtcGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const atc_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.abstract,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_atc_subgraph(atc_array);\n    const generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated ATC subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = [];\n    }\n    const updated_subgraph = await updateAtcTerms(generated_subgraph, client);\n    const finalText = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using ATC API: ${finalText}`);\n    return finalText;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"[]\";\n  }\n}\n\n/**\n * Launch parallel tasks to produce each piece of data from a single paper.\n */\nexport async function process_paper(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<PaperProcessResult> {\n  const [\n    generated_basic_info,\n    generated_citations,\n    generated_go_subgraph,\n    generated_doid_subgraph,\n    generated_chebi_subgraph,\n    generated_atc_subgraph,\n  ] = await Promise.all([\n    getGeneratedBasicInfoText(client, paper_dict),\n    getGeneratedCitations(client, paper_dict),\n    getGoGeneratedSubgraphText(client, paper_dict),\n    getDoidGeneratedSubgraphText(client, paper_dict),\n    getChebiGeneratedSubgraphText(client, paper_dict),\n    getAtcGeneratedSubgraphText(client, paper_dict),\n  ]);\n\n  return [\n    generated_basic_info,\n    generated_citations,\n    generated_go_subgraph,\n    generated_doid_subgraph,\n    generated_chebi_subgraph,\n    generated_atc_subgraph,\n  ];\n}\n\n/**\n * Truncates a JSON array string at the last valid element, then closes the array.\n */\nexport function fix_json_string_manually(json_string: string): string {\n  const lastBraceIndex = json_string.lastIndexOf(\"},\");\n  if (lastBraceIndex !== -1) {\n    json_string = json_string.slice(0, lastBraceIndex + 1);\n  }\n\n  if (json_string.endsWith(\",\")) {\n    json_string = json_string.slice(0, -1);\n  }\n\n  return json_string + \"]\";\n}\n\n/**\n * Convert freeform citations text into a JSON-LD array (SPAR citations).\n */\nexport async function get_subgraph_citations(\n  client: Anthropic,\n  citations_text: string\n): Promise<JSONValue> {\n  const prompt_spar_citations = get_prompt_spar_citations(citations_text);\n  const generated_citations_spar_text = await generateResponse(\n    client,\n    prompt_spar_citations,\n    \"claude-3-5-sonnet-20241022\",\n    8192\n  );\n  logger.info(\n    `Generated SPAR citations from Claude: ${generated_citations_spar_text}`\n  );\n\n  try {\n    return JSON.parse(generated_citations_spar_text) as JSONValue;\n  } catch {\n    const fixed_citations = fix_json_string_manually(\n      generated_citations_spar_text\n    );\n    logger.info(`Fixed citations: ${fixed_citations}`);\n    return JSON.parse(fixed_citations) as JSONValue;\n  }\n}\n\n/**\n * Use SPAR+OBI ontology prompt to convert basic info text into JSON-LD.\n */\nexport async function get_subgraph_basic_info(\n  client: Anthropic,\n  basic_info_text: string\n): Promise<string> {\n  if (isEmptyArray(basic_info_text)) {\n    return basic_info_text;\n  }\n\n  const prompt_spar_ontology_ = get_prompt_spar_ontology(basic_info_text);\n  const generated_graph_text = await generateResponse(\n    client,\n    prompt_spar_ontology_,\n    \"claude-3-5-sonnet-20241022\",\n    8192\n  );\n  logger.info(`Generated SPAR graph from Claude: ${generated_graph_text}`);\n\n  let textTrimmed = generated_graph_text.trim();\n  if (textTrimmed.startsWith(\"```json\") && textTrimmed.endsWith(\"```\")) {\n    textTrimmed = textTrimmed.slice(7, -3).trim();\n  }\n  return textTrimmed;\n}\n\n/**\n * Convert a GO subgraph from raw JSON to an ontology array (JSON-LD).\n */\nexport async function get_subgraph_go(\n  client: Anthropic,\n  generated_go_subgraph: string\n): Promise<JSONValue> {\n  try {\n    if (isEmptyArray(generated_go_subgraph)) {\n      return [];\n    }\n    const prompt_go_ontology_ = get_prompt_go_ontology(generated_go_subgraph);\n    const generated_graph_text = await generateResponse(\n      client,\n      prompt_go_ontology_,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(`Generated GO subgraph from Claude: ${generated_graph_text}`);\n\n    const extracted_content = extractBracketContent(generated_graph_text);\n    if (extracted_content === null) {\n      return [];\n    }\n    return JSON.parse(extracted_content);\n  } catch (e) {\n    logger.error(\"Error generating GO subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Convert the DOID subgraph from raw JSON to RDF.\n */\nexport function get_subgraph_doid(generated_doid_subgraph: string): JSONValue {\n  try {\n    const doidData = JSON.parse(generated_doid_subgraph);\n    if (!Array.isArray(doidData)) return [];\n\n    const rdf = doidData.map((item: Record<string, JSONValue>) => ({\n      \"@id\": `http://purl.obolibrary.org/obo/${item[\"disease_id\"]}`,\n      \"dcterms:title\": item[\"disease\"],\n      \"dcterms:description\": item[\"findings\"],\n    }));\n    return rdf;\n  } catch (e) {\n    logger.error(\"Error generating DOID subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Convert the CHEBI subgraph from raw JSON to RDF.\n */\nexport function get_subgraph_chebi(\n  generated_chebi_subgraph: string\n): JSONValue {\n  try {\n    const chebiData = JSON.parse(generated_chebi_subgraph);\n    if (!Array.isArray(chebiData)) return [];\n\n    const rdf = chebiData.map((item: Record<string, JSONValue>) => ({\n      \"@id\": `http://purl.obolibrary.org/obo/${item[\"compound_id\"]}`,\n      \"dcterms:title\": item[\"compound\"],\n      \"dcterms:description\": item[\"findings\"],\n    }));\n    return rdf;\n  } catch (e) {\n    logger.error(\"Error generating CHEBI subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Convert the ATC subgraph from raw JSON to RDF.\n */\nexport function get_subgraph_atc(generated_atc_subgraph: string): JSONValue {\n  try {\n    const atcData = JSON.parse(generated_atc_subgraph);\n    if (!Array.isArray(atcData)) return [];\n\n    const rdf = atcData.map((item: Record<string, JSONValue>) => ({\n      \"@id\": `http://purl.bioontology.org/ontology/ATC/${item[\"drug_id\"]}`,\n      \"dcterms:title\": item[\"drug\"],\n      \"dcterms:description\": item[\"findings\"],\n    }));\n    return rdf;\n  } catch (e) {\n    logger.error(\"Error generating ATC subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Build a final combined JSON-LD-like graph from basic info, citations, and subgraphs.\n */\nexport async function create_graph(\n  client: Anthropic,\n  basic_info_text: string,\n  citations_text: string,\n  subgraph: SubgraphSet\n): Promise<Record<string, JSONValue>> {\n  const { go, doid, chebi, atc } = subgraph;\n\n  let generated_graph: Record<string, JSONValue> = {};\n\n  try {\n    const generated_graph_text = await get_subgraph_basic_info(\n      client,\n      basic_info_text\n    );\n    generated_graph = JSON.parse(generated_graph_text) as Record<\n      string,\n      JSONValue\n    >;\n  } catch (e) {\n    logger.error(\"Generating graph exception\", e);\n  }\n\n  // Ensure the array is in place\n  if (!generated_graph[\"obi:OBI_0000299\"]) {\n    generated_graph[\"obi:OBI_0000299\"] = [];\n  }\n\n  // Merge subgraphs\n  const goArray = await get_subgraph_go(client, go);\n  const doidArray = get_subgraph_doid(doid);\n  const chebiArray = get_subgraph_chebi(chebi);\n  const atcArray = get_subgraph_atc(atc);\n\n  // We know \"obi:OBI_0000299\" must be an array\n  const obiArr = generated_graph[\"obi:OBI_0000299\"];\n  if (Array.isArray(obiArr)) {\n    if (Array.isArray(goArray)) obiArr.push(...goArray);\n    if (Array.isArray(doidArray)) obiArr.push(...doidArray);\n    if (Array.isArray(chebiArray)) obiArr.push(...chebiArray);\n    if (Array.isArray(atcArray)) obiArr.push(...atcArray);\n  }\n\n  try {\n    const subgraphCites = await get_subgraph_citations(client, citations_text);\n    generated_graph[\"cito:cites\"] = subgraphCites;\n  } catch (e) {\n    logger.error(\"Error generating citations\", e);\n  }\n\n  // If we have a valid DOI, set @id\n  const doi = generated_graph[\"dcterms:identifier\"];\n  if (doi && doi !== \"https://doi.org/XX.XXXX/XX.XXXX\") {\n    generated_graph[\"@id\"] = doi;\n  } else {\n    generated_graph[\"@id\"] = \"PLEASE FILL IN THE DOI URL IDENTIFIER HERE\";\n  }\n\n  return generated_graph;\n}\n\n/**\n * Create arrays of text for each recognized section.\n */\nexport function create_section_arrays(\n  paper_array: PaperArrayElement[],\n  section_ranges: LabelPageRanges\n): PaperDict {\n  const introduction_array: string[] = [];\n  const abstract_array: string[] = [];\n  const methods_array: string[] = [];\n  const results_array: string[] = [];\n  const discussion_array: string[] = [];\n\n  for (const element of paper_array) {\n    const page_number = element.metadata.page_number;\n    const text = element.text;\n\n    // Introduction\n    if (\n      \"introduction\" in section_ranges &&\n      page_number >= section_ranges[\"introduction\"][0] &&\n      page_number <= section_ranges[\"introduction\"][1]\n    ) {\n      introduction_array.push(text);\n    }\n    // Abstract\n    if (\n      \"abstract\" in section_ranges &&\n      page_number >= section_ranges[\"abstract\"][0] &&\n      page_number <= section_ranges[\"abstract\"][1]\n    ) {\n      abstract_array.push(text);\n    }\n    // Methods\n    if (\n      \"methods\" in section_ranges &&\n      page_number >= section_ranges[\"methods\"][0] &&\n      page_number <= section_ranges[\"methods\"][1]\n    ) {\n      methods_array.push(text);\n    }\n    // Results\n    if (\n      \"results\" in section_ranges &&\n      page_number >= section_ranges[\"results\"][0] &&\n      page_number <= section_ranges[\"results\"][1]\n    ) {\n      results_array.push(text);\n    }\n    // Discussion\n    if (\n      \"discussion\" in section_ranges &&\n      page_number >= section_ranges[\"discussion\"][0] &&\n      page_number <= section_ranges[\"discussion\"][1]\n    ) {\n      discussion_array.push(text);\n    }\n  }\n\n  return {\n    introduction: introduction_array,\n    abstract: abstract_array,\n    methods: methods_array,\n    results: results_array,\n    discussion: discussion_array,\n    citations: [],\n  };\n}\n\n/**\n * Build a PaperDict from the full PDF text array, including the final citations.\n */\nexport async function processJsonArray(\n  paper_array: PaperArrayElement[],\n  client: Anthropic\n): Promise<PaperDict> {\n  const section_ranges = await extractSections(client, paper_array);\n  const paper_array_dict = create_section_arrays(paper_array, section_ranges);\n\n  const lastPage = paper_array[paper_array.length - 1].metadata.page_number;\n  paper_array_dict.citations = paper_array\n    .filter(\n      (el) =>\n        el.metadata.page_number >= lastPage - CITATIONS_OFFSET &&\n        typeof el.text === \"string\"\n    )\n    .map((el) => el.text);\n\n  return paper_array_dict;\n}\n\n/**\n * Generate three suggested research questions for the given paper dictionary.\n */\nexport async function get_suggested_questions(\n  paper_dict: string\n): Promise<string[]> {\n  try {\n    const client = getClient(); // Adjust if you must await or pass config\n    const prompt_questions = get_prompt_suggested_questions(paper_dict);\n    const generated_questions_text = await generateResponse(\n      client,\n      prompt_questions,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n\n    const lines = generated_questions_text.trim().split(\"\\n\");\n    const questions = lines.filter((q) => q.trim().length > 0);\n    logger.info(`Generated suggested questions from Claude: ${questions}`);\n    return questions;\n  } catch (e) {\n    logger.error(\"Error generating questions\", e);\n    return [];\n  }\n}\n"
    },
    {
      "path": "src\\services\\kaService\\regex.ts",
      "detailed_description": "该文件名为 regexUtils.ts，提供了一组用于处理字符串的实用正则表达式函数。它包含三个主要功能：首先，extractBracketContent 函数用于提取输入字符串中第一个括号内的内容，包括括号本身，功能类似于 Python 的 re.search 方法。其次，isEmptyArray 函数检查输入字符串（去除空白后）是否严格等于 '[]'，用于验证数组表示是否为空。最后，convertToValidJsonString 函数将特定的单引号替换为双引号，以使字符串符合 JSON 格式的要求，尤其是在处理带有嵌套结构的字符串时。此函数依赖于现代 JavaScript/TypeScript 运行时的前瞻和后顾断言功能，确保仅在特定上下文中替换单引号。总体而言，这些函数增强了字符串处理的灵活性和准确性，特别是在处理科学论文元数据的提取和转换过程时。",
      "summary": "该文件提供了实用的正则表达式函数，用于处理和转换字符串，特别是在科学数据处理中。",
      "raw": "// regexUtils.ts\n\n/**\n * Extracts the first bracketed substring (including the brackets).\n * Equivalent to Python's `re.search(r\"\\[.*\\]\", string, re.DOTALL)`.\n *\n * @param input - The string to search\n * @returns The bracketed substring (e.g., \"[something]\") or null if none found\n */\nexport function extractBracketContent(input: string): string | null {\n    // Use a dot-all style pattern in JavaScript: [\\s\\S] matches any character including newlines\n    // The parentheses are optional if you only want the entire match in group(0)\n    const match = input.match(/\\[([\\s\\S]*?)\\]/);\n    return match ? match[0] : null;\n}\n\n/**\n * Checks if a string (after trimming) is exactly \"[]\".\n * Equivalent to Python's `return string.strip() == \"[]\"`.\n *\n * @param input - The string to check\n * @returns True if the trimmed string is \"[]\", false otherwise\n */\nexport function isEmptyArray(input: string): boolean {\n    return input.trim() === \"[]\";\n}\n\n/**\n * Replaces certain single quotes with double quotes, mimicking the Python regex:\n *  re.sub(r\"(?<!\\\\)'(?=[^:]+?')|(?<=: )'(?=[^']+?')\", '\"', input_string)\n *\n * IMPORTANT: This relies on lookbehind assertions which require a modern JavaScript/TypeScript runtime.\n * If your environment doesn't support lookbehinds, you must rewrite this regex.\n *\n * @param input - The string with possible single quotes to replace\n * @returns The corrected string with targeted single quotes replaced by double quotes\n */\nexport function convertToValidJsonString(input: string): string {\n    // Pattern breakdown:\n    //  1) (?<!\\\\)'(?=[^:]+?')\n    //      Match `'` not preceded by a backslash, followed by any chars up until another `'`, but not containing ':' in that span.\n    //  2) (?<=: )'(?=[^']+?')\n    //      Match `'` that is preceded by ': ', followed by chars up to the next `'`.\n    //\n    // Both are replaced with `\"`.\n    const pattern = /(?<!\\\\)'(?=[^:]+?')|(?<=: )'(?=[^']+?')/g;\n    return input.replace(pattern, '\"');\n}\n"
    },
    {
      "path": "src\\services\\kaService\\sparqlQueries.ts",
      "detailed_description": "文件 src/services/kaService/sparqlQueries.ts 定义了两个函数，分别用于生成 SPARQL 查询以获取与 DOI 相关的科研论文信息和检查论文是否存在于知识图谱中。函数 getPaperByDoi 接受一个 DOI 字符串作为参数，并构建一个 SPARQL 查询，该查询从知识图谱中选择与该 DOI 相关的论文，包括标题、摘要、作者、相关多组学数据、实验信息、队列信息和分析描述等。通过使用 GROUP_CONCAT 函数，查询将多个可能的作者或信息合并为单一字符串，方便后续处理。第二个函数 paperExists 则通过 ASK 语句检查特定 DOI 的论文是否存在于知识图谱中，返回布尔结果。两个函数都确保在处理 DOI 时首先去掉 URL 前缀，以保证查询的准确性。这些函数的设计使得应用能够高效地从知识图谱中检索和验证科研论文的信息，支持科学研究和数据整合。",
      "summary": "该文件生成 SPARQL 查询以获取科研论文信息和检查论文存在性，支持科学研究的数据验证与整合。",
      "raw": "export function getPaperByDoi(doi: string) {\n    if (doi.startsWith(\"https://doi.org/\")) {\n        doi = doi.replace(\"https://doi.org/\", \"\");\n    }\n    const getPaperByDoiQuery = `PREFIX fabio: <http://purl.org/spar/fabio/>\n    PREFIX dcterms: <http://purl.org/dc/terms/>\n    PREFIX foaf:   <http://xmlns.com/foaf/0.1/>\n    PREFIX obi:    <http://purl.obolibrary.org/obo/>\n    PREFIX schema: <http://schema.org/>\n\n    SELECT ?paper ?title ?abstract ?doi\n        (GROUP_CONCAT(DISTINCT ?creatorName;    SEPARATOR=\", \") AS ?allCreators)\n        (GROUP_CONCAT(DISTINCT ?multiomics;     SEPARATOR=\" | \") AS ?allMultiomics)\n        (GROUP_CONCAT(DISTINCT ?assays;         SEPARATOR=\" | \") AS ?allAssays)\n        (GROUP_CONCAT(DISTINCT ?cohort;         SEPARATOR=\" | \") AS ?allCohortInfo)\n        (GROUP_CONCAT(DISTINCT ?analysisDesc;   SEPARATOR=\" | \") AS ?allAnalysisDesc)\n        (GROUP_CONCAT(DISTINCT ?relatedOrg;     SEPARATOR=\" | \") AS ?allRelatedOrgs)\n    WHERE {\n    \n    <https://doi.org/${doi}> a fabio:ResearchPaper ;\n                                                dcterms:title ?title ;\n                                                dcterms:abstract ?abstract ;\n                                                dcterms:identifier ?doi .\n    BIND (<https://doi.org/${doi}> AS ?paper)\n    \n    OPTIONAL {\n        ?paper dcterms:creator ?creator .\n        ?creator foaf:name ?creatorName .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0000299 ?multiomicsNode .\n        ?multiomicsNode dcterms:description ?multiomics .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0000968 ?assayNode .\n        ?assayNode dcterms:description ?assays .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0000293 ?cohortNode .\n        ?cohortNode dcterms:description ?cohort .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0200000 ?analysisNode .\n        ?analysisNode dcterms:description ?analysisDesc .\n    }\n    \n    OPTIONAL {\n        ?paper schema:relatedTo ?related .\n        ?related schema:name ?relatedOrg .\n    }\n    }\n    GROUP BY ?paper ?title ?abstract ?doi`;\n\n    return getPaperByDoiQuery;\n}\n\nexport function paperExists(doi: string) {\n    if (doi.startsWith(\"https://doi.org/\")) {\n        doi = doi.replace(\"https://doi.org/\", \"\");\n    }\n    const paperExistsQuery = `PREFIX fabio: <http://purl.org/spar/fabio/>\n    PREFIX dcterms: <http://purl.org/dc/terms/>\n\n    ASK {\n    ?paper a fabio:ResearchPaper ;\n            dcterms:identifier ?doi .\n    \n    FILTER (STR(?doi) = \"https://doi.org/${doi}\")\n    }\n    `;\n    return paperExistsQuery;\n}\n"
    },
    {
      "path": "src\\services\\kaService\\unstructuredPartitioning.ts",
      "detailed_description": "该文件定义了与 Unstructured API 进行交互的功能，主要用于将 PDF 文件的内容发送到该 API 进行解析。文件首先引入所需的库，包括 axios、form-data、fs/promises 和 dotenv，以便处理 HTTP 请求、表单数据以及环境变量。在核心功能上，makeUnstructuredApiRequest 函数接受文件内容（以 Buffer 形式）、文件名和 API 密钥作为参数，构建一个表单数据对象，并将其发送到 Unstructured API 的指定 URL。请求中附带了一些配置选项，如 pdf_infer_table_structure 和 strategy，以控制解析的行为。函数还设置了请求的超时时间，并使用 logger 记录请求的状态和响应。当 API 返回结果时，函数将响应数据返回。该文件还包含一个被注释掉的示例函数 processPdfFiles，演示如何读取本地 PDF 文件，调用 API 进行处理并将结果保存为 JSON 文件，虽然该部分代码未被执行，但提供了如何使用主要功能的示例。此文件在处理科学论文的自动化和结构化信息提取方面具有重要作用。",
      "summary": "该文件用于与 Unstructured API 交互，将 PDF 文件内容解析为结构化数据。",
      "raw": "import axios from \"axios\";\nimport FormData from \"form-data\";\nimport fs from \"fs/promises\";\nimport \"dotenv/config\";\nimport { logger } from \"@elizaos/core\";\n\nconst apiKey = process.env.UNSTRUCTURED_API_KEY;\n\n/**\n * Makes a POST request to the Unstructured API.\n *\n * @param fileBytes - The file content as a Buffer.\n * @param filename - Name of the file.\n * @param apiKey - Unstructured API key.\n * @returns The parsed API response.\n */\nexport async function makeUnstructuredApiRequest(\n  fileBytes: Buffer,\n  filename: string,\n  apiKey: string\n) {\n  const url = \"https://api.unstructuredapp.io/general/v0/general\";\n\n  // Create a FormData instance and append file and other data.\n  const formData = new FormData();\n  formData.append(\"files\", fileBytes, filename);\n  formData.append(\"pdf_infer_table_structure\", \"true\");\n  formData.append(\"skip_infer_table_types\", \"[]\");\n  formData.append(\"strategy\", \"hi_res\");\n\n  // Merge the custom header with form-data headers.\n  const headers = {\n    \"unstructured-api-key\": apiKey,\n    ...formData.getHeaders(),\n  };\n\n  logger.info(\"Making Unstructured API request\");\n  const response = await axios.post(url, formData, {\n    headers,\n    timeout: 300000, // 300000 ms\n  });\n\n  logger.info(\"Got response from Unstructured API\");\n  return response.data;\n}\n\n// async function processPdfFiles(): Promise<void> {\n//   try {\n//     const arxivPdfBuffer = await fs.readFile(\"arxiv_paper.pdf\");\n//     const bioArxivPdfBuffer = await fs.readFile(\"biorxiv_paper.pdf\");\n\n//     const arxivResponse = await makeUnstructuredApiRequest(\n//       arxivPdfBuffer,\n//       \"arxiv_paper.pdf\",\n//       apiKey\n//     );\n//     console.log(\"Response for arxiv_paper.pdf:\", arxivResponse);\n//     await fs.writeFile(\n//       \"arxiv_paper.json\",\n//       JSON.stringify(arxivResponse, null, 2)\n//     );\n\n//     const bioArxivResponse = await makeUnstructuredApiRequest(\n//       bioArxivPdfBuffer,\n//       \"biorxiv_paper.pdf\",\n//       apiKey\n//     );\n//     console.log(\"Response for biorxiv_paper.pdf:\", bioArxivResponse);\n//     await fs.writeFile(\n//       \"biorxiv_paper.json\",\n//       JSON.stringify(bioArxivResponse, null, 2)\n//     );\n//   } catch (error) {\n//     console.error(\"Error processing PDF files:\", error);\n//   }\n// }\n\n// processPdfFiles();\n"
    },
    {
      "path": "src\\services\\kaService\\vectorize.ts",
      "detailed_description": "文件 vectorize.ts 的主要功能是生成科学论文的摘要，利用大型语言模型 (LLM) 客户端与输入的图数据进行交互。该文件定义了几个接口，包括 Graph、CitationEntry 和 SimilarCitationResult，以便于结构化处理论文元数据。核心函数 getSummary 接受一个 LLM 客户端和一个图对象作为参数，构建一个用于生成摘要的提示，并调用 generateResponse 函数发送请求。该函数的执行过程中，使用 logger 记录生成的摘要或捕获的异常，确保任何错误信息都被记录下来以便后续调试。通过该文件，用户可以高效地从科学论文的元数据中提取出简洁且相关的摘要，从而便于信息的快速掌握和知识的传播。",
      "summary": "该文件用于生成科学论文的摘要，利用大型语言模型客户端处理论文元数据。",
      "raw": "// vectorize.ts\n\nimport { get_prompt_vectorization_summary } from \"./llmPrompt\"; // Adjust path as needed\nimport { generateResponse } from \"./anthropicClient\"; // Adjust path as needed\nimport Anthropic from \"@anthropic-ai/sdk\";\nimport { logger } from \"@elizaos/core\";\n\n/**\n * The general structure of your graph object. Extend with any additional fields you need.\n */\ninterface Graph {\n  [key: string]: unknown;\n  \"dcterms:title\"?: string;\n  \"@id\"?: string;\n}\n\n/**\n * A single citation entry. Adjust if your citation objects have more fields.\n */\ninterface CitationEntry {\n  [key: string]: unknown;\n  \"@id\": string;\n  \"dcterms:title\": string;\n}\n\n/**\n * The shape returned by findSimilarTitle. The second element is a similarity score (0-1).\n * The first element's metadata presumably includes a \"doi\" string.\n */\ninterface SimilarCitationResult {\n  metadata: { [key: string]: string };\n}\n\n/**\n * Generate a summary for the provided graph using the LLM client.\n * @param client - The client or config object for your LLM\n * @param graph  - The graph/dictionary containing paper metadata\n */\nexport async function getSummary(\n  client: Anthropic,\n  graph: Graph\n): Promise<string> {\n  let summary = \"\";\n  try {\n    const prompt = get_prompt_vectorization_summary(graph);\n    summary = await generateResponse(client, prompt);\n    logger.info(`Generated graph summary from Claude: ${summary}`);\n  } catch (error) {\n    logger.error(\"Generated graph summary exception\", error);\n    summary = \"\";\n  }\n  return summary;\n}\n"
    }
  ]
}